# LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving

[Link to the paper](http://arxiv.org/abs/2310.03026v1)

## Authors
- Hao Sha
- Yao Mu
- Yuxuan Jiang
- Li Chen
- Chenfeng Xu
- Ping Luo
- Shengbo Eben Li
- Masayoshi Tomizuka
- Wei Zhan
- Mingyu Ding

## Summary
  Existing learning-based autonomous driving (AD) systems face challenges in
comprehending high-level information, generalizing to rare events, and
providing interpretability. To address these problems, this work employs Large
Language Models (LLMs) as a decision-making component for complex AD scenarios
that require human commonsense understanding. We devise cognitive pathways to
enable comprehensive reasoning with LLMs, and develop algorithms for
translating LLM decisions into actionable driving commands. Through this
approach, LLM decisions are seamlessly integrated with low-level controllers by
guided parameter matrix adaptation. Extensive experiments demonstrate that our
proposed method not only consistently surpasses baseline approaches in
single-vehicle tasks, but also helps handle complex driving behaviors even
multi-vehicle coordination, thanks to the commonsense reasoning capabilities of
LLMs. This paper presents an initial step toward leveraging LLMs as effective
decision-makers for intricate AD scenarios in terms of safety, efficiency,
generalizability, and interoperability. We aspire for it to serve as
inspiration for future research in this field. Project page:
https://sites.google.com/view/llm-mpc


# Retrieval meets Long Context Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.03025v1)

## Authors
- Peng Xu
- Wei Ping
- Xianchao Wu
- Lawrence McAfee
- Chen Zhu
- Zihan Liu
- Sandeep Subramanian
- Evelina Bakhturina
- Mohammad Shoeybi
- Bryan Catanzaro

## Summary
  Extending the context window of large language models (LLMs) is getting
popular recently, while the solution of augmenting LLMs with retrieval has
existed for years. The natural questions are: i) Retrieval-augmentation versus
long context window, which one is better for downstream tasks? ii) Can both
methods be combined to get the best of both worlds? In this work, we answer
these questions by studying both solutions using two state-of-the-art
pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps
surprisingly, we find that LLM with 4K context window using simple
retrieval-augmentation at generation can achieve comparable performance to
finetuned LLM with 16K context window via positional interpolation on long
context tasks, while taking much less computation. More importantly, we
demonstrate that retrieval can significantly improve the performance of LLMs
regardless of their extended context window sizes. Our best model,
retrieval-augmented LLaMA2-70B with 32K context window, outperforms
GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long
context tasks including question answering and query-based summarization. It
also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while
being much faster at generation. Our study provides general insights on the
choice of retrieval-augmentation versus long context extension of LLM for
practitioners.


# Human-oriented Representation Learning for Robotic Manipulation

[Link to the paper](http://arxiv.org/abs/2310.03023v1)

## Authors
- Mingxiao Huo
- Mingyu Ding
- Chenfeng Xu
- Thomas Tian
- Xinghao Zhu
- Yao Mu
- Lingfeng Sun
- Masayoshi Tomizuka
- Wei Zhan

## Summary
  Humans inherently possess generalizable visual representations that empower
them to efficiently explore and interact with the environments in manipulation
tasks. We advocate that such a representation automatically arises from
simultaneously learning about multiple simple perceptual skills that are
critical for everyday scenarios (e.g., hand detection, state estimate, etc.)
and is better suited for learning robot manipulation policies compared to
current state-of-the-art visual representations purely based on self-supervised
objectives. We formalize this idea through the lens of human-oriented
multi-task fine-tuning on top of pre-trained visual encoders, where each task
is a perceptual skill tied to human-environment interactions. We introduce Task
Fusion Decoder as a plug-and-play embedding translator that utilizes the
underlying relationships among these perceptual skills to guide the
representation learning towards encoding meaningful structure for what's
important for all perceptual skills, ultimately empowering learning of
downstream robotic manipulation tasks. Extensive experiments across a range of
robotic tasks and embodiments, in both simulations and real-world environments,
show that our Task Fusion Decoder consistently improves the representation of
three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for
downstream manipulation policy-learning. Project page:
https://sites.google.com/view/human-oriented-robot-learning


# Effects of feedback-free starburst galaxies on the 21-cm signal and reionization history

[Link to the paper](http://arxiv.org/abs/2310.03021v1)

## Authors
- Sarah Libanore
- Jordan Flitter
- Ely D. Kovetz
- Zhaozhou Li
- Avishai Dekel

## Summary
  Different star-formation models at Cosmic Dawn produce detectable signatures
in the observables of upcoming 21-cm experiments. In this work, we consider the
physical scenario of feedback-free starbursts (FFB), according to which the
star-formation efficiency (SFE) is enhanced in sufficiently massive halos at
early enough times, thus explaining the indication from the James Webb Space
Telescope for an excess of bright galaxies at $z \geq 10$. We model the
contribution of FFBs to popII SFE and compute the impact these have on the
21-cm global signal and power spectrum. We show that FFBs affect the evolution
of the brightness temperature and the 21-cm power spectrum, but they only have
a limited effect on the neutral hydrogen fraction. We investigate how the
observables are affected by changes in the underlying star formation model and
by contribution from popIII stars. Finally, we forecast the capability of
next-generation Hydrogen Epoch of Reionization Array (HERA) to detect the
existence of FFB galaxies via power spectrum measurements. Our results show the
possibility of a significant detection, provided that popII stars are the main
drivers of lowering the spin temperature. Efficient popIII star formation will
make the detection more challenging.


# Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models

[Link to the paper](http://arxiv.org/abs/2310.03020v1)

## Authors
- Jianglong Ye
- Peng Wang
- Kejie Li
- Yichun Shi
- Heng Wang

## Summary
  Zero-shot novel view synthesis (NVS) from a single image is an essential
problem in 3D object understanding. While recent approaches that leverage
pre-trained generative models can synthesize high-quality novel views from
in-the-wild inputs, they still struggle to maintain 3D consistency across
different views. In this paper, we present Consistent-1-to-3, which is a
generative framework that significantly mitigate this issue. Specifically, we
decompose the NVS task into two stages: (i) transforming observed regions to a
novel view, and (ii) hallucinating unseen regions. We design a scene
representation transformer and view-conditioned diffusion model for performing
these two stages respectively. Inside the models, to enforce 3D consistency, we
propose to employ epipolor-guided attention to incorporate geometry
constraints, and multi-view attention to better aggregate multi-view
information. Finally, we design a hierarchy generation paradigm to generate
long sequences of consistent views, allowing a full 360 observation of the
provided object image. Qualitative and quantitative evaluation over multiple
datasets demonstrate the effectiveness of the proposed mechanisms against
state-of-the-art approaches. Our project page is at
https://jianglongye.com/consistent123/


# Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages

[Link to the paper](http://arxiv.org/abs/2310.03018v1)

## Authors
- Kuan-Po Huang
- Chih-Kai Yang
- Yu-Kuan Fu
- Ewan Dunbar
- Hung-yi Lee

## Summary
  We introduce a new zero resource code-switched speech benchmark designed to
directly assess the code-switching capabilities of self-supervised speech
encoders. We showcase a baseline system of language modeling on discrete units
to demonstrate how the code-switching abilities of speech encoders can be
assessed in a zero-resource manner. Our experiments encompass a variety of
well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We
examine the impact of pre-training languages and model size on benchmark
performance. Notably, though our results demonstrate that speech encoders with
multilingual pre-training, exemplified by XLSR, outperform monolingual variants
(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial
room for improvement in their code-switching linguistic abilities.


# Multimodal Question Answering for Unified Information Extraction

[Link to the paper](http://arxiv.org/abs/2310.03017v1)

## Authors
- Yuxuan Sun
- Kai Zhang
- Yu Su

## Summary
  Multimodal information extraction (MIE) aims to extract structured
information from unstructured multimedia content. Due to the diversity of tasks
and settings, most current MIE models are task-specific and data-intensive,
which limits their generalization to real-world scenarios with diverse task
requirements and limited labeled data. To address these issues, we propose a
novel multimodal question answering (MQA) framework to unify three MIE tasks by
reformulating them into a unified span extraction and multi-choice QA pipeline.
Extensive experiments on six datasets show that: 1) Our MQA framework
consistently and significantly improves the performances of various
off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla
prompting. 2) In the zero-shot setting, MQA outperforms previous
state-of-the-art baselines by a large margin. In addition, the effectiveness of
our framework can successfully transfer to the few-shot setting, enhancing LMMs
on a scale of 10B parameters to be competitive or outperform much larger
language models such as ChatGPT and GPT-4. Our MQA framework can serve as a
general principle of utilizing LMMs to better solve MIE and potentially other
downstream multimodal tasks.


# Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions

[Link to the paper](http://arxiv.org/abs/2310.03016v1)

## Authors
- Satwik Bhattamishra
- Arkil Patel
- Phil Blunsom
- Varun Kanade

## Summary
  In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.


# Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day

[Link to the paper](http://arxiv.org/abs/2310.03015v1)

## Authors
- Yifan Jiang
- Hao Tang
- Jen-Hao Rick Chang
- Liangchen Song
- Zhangyang Wang
- Liangliang Cao

## Summary
  The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.


# The philosophical problems of implementing superselection rules

[Link to the paper](http://arxiv.org/abs/2310.03014v1)

## Authors
- Jorge Manero

## Summary
  Some physicists believe that superselection rules should be implemented to
get rid of inconsistencies when a theory is framed in terms of a new
mathematical formulation, whilst others think that this new formulation should
be modified instead of implementing those rules, at the expense of introducing
additional mathematical structure. The outcome, however, is that we are still
uncertain whether these rules should be implemented and how they should be
interpreted and assessed from the philosophical point of view. Based on a
detailed examination of the group-theoretic reformulation of (relativistic and
non-relativistic) quantum mechanics that prompts physicists to impose
superselection rules, I shall argue that the implementation of these rules
involves serious heuristic and epistemological concerns. Considering this
argument, I shall conclude that there are suitable philosophical reasons to
claim that the implementation of superselection rules should be rejected and
that there are certain circumstances when the formulation of a theory should be
modified.


# One Sense per Translation

[Link to the paper](http://arxiv.org/abs/2106.06082v2)

## Authors
- Bradley Hauer
- Grzegorz Kondrak

## Summary
  Word sense disambiguation (WSD) is the task of determining the sense of a
word in context. Translations have been used in WSD as a source of knowledge,
and even as a means of delimiting word senses. In this paper, we define three
theoretical properties of the relationship between senses and translations, and
argue that they constitute necessary conditions for using translations as sense
inventories. The key property of One Sense per Translation (OSPT) provides a
foundation for a translation-based WSD method. The results of an intrinsic
evaluation experiment indicate that our method achieves a precision of
approximately 93% compared to manual corpus annotations. Our extrinsic
evaluation experiments demonstrate WSD improvements of up to 4.6% F1-score on
difficult WSD datasets.


# Quantum algorithms: A survey of applications and end-to-end complexities

[Link to the paper](http://arxiv.org/abs/2310.03011v1)

## Authors
- Alexander M. Dalzell
- Sam McArdle
- Mario Berta
- Przemyslaw Bienias
- Chi-Fang Chen
- András Gilyén
- Connor T. Hann
- Michael J. Kastoryano
- Emil T. Khabiboulline
- Aleksander Kubica
- Grant Salton
- Samson Wang
- Fernando G. S. L. Brandão

## Summary
  The anticipated applications of quantum computers span across science and
industry, ranging from quantum chemistry and many-body physics to optimization,
finance, and machine learning. Proposed quantum solutions in these areas
typically combine multiple quantum algorithmic primitives into an overall
quantum algorithm, which must then incorporate the methods of quantum error
correction and fault tolerance to be implemented correctly on quantum hardware.
As such, it can be difficult to assess how much a particular application
benefits from quantum computing, as the various approaches are often sensitive
to intricate technical details about the underlying primitives and their
complexities. Here we present a survey of several potential application areas
of quantum algorithms and their underlying algorithmic primitives, carefully
considering technical caveats and subtleties. We outline the challenges and
opportunities in each area in an "end-to-end" fashion by clearly defining the
problem being solved alongside the input-output model, instantiating all
"oracles," and spelling out all hidden costs. We also compare quantum solutions
against state-of-the-art classical methods and complexity-theoretic limitations
to evaluate possible quantum speedups.
  The survey is written in a modular, wiki-like fashion to facilitate
navigation of the content. Each primitive and application area is discussed in
a standalone section, with its own bibliography of references and embedded
hyperlinks that direct to other relevant sections. This structure mirrors that
of complex quantum algorithms that involve several layers of abstraction, and
it enables rapid evaluation of how end-to-end complexities are impacted when
subroutines are altered.


# LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples

[Link to the paper](http://arxiv.org/abs/2310.01469v2)

## Authors
- Jia-Yu Yao
- Kun-Peng Ning
- Zhen-Hui Liu
- Mu-Nan Ning
- Li Yuan

## Summary
  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.


# SpiDy.jl -- open-source Julia package for the study of non-Markovian stochastic dynamics

[Link to the paper](http://arxiv.org/abs/2310.03008v1)

## Authors
- Stefano Scali
- Simon Horsley
- Janet Anders
- Federico Cerisola

## Summary
  SpiDy.jl solves the non-Markovian stochastic dynamics of interacting
classical spin vectors and harmonic oscillator networks in contact with a
dissipative environment. The methods implemented allow the user to include
arbitrary memory effects and colored quantum noise spectra. In this way,
SpiDy.jl provides key tools for the simulation of classical and quantum open
systems including non-Markovian effects and arbitrarily strong coupling to the
environment. Among the wide range of applications, some examples range from
atomistic spin dynamics to ultrafast magnetism and the study of anisotropic
materials. We provide the user with Julia notebooks to guide them through the
various mathematical methods and help them quickly setup complex simulations.


# Towards Domain-Specific Features Disentanglement for Domain Generalization

[Link to the paper](http://arxiv.org/abs/2310.03007v1)

## Authors
- Hao Chen
- Qi Zhang
- Zenan Huang
- Haobo Wang
- Junbo Zhao

## Summary
  Distributional shift between domains poses great challenges to modern machine
learning algorithms. The domain generalization (DG) signifies a popular line
targeting this issue, where these methods intend to uncover universal patterns
across disparate distributions. Noted, the crucial challenge behind DG is the
existence of irrelevant domain features, and most prior works overlook this
information. Motivated by this, we propose a novel contrastive-based
disentanglement method CDDG, to effectively utilize the disentangled features
to exploit the over-looked domain-specific features, and thus facilitating the
extraction of the desired cross-domain category features for DG tasks.
Specifically, CDDG learns to decouple inherent mutually exclusive features by
leveraging them in the latent space, thus making the learning discriminative.
Extensive experiments conducted on various benchmark datasets demonstrate the
superiority of our method compared to other state-of-the-art approaches.
Furthermore, visualization evaluations confirm the potential of our method in
achieving effective feature disentanglement.


# The measurement problem and scientific realism

[Link to the paper](http://arxiv.org/abs/2306.11627v2)

## Authors
- Jorge Alberto Manero Orozco
- Juan Alberto Guzman Garcia

## Summary
  Many attempts have been made to characterise and solve the infamous
measurement problem of quantum mechanics by advocating, implicitly or
explicitly, different realist perspectives. As a result, we are still uncertain
where this problem and its corresponding solution are to be located in the
realism-antirealism debate. On the basis of a well-known characterisation of
scientific realism, this paper intends to fill this gap by arguing that the
quantum description of the processes involved in typical measurements is
problematic from a minimal realist point of view, known as semantic realism.


# COOLer: Class-Incremental Learning for Appearance-Based Multiple Object Tracking

[Link to the paper](http://arxiv.org/abs/2310.03006v1)

## Authors
- Zhizheng Liu
- Mattia Segu
- Fisher Yu

## Summary
  Continual learning allows a model to learn multiple tasks sequentially while
retaining the old knowledge without the training data of the preceding tasks.
This paper extends the scope of continual learning research to
class-incremental learning for \ac{mot}, which is desirable to accommodate the
continuously evolving needs of autonomous systems. Previous solutions for
continual learning of object detectors do not address the data association
stage of appearance-based trackers, leading to catastrophic forgetting of
previous classes' re-identification features. We introduce COOLer, a
COntrastive- and cOntinual-Learning-based tracker, which incrementally learns
to track new categories while preserving past knowledge by training on a
combination of currently available ground truth labels and pseudo-labels
generated by the past tracker. To further exacerbate the disentanglement of
instance representations, we introduce a novel contrastive class-incremental
instance representation learning technique. Finally, we propose a practical
evaluation protocol for continual learning for MOT and conduct experiments on
the \bdd and \shift datasets. Experimental results demonstrate that COOLer
continually learns while effectively addressing catastrophic forgetting of both
tracking and detection. The code is available at
\url{https://github.com/BoSmallEar/COOLer}.


# Reversing Deep Face Embeddings with Probable Privacy Protection

[Link to the paper](http://arxiv.org/abs/2310.03005v1)

## Authors
- Daile Osorio-Roig
- Paul A. Gerlitz
- Christian Rathgeb
- Christoph Busch

## Summary
  Generally, privacy-enhancing face recognition systems are designed to offer
permanent protection of face embeddings. Recently, so-called soft-biometric
privacy-enhancement approaches have been introduced with the aim of canceling
soft-biometric attributes. These methods limit the amount of soft-biometric
information (gender or skin-colour) that can be inferred from face embeddings.
Previous work has underlined the need for research into rigorous evaluations
and standardised evaluation protocols when assessing privacy protection
capabilities. Motivated by this fact, this paper explores to what extent the
non-invertibility requirement can be met by methods that claim to provide
soft-biometric privacy protection. Additionally, a detailed vulnerability
assessment of state-of-the-art face embedding extractors is analysed in terms
of the transformation complexity used for privacy protection. In this context,
a well-known state-of-the-art face image reconstruction approach has been
evaluated on protected face embeddings to break soft biometric privacy
protection. Experimental results show that biometric privacy-enhanced face
embeddings can be reconstructed with an accuracy of up to approximately 98%,
depending on the complexity of the protection algorithm.


# Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization

[Link to the paper](http://arxiv.org/abs/2310.03004v1)

## Authors
- Tanmay Gautam
- Reid Pryzant
- Ziyi Yang
- Chenguang Zhu
- Somayeh Sojoudi

## Summary
  Vector Quantization (VQ) is a well-known technique in deep learning for
extracting informative discrete latent representations. VQ-embedded models have
shown impressive results in a range of applications including image and speech
generation. VQ operates as a parametric K-means algorithm that quantizes inputs
using a single codebook vector in the forward pass. While powerful, this
technique faces practical challenges including codebook collapse,
non-differentiability and lossy compression. To mitigate the aforementioned
issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for
VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the
forward pass, we solve for the optimal convex combination of codebook vectors
that quantize the inputs. In the backward pass, we leverage differentiability
through the optimality conditions of the forward solution. We then introduce a
scalable relaxation of the SCQ optimization and demonstrate its efficacy on the
CIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models
that significantly outperform matched VQ-based architectures, observing an
order of magnitude better image reconstruction and codebook usage with
comparable quantization runtime.


# Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models

[Link to the paper](http://arxiv.org/abs/2305.14585v2)

## Authors
- Andrew Engel
- Zhichao Wang
- Natalie S. Frank
- Ioana Dumitriu
- Sutanay Choudhury
- Anand Sarwate
- Tony Chiang

## Summary
  A recent trend in explainable AI research has focused on surrogate modeling,
where neural networks are approximated as simpler ML algorithms such as kernel
machines. A second trend has been to utilize kernel functions in various
explain-by-example or data attribution tasks to investigate a diverse set of
neural network behavior. In this work, we combine these two trends to analyze
approximate empirical neural tangent kernels (eNTK) for data attribution.
Approximation is critical for eNTK analysis due to the high computational cost
to compute the eNTK. We define new approximate eNTK and perform novel analysis
on how well the resulting kernel machine surrogate models correlate with the
underlying neural network. We introduce two new random projection variants of
approximate eNTK which allow users to tune the time and memory complexity of
their calculation. We conclude that kernel machines using approximate neural
tangent kernel as the kernel function are effective surrogate models, with the
introduced trace NTK the most consistent performer.


# From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference

[Link to the paper](http://arxiv.org/abs/2310.03003v1)

## Authors
- Siddharth Samsi
- Dan Zhao
- Joseph McDonald
- Baolin Li
- Adam Michaleas
- Michael Jones
- William Bergeron
- Jeremy Kepner
- Devesh Tiwari
- Vijay Gadepally

## Summary
  Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
  In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \& A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.


# Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks

[Link to the paper](http://arxiv.org/abs/2310.03001v1)

## Authors
- Felipe de Castro Teixeira Carvalho
- Kamaljyoti Nath
- Alberto Luiz Serpa
- George Em Karniadakis

## Summary
  Electrical submersible pumps (ESP) are the second most used artificial
lifting equipment in the oil and gas industry due to their high flow rates and
boost pressures. They often have to handle multiphase flows, which usually
contain a mixture of hydrocarbons, water, and/or sediments. Given these
circumstances, emulsions are commonly formed. It is a liquid-liquid flow
composed of two immiscible fluids whose effective viscosity and density differ
from the single phase separately. In this context, accurate modeling of ESP
systems is crucial for optimizing oil production and implementing control
strategies. However, real-time and direct measurement of fluid and system
characteristics is often impractical due to time constraints and economy.
Hence, indirect methods are generally considered to estimate the system
parameters. In this paper, we formulate a machine learning model based on
Physics-Informed Neural Networks (PINNs) to estimate crucial system parameters.
In order to study the efficacy of the proposed PINN model, we conduct
computational studies using not only simulated but also experimental data for
different water-oil ratios. We evaluate the state variable's dynamics and
unknown parameters for various combinations when only intake and discharge
pressure measurements are available. We also study structural and practical
identifiability analyses based on commonly available pressure measurements. The
PINN model could reduce the requirement of expensive field laboratory tests
used to estimate fluid properties.


# ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models

[Link to the paper](http://arxiv.org/abs/2310.02998v1)

## Authors
- Yi-Lin Sung
- Jaehong Yoon
- Mohit Bansal

## Summary
  Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
performance improvements on various multimodal downstream tasks. However,
deploying LVLMs is often problematic due to their massive computational/energy
costs and carbon consumption. Such issues make it infeasible to adopt
conventional iterative global pruning, which is costly due to computing the
Hessian matrix of the entire large model for sparsification. Alternatively,
several studies have recently proposed layer-wise pruning approaches to avoid
the expensive computation of global pruning and efficiently compress model
weights according to their importance within a layer. However, these methods
often suffer from suboptimal model compression due to their lack of a global
perspective. To address this limitation in recent efficient pruning methods for
large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),
a two-stage coarse-to-fine weight pruning approach for LVLMs. We first
determine the sparsity ratios of different layers or blocks by leveraging the
global importance score, which is efficiently computed based on the
zeroth-order approximation of the global model gradients. Then, the multimodal
model performs local layer-wise unstructured weight pruning based on
globally-informed sparsity ratios. We validate our proposed method across
various multimodal and unimodal models and datasets, demonstrating significant
performance improvements over prevalent pruning techniques in the high-sparsity
regime.


# Induced gravitational waves from flipped SU(5) superstring theory at $\mathrm{nHz}$

[Link to the paper](http://arxiv.org/abs/2309.15820v2)

## Authors
- Spyros Basilakos
- Dimitri V. Nanopoulos
- Theodoros Papanikolaou
- Emmanuel N. Saridakis
- Charalampos Tzerefos

## Summary
  The no-scale flipped SU(5) superstring framework constitutes a very promising
paradigm for physics below the Planck scale providing us with a very rich
cosmological phenomenology in accordance with observations. In particular, it
can accommodate Starobinsky-like inflation, followed by a reheating phase,
which is driven by a light "flaton" field, and during which the GUT phase
transition occurs. In this Letter, we extract for the first time a
gravitational-wave (GW) signal which naturally arises in the context of the
flipped SU(5) cosmological phenomenology and is related to the existence of an
early matter era (eMD) driven by the flaton field. Specifically, we study GWs
non-linearly induced by inflationary perturbations and which are abundantly
produced during a sudden transition from the flaton-driven eMD era to the
late-time radiation-dominated era. Remarkably, we find a GW signal with a
characteristic peak frequency $f_\mathrm{GW,peak}$ depending only on the string
slope $\alpha'$ and reading as $f_\mathrm{GW,peak} \propto 10^{-9}
\left(\frac{\alpha'}{\alpha'_*}\right)^4 \mathrm{Hz}$, where $\alpha'_{*}$ is
the fiducial string slope being related directly to the reduced Planck scale
$M_\mathrm{Pl}$ as $\alpha'_{*} = 8/M^2_\mathrm{Pl}$. Interestingly enough,
$f_\mathrm{GW,peak}$ lies within the $\mathrm{nHz}$ frequency range; hence
rendering this primordial GW signal potentially detectable by SKA, NANOGrav and
PTA probes at their very low frequency region of their detection bands.


# Trapped-ion quantum simulations for condensed-phase chemical dynamics: seeking a quantum advantage

[Link to the paper](http://arxiv.org/abs/2305.03156v2)

## Authors
- Mingyu Kang
- Hanggai Nuomin
- Sutirtha N. Chowdhury
- Jonathon L. Yuly
- Ke Sun
- Jacob Whitlow
- Jesús Valdiviezo
- Zhendian Zhang
- Peng Zhang
- David N. Beratan
- Kenneth R. Brown

## Summary
  Simulating the quantum dynamics of molecules in the condensed phase
represents a longstanding challenge in chemistry. Trapped-ion quantum systems
may serve as a platform for the analog-quantum simulation of chemical dynamics
that is beyond the reach of current classical-digital simulation. To identify a
"quantum advantage" for these simulations, performance analysis of both
analog-quantum simulation on noisy hardware and classical-digital algorithms is
needed. In this Review, we make a comparison between a noisy analog trapped-ion
simulator and a few choice classical-digital methods on simulating the dynamics
of a model molecular Hamiltonian with linear vibronic coupling. We describe
several simple Hamiltonians that are commonly used to model molecular systems,
which can be simulated with existing or emerging trapped-ion hardware. These
Hamiltonians may serve as stepping stones toward the use of trapped-ion
simulators for systems beyond the reach of classical-digital methods. Finally,
we identify dynamical regimes where classical-digital simulations seem to have
the weakest performance compared to analog-quantum simulations. These regimes
may provide the lowest hanging fruit to exploit potential quantum advantages.


# Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing

[Link to the paper](http://arxiv.org/abs/2310.02997v1)

## Authors
- Daile Osorio-Roig
- Mahdi Ghafourian
- Christian Rathgeb
- Ruben Vera-Rodriguez
- Christoph Busch
- Julian Fierrez

## Summary
  Nowadays, facial recognition systems are still vulnerable to adversarial
attacks. These attacks vary from simple perturbations of the input image to
modifying the parameters of the recognition model to impersonate an authorised
subject. So-called privacy-enhancing facial recognition systems have been
mostly developed to provide protection of stored biometric reference data, i.e.
templates. In the literature, privacy-enhancing facial recognition approaches
have focused solely on conventional security threats at the template level,
ignoring the growing concern related to adversarial attacks. Up to now, few
works have provided mechanisms to protect face recognition against adversarial
attacks while maintaining high security at the template level. In this paper,
we propose different key selection strategies to improve the security of a
competitive cancelable scheme operating at the signal level. Experimental
results show that certain strategies based on signal-level key selection can
lead to complete blocking of the adversarial attack based on an iterative
optimization for the most secure threshold, while for the most practical
threshold, the attack success chance can be decreased to approximately 5.0%.


# Generalized Stochastic Aggregative Game for Demand-Side Management in Microgrids with Shared Battery

[Link to the paper](http://arxiv.org/abs/2310.02996v1)

## Authors
- Shahram Yadollahi
- Hamed Kebriaei
- Sadegh Soudjani

## Summary
  In this paper, we focus on modeling and analysis of demand-side management in
a microgrid where agents utilize grid energy and a shared battery charged by
renewable energy sources. We model the problem as a generalized stochastic
dynamic aggregative game with chance constraints that capture the effects of
uncertainties in the renewable generation and agents' demands. Computing the
solution of the game is a complex task due to probabilistic and coupling
constraints among the agents through the state of charge of the shared battery.
We investigate the Nash equilibrium of this game under uncertainty considering
both the uniqueness of the solution and the effect of uncertainty on the
solution. Simulation results demonstrate that the presented stochastic method
is superior to deterministic methods.


# Kosmos-G: Generating Images in Context with Multimodal Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02992v1)

## Authors
- Xichen Pan
- Li Dong
- Shaohan Huang
- Zhiliang Peng
- Wenhu Chen
- Furu Wei

## Summary
  Recent advancements in text-to-image (T2I) and vision-language-to-image
(VL2I) generation have made significant strides. However, the generation from
generalized vision-language inputs, especially involving multiple images,
remains under-explored. This paper presents Kosmos-G, a model that leverages
the advanced perception capabilities of Multimodal Large Language Models
(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output
space of MLLM with CLIP using the textual modality as an anchor and performs
compositional instruction tuning on curated data. Kosmos-G demonstrates a
unique capability of zero-shot multi-entity subject-driven generation. Notably,
the score distillation instruction tuning requires no modifications to the
image decoder. This allows for a seamless substitution of CLIP and effortless
integration with a myriad of U-Net techniques ranging from fine-grained
controls to personalized image decoder variants. We posit Kosmos-G as an
initial attempt towards the goal of "image as a foreign language in image
generation."


# Diagnostic Tomography of Applied Holography

[Link to the paper](http://arxiv.org/abs/2310.02991v1)

## Authors
- D. V. Khveshchenko

## Summary
  The single-particle behavior in $d\geq 1$-dimensional Fermi gases with a
large number $N$ of species and strong short-range $s$-wave scattering is
discussed in the $2d$ 'tomographic' framework of a (pseudo)holographic
correspondence with a certain $3d$ gravity of the $AdS_3$ type. However, due to
the intrinsically topological nature of such a bulk theory its dynamics reduces
to a purely boundary one and so, akin to its $SYK/AdS_2$ counterpart, this
formal correspondence neither represents a genuine case of, nor endorses the
hypothetical generalized holographic duality.


# xVal: A Continuous Number Encoding for Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02989v1)

## Authors
- Siavash Golkar
- Mariel Pettee
- Michael Eickenberg
- Alberto Bietti
- Miles Cranmer
- Geraud Krawezik
- Francois Lanusse
- Michael McCabe
- Ruben Ohana
- Liam Parker
- Bruno Régaldo-Saint Blancard
- Tiberiu Tesileanu
- Kyunghyun Cho
- Shirley Ho

## Summary
  Large Language Models have not yet been broadly adapted for the analysis of
scientific datasets due in part to the unique difficulties of tokenizing
numbers. We propose xVal, a numerical encoding scheme that represents any real
number using just a single token. xVal represents a given real number by
scaling a dedicated embedding vector by the number value. Combined with a
modified number-inference approach, this strategy renders the model end-to-end
continuous when considered as a map from the numbers of the input string to
those of the output string. This leads to an inductive bias that is generally
more suitable for applications in scientific domains. We empirically evaluate
our proposal on a number of synthetic and real-world datasets. Compared with
existing number encoding schemes, we find that xVal is more token-efficient and
demonstrates improved generalization.


# Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples

[Link to the paper](http://arxiv.org/abs/2310.02988v1)

## Authors
- Phillip Howard
- Avinash Madasu
- Tiep Le
- Gustavo Lujan Moreno
- Vasudev Lal

## Summary
  While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes from existing
datasets. To address this challenge, we employ text-to-image diffusion models
to produce counterfactual examples for probing intserctional social biases at
scale. Our approach utilizes Stable Diffusion with cross attention control to
produce sets of counterfactual image-text pairs that are highly similar in
their depiction of a subject (e.g., a given occupation) while differing only in
their depiction of intersectional social attributes (e.g., race & gender). We
conduct extensive experiments using our generated dataset which reveal the
intersectional social biases present in state-of-the-art VLMs.


# Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions

[Link to the paper](http://arxiv.org/abs/2310.02987v1)

## Authors
- Xufeng Cai
- Ahmet Alacaoglu
- Jelena Diakonikolas

## Summary
  Machine learning approaches relying on such criteria as adversarial
robustness or multi-agent settings have raised the need for solving
game-theoretic equilibrium problems. Of particular relevance to these
applications are methods targeting finite-sum structure, which generically
arises in empirical variants of learning problems in these contexts. Further,
methods with computable approximation errors are highly desirable, as they
provide verifiable exit criteria. Motivated by these applications, we study
finite-sum monotone inclusion problems, which model broad classes of
equilibrium problems. Our main contributions are variants of the classical
Halpern iteration that employ variance reduction to obtain improved complexity
guarantees in which $n$ component operators in the finite sum are ``on
average'' either cocoercive or Lipschitz continuous and monotone, with
parameter $L$. The resulting oracle complexity of our methods, which provide
guarantees for the last iterate and for a (computable) operator norm residual,
is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improves
upon existing methods by a factor up to $\sqrt{n}$. This constitutes the first
variance reduction-type result for general finite-sum monotone inclusions and
for more specific problems such as convex-concave optimization when operator
norm residual is the optimality measure. We further argue that, up to
poly-logarithmic factors, this complexity is unimprovable in the monotone
Lipschitz setting; i.e., the provided result is near-optimal.


# Vector Field Based Volume Peeling for Multi-Axis Machining

[Link to the paper](http://arxiv.org/abs/2308.00472v2)

## Authors
- Neelotpal Dutta
- Tianyu Zhang
- Guoxin Fang
- Ismail E. Yigit
- Charlie C. L. Wang

## Summary
  This paper presents an easy-to-control volume peeling method for multi-axis
machining based on the computation taken on vector fields. The current scalar
field based methods are not flexible and the vector-field based methods do not
guarantee the satisfaction of the constraints in the final results. We first
conduct an optimization formulation to compute an initial vector field that is
well aligned with those anchor vectors specified by users according to
different manufacturing requirements. The vector field is further optimized to
be an irrotational field so that it can be completely realized by a scalar
field's gradients. Iso-surfaces of the scalar field will be employed as the
layers of working surfaces for multi-axis volume peeling in the rough
machining. Algorithms are also developed to remove and process singularities of
the fields. Our method has been tested on a variety of models and verified by
physical experimental machining.


# Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks

[Link to the paper](http://arxiv.org/abs/2310.02244v2)

## Authors
- Greg Yang
- Dingli Yu
- Chen Zhu
- Soufiane Hayou

## Summary
  By classifying infinite-width neural networks and identifying the *optimal*
limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,
for *widthwise hyperparameter transfer*, i.e., predicting optimal
hyperparameters of wide neural networks from narrow ones. Here we investigate
the analogous classification for *depthwise parametrizations* of deep residual
networks (resnets). We classify depthwise parametrizations of block multiplier
and learning rate by their infinite-width-then-depth limits. In resnets where
each block has only one layer, we identify a unique optimal parametrization,
called Depth-$\mu$P that extends $\mu$P and show empirically it admits
depthwise hyperparameter transfer. We identify *feature diversity* as a crucial
factor in deep networks, and Depth-$\mu$P can be characterized as maximizing
both feature learning and feature diversity. Exploiting this, we find that
absolute value, among all homogeneous nonlinearities, maximizes feature
diversity and indeed empirically leads to significantly better performance.
However, if each block is deeper (such as modern transformers), then we find
fundamental limitations in all possible infinite-depth limits of such
parametrizations, which we illustrate both theoretically and empirically on
simple networks as well as Megatron transformer trained on Common Crawl.


# Scaling Laws for Associative Memories

[Link to the paper](http://arxiv.org/abs/2310.02984v1)

## Authors
- Vivien Cabannes
- Elvis Dohmatob
- Alberto Bietti

## Summary
  Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.


# Are LLMs Useful in the Poorest Schools? theTeacherAI in Sierra Leone

[Link to the paper](http://arxiv.org/abs/2310.02982v1)

## Authors
- Jun Ho Choi
- Oliver Garrod
- Paul Atherton
- Andrew Joyce-Gibbons
- Miriam Mason-Sesay
- Daniel Björkegren

## Summary
  Education systems in developing countries have few resources to serve large,
poor populations. How might generative AI integrate into classrooms? This paper
introduces an AI chatbot designed to assist teachers in Sierra Leone with
professional development to improve their instruction. We describe initial
findings from early implementation across 122 schools and 193 teachers, and
analyze its use with qualitative observations and by analyzing queries.
Teachers use the system for lesson planning, classroom management, and subject
matter. A subset of teachers use the system intensively. We draw conclusions
from these findings about how generative AI systems can be integrated into
school systems in low income countries.


# Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors

[Link to the paper](http://arxiv.org/abs/2310.02980v1)

## Authors
- Ido Amos
- Jonathan Berant
- Ankit Gupta

## Summary
  Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.


# Reheating formulas in Quintessential Inflation via Gravitational Particle Production

[Link to the paper](http://arxiv.org/abs/2310.02245v2)

## Authors
- Jaume de Haro

## Summary
  We calculate the reheating temperature in scenarios where heavy particles are
gravitationally produced during a phase transition. We explore two distinct
situations: the decay of these particles both during and after the kination
phase. Subsequently, we determine the respective reheating temperatures.
Finally, we constrain these temperatures based on considerations related to the
overproduction of Gravitational Waves during the phase transition from the end
of inflation to the onset of kination.


# T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation

[Link to the paper](http://arxiv.org/abs/2310.02977v1)

## Authors
- Yuze He
- Yushi Bai
- Matthieu Lin
- Wang Zhao
- Yubin Hu
- Jenny Sheng
- Ran Yi
- Juanzi Li
- Yong-Jin Liu

## Summary
  Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.


# UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network

[Link to the paper](http://arxiv.org/abs/2310.02973v1)

## Authors
- Siddhant Arora
- Hayato Futami
- Jee-weon Jung
- Yifan Peng
- Roshan Sharma
- Yosuke Kashiwagi
- Emiru Tsunoo
- Shinji Watanabe

## Summary
  Recent studies have demonstrated promising outcomes by employing large
language models with multi-tasking capabilities. They utilize prompts to guide
the model's behavior and surpass performance of task-specific models. Motivated
by this, we ask: can we build a single model that jointly perform various
spoken language understanding (SLU) tasks? To address this, we utilize
pre-trained automatic speech recognition (ASR) models and employ various task
and dataset specifiers as discrete prompts. We demonstrate efficacy of our
single multi-task learning (MTL) model "UniverSLU" for 12 different speech
classification and sequence generation tasks across 17 datasets and 9
languages. Results show that UniverSLU achieves competitive performance and
even surpasses task-specific models. We also conduct preliminary investigations
into enabling human-interpretable natural phrases instead of task specifiers as
discrete prompts and test the model's generalization capabilities to new
paraphrases.


# Quantum linear polynomial evaluation based on XOR oblivious transfer compatible with classical partially homomorphic encryption

[Link to the paper](http://arxiv.org/abs/2305.11114v5)

## Authors
- Li Yu
- Jie Xu
- Fuqun Wang
- Chui-Ping Yang

## Summary
  XOR oblivious transfer is a universal cryptographic primitive that can be
related to linear polynomial evaluation. We firstly introduce some bipartite
quantum protocols for XOR oblivious transfer, which are not secure if one party
cheats, and some of them can be combined with a classical XOR homomorphic
encryption scheme for evaluation of linear polynomials modulo 2 with hybrid
security. We then introduce a general protocol using modified versions of the
XOR oblivious transfer protocols to evaluate linear polynomials modulo 2 with
partial information-theoretic security. When combined with the ability to
perform arbitrary quantum computation, this would lead to deterministic
interactive two-party computation which is quite secure in the
information-theoretic sense when the allowed set of inputs is large. For the
task of classical function evaluation, although the quantum computation
approach is still usable, we also discuss purely classical post-processing
methods based on the proposed linear polynomial evaluation protocols.


# Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk for Radiotherapy Planning of Nasopharyngeal Carcinoma

[Link to the paper](http://arxiv.org/abs/2310.02972v1)

## Authors
- Mehdi Astaraki
- Simone Bendazzoli
- Iuliana Toma-Dasu

## Summary
  Target segmentation in CT images of Head&Neck (H&N) region is challenging due
to low contrast between adjacent soft tissue. The SegRap 2023 challenge has
been focused on benchmarking the segmentation algorithms of Nasopharyngeal
Carcinoma (NPC) which would be employed as auto-contouring tools for radiation
treatment planning purposes. We propose a fully-automatic framework and develop
two models for a) segmentation of 45 Organs at Risk (OARs) and b) two Gross
Tumor Volumes (GTVs). To this end, we preprocess the image volumes by
harmonizing the intensity distributions and then automatically cropping the
volumes around the target regions. The preprocessed volumes were employed to
train a standard 3D U-Net model for each task, separately. Our method took
second place for each of the tasks in the validation phase of the challenge.
The proposed framework is available at https://github.com/Astarakee/segrap2023


# Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model

[Link to the paper](http://arxiv.org/abs/2310.02971v1)

## Authors
- Kai-Wei Chang
- Ming-Hsin Chen
- Yun-Ping Lin
- Jing Neng Hsu
- Paul Kuo-Ming Huang
- Chien-yu Huang
- Shang-Wen Li
- Hung-yi Lee

## Summary
  Prompting and adapter tuning have emerged as efficient alternatives to
fine-tuning (FT) methods. However, existing studies on speech prompting focused
on classification tasks and failed on more complex sequence generation tasks.
Besides, adapter tuning is primarily applied with a focus on encoder-only
self-supervised models. Our experiments show that prompting on Wav2Seq, a
self-supervised encoder-decoder model, surpasses previous works in sequence
generation tasks. It achieves a remarkable 53% relative improvement in word
error rate for ASR and a 27% in F1 score for slot filling. Additionally,
prompting competes with the FT method in the low-resource scenario. Moreover,
we show the transferability of prompting and adapter tuning on Wav2Seq in
cross-lingual ASR. When limited trainable parameters are involved, prompting
and adapter tuning consistently outperform conventional FT across 7 languages.
Notably, in the low-resource scenario, prompting consistently outperforms
adapter tuning.


# Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space

[Link to the paper](http://arxiv.org/abs/2310.02970v1)

## Authors
- Erik J Bekkers
- Sharvaree Vadgama
- Rob D Hesselink
- Putri A van der Linden
- David W Romero

## Summary
  Based on the theory of homogeneous spaces we derive \textit{geometrically
optimal edge attributes} to be used within the flexible message passing
framework. We formalize the notion of weight sharing in convolutional networks
as the sharing of message functions over point-pairs that should be treated
equally. We define equivalence classes of point-pairs that are identical up to
a transformation in the group and derive attributes that uniquely identify
these classes. Weight sharing is then obtained by conditioning message
functions on these attributes. As an application of the theory, we develop an
efficient equivariant group convolutional network for processing 3D point
clouds. The theory of homogeneous spaces tells us how to do group convolutions
with feature maps over the homogeneous space of positions $\mathbb{R}^3$,
position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$
itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to
the ability to represent directional information, which $\mathbb{R}^3$ methods
cannot, and it significantly enhances computational efficiency compared to
indexing features on the full SE$(3)$ group. We empirically support this claim
by reaching state-of-the-art results -- in accuracy and speed -- on three
different benchmarks: interatomic potential energy prediction, trajectory
forecasting in N-body systems, and generating molecules via equivariant
diffusion models.


# Structure transition and zigzag magnetic order in Ir/Rh-substituted honeycomb lattice RuCl3

[Link to the paper](http://arxiv.org/abs/2310.02965v1)

## Authors
- Zachary Morgan
- Iris Ye
- Colin L. Sarkis
- Xiaoping Wang
- Stephen Nagler
- Jiaqiang Yan

## Summary
  We report magnetization and neutron diffraction studies on crystal and
magnetic structures of Ir- and Rh-substituted honeycomb lattice
$\alpha$-RuCl$_3$. The iridium or rhodium atoms are distributed at the Ru site
with little structural modification. Both systems undergo a room-temperature
monoclinic $C2/m$ to low-temperature trigonal $R\bar{3}$ phase transformation
with a large recoverable hysteresis. At low temperature, a zigzag spin order is
observed with the same characteristic wavevector $(0,0.5,1)$ as in the parent
$\alpha$-RuCl$_3$. Detailed magnetic structure refinement reveals an ordered
moment of $\rm 0.32(5) \mu_B/Ru$ and a upper boundary of canting angle of
$15(4)^\circ$ away from the basal plane at 5~K for the 10\% Ir-substituted
$\alpha$-RuCl$_3$, which is different from the 0.45-0.73~$\rm \mu_B/Ru$ and
$32^\circ$-$48^\circ$ canting angle reported in the parent compound
$\alpha$-RuCl$_3$. The observation of unchanged RuCl$_6$ local octahedral
environment, reduced magnetic moment size and canting angle highlights the
potential to study quantum spin liquid behavior through non-magnetic ion
doping.


# Dynamics of a quantum polariton vortex: Low excitation scenario

[Link to the paper](http://arxiv.org/abs/2212.12010v2)

## Authors
- J. P. Restrepo Cuartas
- C. A. Flórez-Acosta
- J. D. Rodríguez-Durán
- D. G. Suárez-Forero
- William J. Herrera
- H. Vinck-Posada

## Summary
  Quantum vorticity in polariton systems has been traditionally investigated
within the frame of many-body phenomena under the mean-field or coherent
approaches. In the present work, we show that the fully quantized picture
describes richer dynamics for the vortex core at the quantum coupling limit,
where two systems exchange an indivisible excitation. The quantum correlations
intrinsic to our formalism account for the emergence of a family of
trajectories that differ from the circular paths known in macroscopic vorticity
phenomena. These results indicate that there exists a criterion to
differentiate the behavior at the edge between the quantum and classical
polariton vortex dynamics.


# P-wave Sommerfeld enhancement near threshold: a simplified approach

[Link to the paper](http://arxiv.org/abs/2208.13309v4)

## Authors
- Carlos Henrique de Lima
- Alberto Tonero
- Andres Vasquez
- Rogerio Rosenfeld

## Summary
  The calculation of P-wave Sommerfeld enhancement in processes with unstable
particles in the final state is known to be divergent. In a complete
description, where resonant (on-shell unstable particles) and non-resonant
contributions are included, it has been shown that results are finite. For most
beyond the Standard Model applications, these complete calculations are not
readily available. In this work, we are interested in the near-threshold region
and we consider only the resonant contribution. In this case, we provide a
simplified prescription to compute the P-wave Sommerfeld enhancement in the
narrow-width approximation of the unstable particle that directly eliminates
divergences. We show that we can define a finite resonant contribution without
the inclusion of the non-resonant processes in a way similar to the usual
S-wave Sommerfeld enhancement.


# Real-Time Risk Analysis with Optimization Proxies

[Link to the paper](http://arxiv.org/abs/2310.00709v2)

## Authors
- Wenbo Chen
- Mathieu Tanneau
- Pascal Van Hentenryck

## Summary
  The increasing penetration of renewable generation and distributed energy
resources requires new operating practices for power systems, wherein risk is
explicitly quantified and managed. However, traditional risk-assessment
frameworks are not fast enough for real-time operations, because they require
numerous simulations, each of which requires solving multiple economic dispatch
problems sequentially. The paper addresses this computational challenge by
proposing proxy-based risk assessment, wherein optimization proxies are trained
to learn the input-to-output mapping of an economic dispatch optimization
solver. Once trained, the proxies make predictions in milliseconds, thereby
enabling real-time risk assessment. The paper leverages self-supervised
learning and end-to-end-feasible architecture to achieve high-quality
sequential predictions. Numerical experiments on large systems demonstrate the
scalability and accuracy of the proposed approach.


# Measurement of the $\mathrm{t\bar{t}}$ charge asymmetry in events with highly Lorentz-boosted top quarks in pp collisions at $\sqrt{s}$ = 13 TeV

[Link to the paper](http://arxiv.org/abs/2208.02751v2)

## Authors
-  CMS Collaboration

## Summary
  The measurement of the charge asymmetry in top quark pair events with highly
Lorentz-boosted top quarks decaying to a single lepton and jets is presented.
The analysis is performed using proton-proton collisions at $\sqrt{s}$ = 13 TeV
with the CMS detector at the LHC and corresponding to an integrated luminosity
of 138 fb$^{-1}$. The selection is optimized for top quarks produced with large
Lorentz boosts, resulting in nonisolated leptons and overlapping jets. The top
quark charge asymmetry is measured for events with a $\mathrm{t\bar{t}}$
invariant mass larger than 750 GeV and corrected for detector and acceptance
effects using a binned maximum likelihood fit. The measured top quark charge
asymmetry of (0.42 $_{-0.69}^{+0.64}$)% is in good agreement with the standard
model prediction at next-to-next-to-leading order in quantum chromodynamic
perturbation theory with next-to-leading-order electroweak corrections. The
result is also presented for two invariant mass ranges, 750-900 and $\gt$ 900
GeV.


# DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models

[Link to the paper](http://arxiv.org/abs/2309.14509v2)

## Authors
- Sam Ade Jacobs
- Masahiro Tanaka
- Chengming Zhang
- Minjia Zhang
- Shuaiwen Leon Song
- Samyam Rajbhandari
- Yuxiong He

## Summary
  Computation in a typical Transformer-based large language model (LLM) can be
characterized by batch size, hidden dimension, number of layers, and sequence
length. Until now, system works for accelerating LLM training have focused on
the first three dimensions: data parallelism for batch size, tensor parallelism
for hidden size and pipeline parallelism for model depth or layers. These
widely studied forms of parallelism are not targeted or optimized for long
sequence Transformer models. Given practical application needs for long
sequence LLM, renewed attentions are being drawn to sequence parallelism.
However, existing works in sequence parallelism are constrained by
memory-communication inefficiency, limiting their scalability to long sequence
large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable
and effective methodology for enabling highly efficient and scalable LLM
training with extremely long sequence length. DeepSpeed-Ulysses at its core
partitions input data along the sequence dimension and employs an efficient
all-to-all collective communication for attention computation. Theoretical
communication analysis shows that whereas other methods incur communication
overhead as sequence length increases, DeepSpeed-Ulysses maintains constant
communication volume when sequence length and compute devices are increased
proportionally. Furthermore, experimental evaluations show that
DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the
existing method SOTA baseline.


# CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection

[Link to the paper](http://arxiv.org/abs/2310.02960v1)

## Authors
- Yang Cao
- Yihan Zeng
- Hang Xu
- Dan Xu

## Summary
  Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from an
arbitrary list of categories within a 3D scene, which remains seldom explored
in the literature. There are primarily two fundamental problems in OV-3DDet,
i.e., localizing and classifying novel objects. This paper aims at addressing
the two problems simultaneously via a unified framework, under the condition of
limited base categories. To localize novel 3D objects, we propose an effective
3D Novel Object Discovery strategy, which utilizes both the 3D box geometry
priors and 2D semantic open-vocabulary priors to generate pseudo box labels of
the novel objects. To classify novel object boxes, we further develop a
cross-modal alignment module based on discovered novel boxes, to align feature
spaces between 3D point cloud and image/text modalities. Specifically, the
alignment process contains a class-agnostic and a class-discriminative
alignment, incorporating not only the base objects with annotations but also
the increasingly discovered novel objects, resulting in an iteratively enhanced
alignment. The novel box discovery and crossmodal alignment are jointly learned
to collaboratively benefit each other. The novel object discovery can directly
impact the cross-modal alignment, while a better feature alignment can, in
turn, boost the localization capability, leading to a unified OV-3DDet
framework, named CoDA, for simultaneous novel object localization and
classification. Extensive experiments on two challenging datasets (i.e.,
SUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also show
a significant mAP improvement upon the best-performing alternative method by
80%. Codes and pre-trained models are released on the project page.


# Co-Optimizing Cache Partitioning and Multi-Core Task Scheduling: Exploit Cache Sensitivity or Not?

[Link to the paper](http://arxiv.org/abs/2310.02959v1)

## Authors
- Binqi Sun
- Debayan Roy
- Tomasz Kloda
- Andrea Bastoni
- Rodolfo Pellizzoni
- Marco Caccamo

## Summary
  Cache partitioning techniques have been successfully adopted to mitigate
interference among concurrently executing real-time tasks on multi-core
processors. Considering that the execution time of a cache-sensitive task
strongly depends on the cache available for it to use, co-optimizing cache
partitioning and task allocation improves the system's schedulability. In this
paper, we propose a hybrid multi-layer design space exploration technique to
solve this multi-resource management problem. We explore the interplay between
cache partitioning and schedulability by systematically interleaving three
optimization layers, viz., (i) in the outer layer, we perform a breadth-first
search combined with proactive pruning for cache partitioning; (ii) in the
middle layer, we exploit a first-fit heuristic for allocating tasks to cores;
and (iii) in the inner layer, we use the well-known recurrence relation for the
schedulability analysis of non-preemptive fixed-priority (NP-FP) tasks in a
uniprocessor setting. Although our focus is on NP-FP scheduling, we evaluate
the flexibility of our framework in supporting different scheduling policies
(NP-EDF, P-EDF) by plugging in appropriate analysis methods in the inner layer.
Experiments show that, compared to the state-of-the-art techniques, the
proposed framework can improve the real-time schedulability of NP-FP task sets
by an average of 15.2% with a maximum improvement of 233.6% (when tasks are
highly cache-sensitive) and a minimum of 1.6% (when cache sensitivity is low).
For such task sets, we found that clustering similar-period (or mutually
compatible) tasks often leads to higher schedulability (on average 7.6%) than
clustering by cache sensitivity. In our evaluation, the framework also achieves
good results for preemptive and dynamic-priority scheduling policies.


# What if Quantum Gravity is "just'' Quantum Information Theory?

[Link to the paper](http://arxiv.org/abs/2310.02958v1)

## Authors
- Aron C. Wall

## Summary
  I suggest the possibility that holographic quantum gravity is, in some sense,
equivalent to quantum information theory. Some radical implications would
follow. First, the theory of quantum gravity should have no adjustable coupling
constants, similar to string theory. Thus, all complete bulk theories of
quantum gravity are dual to each other. By setting up an appropriately
entangled state, it should be possible to find wormholes connecting any two
quantum gravity theories (e.g. string theory and loop quantum gravity).
Secondly, if we represent space at one time as a tensor network, then dynamics
is automatically encoded via gauge-equivalent descriptions of the boundary
state. This would appear to imply, contrary to semiclassical expectations, that
a closed universe should have only one state.


# Entropy Cost of "Erasure" in Physically Irreversible Processes

[Link to the paper](http://arxiv.org/abs/2307.02643v3)

## Authors
- R. E. Kastner
- Andreas Schlatter

## Summary
  A restricted form of Landauer's Principle, independent of computational
considerations, is shown to hold for thermal systems by reference to the joint
entropy associated with conjugate observables. It is shown that the source of
the compensating entropy for irreversible physical processes is due to the
ontological uncertainty attending values of such mutually incompatible
observables, rather than due to epistemic uncertainty as traditionally assumed
in the information-theoretic approach. In particular, it is explicitly shown
that erasure of logical (epistemic) information via reset operations is not
equivalent to erasure of thermodynamic entropy, so that the traditional,
information-theoretic form of Landauer's Principle is not supported by the
physics. A further implication of the analysis is that there is no Maxwell's
Demon in the real world.


# Prescribed Robustness in Optimal Power Flow

[Link to the paper](http://arxiv.org/abs/2310.02957v1)

## Authors
- Robert Mieth
- H. Vincent Poor

## Summary
  For a timely decarbonization of our economy, power systems need to
accommodate increasing numbers of clean but stochastic resources. This requires
new operational methods that internalize this stochasticity to ensure safety
and efficiency. This paper proposes a novel approach to compute adaptive safety
intervals for each stochastic resource that internalize power flow physics and
optimize the expected cost of system operations, making them ``prescriptive''.
The resulting intervals are interpretable and can be used in a tractable robust
optimal power flow problem as uncertainty sets. We use stochastic gradient
descent with differentiable optimization layers to compute a mapping that
obtains these intervals from a given vector of context parameters that captures
the expected system state. We demonstrate and discuss the proposed approach on
two case studies.


# DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning

[Link to the paper](http://arxiv.org/abs/2310.02954v1)

## Authors
- Jiong Xiong
- Zixuan Li
- Chuanyang Zheng
- Zhijiang Guo
- Yichun Yin
- Enze Xie
- Zhicheng Yang
- Qingxing Cao
- Haiming Wang
- Xiongwei Han
- Jing Tang
- Chengming Li
- Xiaodan Liang

## Summary
  Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question's knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.


# JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning

[Link to the paper](http://arxiv.org/abs/2310.02953v1)

## Authors
- Chang Gao
- Wenxuan Zhang
- Guizhen Chen
- Wai Lam

## Summary
  Instruction tuning has emerged as a crucial process for harnessing the
capabilities of large language models (LLMs) by providing explicit task
instructions, leading to improved performance in various tasks. However,
prevalent text-to-text instruction tuning (TextTuning) methods suffer from
limitations in generalization, robustness, and controllability due to the
ambiguity and lack of explicit structure in tasks. In this paper, we propose
JsonTuning, a novel structure-to-structure approach for instruction tuning. By
leveraging the versatility and structured nature of JSON to represent tasks,
JsonTuning enhances generalization by helping the model understand essential
task elements and their relations, improves robustness by minimizing ambiguity,
and increases controllability by providing explicit control over the output. We
conduct a comprehensive comparative study with diverse language models and
evaluation benchmarks. Experimental results show that JsonTuning outperforms
TextTuning in various applications, showcasing improved performance,
adaptability, robustness, and controllability. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for more effective
and reliable LLMs capable of handling diverse scenarios.


# Some more theorems on structural entailment relations and non-deterministic semantics

[Link to the paper](http://arxiv.org/abs/2310.02952v1)

## Authors
- Carlos Caleiro
- Sérgio Marcelino
- Umberto Rivieccio

## Summary
  We extend classical work by Janusz Czelakowski on the closure properties of
the class of matrix models of entailment relations - nowadays more commonly
called multiple-conclusion logics - to the setting of non-deterministic
matrices (Nmatrices), characterizing the Nmatrix models of an arbitrary logic
through a generalization of the standard class operators to the
non-deterministic setting. We highlight the main differences that appear in
this more general setting, in particular: the possibility to obtain Nmatrix
quotients using any compatible equivalence relation (not necessarily a
congruence); the problem of determining when strict homomorphisms preserve the
logic of a given Nmatrix; the fact that the operations of taking images and
preimages cannot be swapped, which determines the exact sequence of operators
that generates, from any complete semantics, the class of all Nmatrix models of
a logic. Many results, on the other hand, generalize smoothly to the
non-deterministic setting: we show for instance that a logic is finitely based
if and only if both the class of its Nmatrix models and its complement are
closed under ultraproducts. We conclude by mentioning possible developments in
adapting the Abstract Algebraic Logic approach to logics induced by Nmatrices
and the associated equational reasoning over non-deterministic algebras.


# A Novel Approach with Monte-Carlo Simulation and Hybrid Optimization Approach for Inventory Management with Stochastic Demand

[Link to the paper](http://arxiv.org/abs/2310.01079v2)

## Authors
- Sarit Maitra
- Vivek Mishra
- Sukanya Kundu

## Summary
  This study addresses the difficulties associated with inventory management of
products with stochastic demand. The objective is to find the optimal
combination of order quantity and reorder point that maximizes profit while
considering ethical considerations in inventory management. The ethical
considerations are risk assessment, social responsibility, environmental
sustainability, and customer satisfaction. Monte Carlo simulation (MCS) is used
in this study to generate a distribution of demand and lead times for the
inventory items, which is then used to estimate the potential profit and risk
associated with different inventory policies. This work proposes a hybrid
optimization approach combining Gaussian process regression and conditioning
function to efficiently search the high-dimensional space of potential
continuous review (r, Q) and periodic review (p, Q) values to find the optimal
combination that maximizes profit while considering ethical considerations. The
findings show that both the (r, Q) and (p, Q) approaches can effectively manage
inventory with stochastic demand, but the (r, Q) approach performs better
(profits up by 12.73%) when demand is more volatile. The study adds
quantifiable risk assessment and sensitivity analysis to these considerations,
considering the variation in demand and expected output in profit percentage.
The results provide useful information for making ethical and responsible
choices in supply chain analytics, boosting efficiency and profits.


# Evanescent Electron Wave Spin

[Link to the paper](http://arxiv.org/abs/2309.17325v2)

## Authors
- Ju Gao
- Fang Shen

## Summary
  We demonstrate that an evanescent wave spin exists outside a finite quantum
well by solving the Dirac equation in a finite cylindrical quantum well. The
analytical analysis validates the wavefunction inside an infinite quantum well
but recovers a non-zero evanescent wave outside the well. We propose that it is
possible to probe or eavesdrop on quantum spin information through the
evanescent wave spin without destroying the entire spin state. We argue that a
spin-based quantum process or device is deterministic rather than
probabilistic.


# Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models

[Link to the paper](http://arxiv.org/abs/2310.02949v1)

## Authors
- Xianjun Yang
- Xiao Wang
- Qi Zhang
- Linda Petzold
- William Yang Wang
- Xun Zhao
- Dahua Lin

## Summary
  Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.


# HappyFeat -- An interactive and efficient BCI framework for clinical applications

[Link to the paper](http://arxiv.org/abs/2310.02948v1)

## Authors
- Arthur Desbois
- Tristan Venot
- Fabrizio De Vico Fallani
- Marie-Constance Corsi

## Summary
  Brain-Computer Interface (BCI) systems allow users to perform actions by
translating their brain activity into commands. Such systems usually need a
training phase, consisting in training a classification algorithm to
discriminate between mental states using specific features from the recorded
signals. This phase of feature selection and training is crucial for BCI
performance and presents specific constraints to be met in a clinical context,
such as post-stroke rehabilitation.
  In this paper, we present HappyFeat, a software making Motor Imagery (MI)
based BCI experiments easier, by gathering all necessary manipulations and
analysis in a single convenient GUI and via automation of experiment or
analysis parameters. The resulting workflow allows for effortlessly selecting
the best features, helping to achieve good BCI performance in time-constrained
environments. Alternative features based on Functional Connectivity can be used
and compared or combined with Power Spectral Density, allowing a
network-oriented approach.
  We then give details of HappyFeat's main mechanisms, and a review of its
performances in typical use cases. We also show that it can be used as an
efficient tool for comparing different metrics extracted from the signals, to
train the classification algorithm. To this end, we show a comparison between
the commonly-used Power Spectral Density and network metrics based on
Functional Connectivity.
  HappyFeat is available as an open-source project which can be freely
downloaded on GitHub.


# DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text

[Link to the paper](http://arxiv.org/abs/2305.17359v2)

## Authors
- Xianjun Yang
- Wei Cheng
- Yue Wu
- Linda Petzold
- William Yang Wang
- Haifeng Chen

## Summary
  Large language models (LLMs) have notably enhanced the fluency and diversity
of machine-generated text. However, this progress also presents a significant
challenge in detecting the origin of a given text, and current research on
detection methods lags behind the rapid evolution of LLMs. Conventional
training-based methods have limitations in flexibility, particularly when
adapting to new domains, and they often lack explanatory power. To address this
gap, we propose a novel training-free detection strategy called Divergent
N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and
then use only the preceding portion as input to the LLMs to regenerate the new
remaining parts. By analyzing the differences between the original and new
remaining parts through N-gram analysis in black-box or probability divergence
in white-box, we unveil significant discrepancies between the distribution of
machine-generated text and the distribution of human-written text. We conducted
extensive experiments on the most advanced LLMs from OpenAI, including
text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such
as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach
exhibits state-of-the-art performance in distinguishing between human and
GPT-generated text on four English and one German dataset, outperforming
OpenAI's own classifier, which is trained on millions of text. Additionally,
our methods provide reasonable explanations and evidence to support our claim,
which is a unique feature of explainable detection. Our method is also robust
under the revised text attack and can additionally solve model sourcing. Codes
are available at https://github.com/Xianjun-Yang/DNA-GPT.


# Structural Adversarial Objectives for Self-Supervised Representation Learning

[Link to the paper](http://arxiv.org/abs/2310.00357v2)

## Authors
- Xiao Zhang
- Michael Maire

## Summary
  Within the framework of generative adversarial networks (GANs), we propose
objectives that task the discriminator for self-supervised representation
learning via additional structural modeling responsibilities. In combination
with an efficient smoothness regularizer imposed on the network, these
objectives guide the discriminator to learn to extract informative
representations, while maintaining a generator capable of sampling from the
domain. Specifically, our objectives encourage the discriminator to structure
features at two levels of granularity: aligning distribution characteristics,
such as mean and variance, at coarse scales, and grouping features into local
clusters at finer scales. Operating as a feature learner within the GAN
framework frees our self-supervised system from the reliance on hand-crafted
data augmentation schemes that are prevalent across contrastive representation
learning methods. Across CIFAR-10/100 and an ImageNet subset, experiments
demonstrate that equipping GANs with our self-supervised objectives suffices to
produce discriminators which, evaluated in terms of representation learning,
compete with networks trained by contrastive learning approaches.


# Machine Learning Inference on Inequality of Opportunity

[Link to the paper](http://arxiv.org/abs/2206.05235v3)

## Authors
- Juan Carlos Escanciano
- Joël Robert Terschuur

## Summary
  Equality of opportunity has emerged as an important ideal of distributive
justice. Empirically, Inequality of Opportunity (IOp) is measured in two steps:
first, an outcome (e.g., income) is predicted given individual circumstances;
and second, an inequality index (e.g., Gini) of the predictions is computed.
Machine Learning (ML) methods are tremendously useful in the first step.
However, they can cause sizable biases in IOp since the bias-variance trade-off
allows the bias to creep in the second step. We propose a simple debiased IOp
estimator robust to such ML biases and provide the first valid inferential
theory for IOp. We demonstrate improved performance in simulations and report
the first unbiased measures of income IOp in Europe. Mother's education and
father's occupation are the circumstances that explain the most. Plug-in
estimators are very sensitive to the ML algorithm, while debiased IOp
estimators are robust. These results are extended to a general U-statistics
setting.


# Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments

[Link to the paper](http://arxiv.org/abs/2310.02944v1)

## Authors
- Corey Knutson
- Zhipeng Cao
- Junaed Sattar

## Summary
  Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the
need for human intervention. A docking station (DS) can extend mission times of
an AUV by providing a location for the AUV to recharge its batteries and
receive updated mission information. Various methods for locating and tracking
a DS exist, but most rely on expensive acoustic sensors, or are vision-based,
which is significantly affected by water quality. In this \doctype, we present
a vision-based method that utilizes adaptive color LED markers and dynamic
color filtering to maximize landmark visibility in varying water conditions.
Both AUV and DS utilize cameras to determine the water background color in
order to calculate the desired marker color. No communication between AUV and
DS is needed to determine marker color. Experiments conducted in a pool and
lake show our method performs 10 times better than static color thresholding
methods as background color varies. DS detection is possible at a range of 5
meters in clear water with minimal false positives.


# LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models

[Link to the paper](http://arxiv.org/abs/2310.02943v1)

## Authors
- Aleksandr Meister
- Matvei Novikov
- Nikolay Karpov
- Evelina Bakhturina
- Vitaly Lavrukhin
- Boris Ginsburg

## Summary
  Traditional automatic speech recognition (ASR) models output lower-cased
words without punctuation marks, which reduces readability and necessitates a
subsequent text processing model to convert ASR transcripts into a proper
format. Simultaneously, the development of end-to-end ASR models capable of
predicting punctuation and capitalization presents several challenges,
primarily due to limited data availability and shortcomings in the existing
evaluation methods, such as inadequate assessment of punctuation prediction. In
this paper, we introduce a LibriSpeech-PC benchmark designed to assess the
punctuation and capitalization prediction capabilities of end-to-end ASR
models. The benchmark includes a LibriSpeech-PC dataset with restored
punctuation and capitalization, a novel evaluation metric called Punctuation
Error Rate (PER) that focuses on punctuation marks, and initial baseline
models. All code, data, and models are publicly available.


# (2,0) theory on $S^5 \times S^1$ and quantum M2 branes

[Link to the paper](http://arxiv.org/abs/2309.10786v2)

## Authors
- Matteo Beccaria
- Simone Giombi
- Arkady A. Tseytlin

## Summary
  The superconformal index $Z$ of the 6d (2,0) theory on $S^5 \times S^1$
(which is related to the localization partition function of 5d SYM on $S^5$)
should be captured at large $N$ by the quantum M2 brane theory in the dual
M-theory background. Generalizing the type IIA string theory limit of this
relation discussed in arXiv:2111.15493 and arXiv:2304.12340, we consider
semiclassically quantized M2 branes in a half-supersymmetric 11d background
which is a twisted product of thermal AdS$_7$ and $S^4$. We show that the
leading non-perturbative term at large $N$ is reproduced precisely by the
1-loop partition function of an "instanton" M2 brane wrapped on $S^1\times S^2$
with $S^2\subset S^4$. Similarly, the (2,0) theory analog of the BPS Wilson
loop expectation value is reproduced by the partition function of a "defect" M2
brane wrapped on thermal AdS$_3\subset$ AdS$_7$. We comment on a curious
analogy of these results with similar computations in arXiv:2303.15207 and
arXiv:2307.14112 of the partition function of quantum M2 branes in AdS$_4
\times S^7/\mathbb Z_k$ which reproduced the corresponding localization
expressions in the ABJM 3d gauge theory.


# Image-based Navigation in Real-World Environments via Multiple Mid-level Representations: Fusion Models, Benchmark and Efficient Evaluation

[Link to the paper](http://arxiv.org/abs/2202.01069v2)

## Authors
- Marco Rosano
- Antonino Furnari
- Luigi Gulino
- Corrado Santoro
- Giovanni Maria Farinella

## Summary
  Navigating complex indoor environments requires a deep understanding of the
space the robotic agent is acting into to correctly inform the navigation
process of the agent towards the goal location. In recent learning-based
navigation approaches, the scene understanding and navigation abilities of the
agent are achieved simultaneously by collecting the required experience in
simulation. Unfortunately, even if simulators represent an efficient tool to
train navigation policies, the resulting models often fail when transferred
into the real world. One possible solution is to provide the navigation model
with mid-level visual representations containing important domain-invariant
properties of the scene. But, what are the best representations that facilitate
the transfer of a model to the real-world? How can they be combined? In this
work we address these issues by proposing a benchmark of Deep Learning
architectures to combine a range of mid-level visual representations, to
perform a PointGoal navigation task following a Reinforcement Learning setup.
All the proposed navigation models have been trained with the Habitat simulator
on a synthetic office environment and have been tested on the same real-world
environment using a real robotic platform. To efficiently assess their
performance in a real context, a validation tool has been proposed to generate
realistic navigation episodes inside the simulator. Our experiments showed that
navigation models can benefit from the multi-modal input and that our
validation tool can provide good estimation of the expected navigation
performance in the real world, while saving time and resources. The acquired
synthetic and real 3D models of the environment, together with the code of our
validation tool built on top of Habitat, are publicly available at the
following link: https://iplab.dmi.unict.it/EmbodiedVN/


# Hidden symmetry in interacting-quantum-dot-based multi-terminal Josephson junctions

[Link to the paper](http://arxiv.org/abs/2310.02933v1)

## Authors
- Peter Zalom
- Martin Žonda
- T. Novotný

## Summary
  We study a multi-terminal Josephson junction based on an interacting quantum
dot coupled to n superconducting BCS leads. Using an Anderson type model of a
local level with an arbitrary onsite Coulomb repulsion, we uncover its
surprising equivalence with an effective two-terminal junction with symmetric
couplings to appropriately phase-biased leads. Regardless of the strength of
the Coulomb interaction, this hidden symmetry enables us to apply
well-established numerical and theoretical tools for exact evaluation of
various physical quantities, and imposes strict relations among them. Focusing
on three-terminal devices, we then demonstrate several phenomena such as the
existence of the finite energy band crossings, superconducting transistor and
superconducting diode effect, as well as current phase relation modulation.
Brief analysis of four-terminal devices is also given.


# Assessing Large Language Models on Climate Information

[Link to the paper](http://arxiv.org/abs/2310.02932v1)

## Authors
- Jannis Bulian
- Mike S. Schäfer
- Afra Amini
- Heidi Lam
- Massimiliano Ciaramita
- Ben Gaiarin
- Michelle Chen Huebscher
- Christian Buck
- Niels Mede
- Markus Leippold
- Nadine Strauss

## Summary
  Understanding how climate change affects us and learning about available
solutions are key steps toward empowering individuals and communities to
mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,
it is necessary to assess their capability in this domain. In this study, we
present a comprehensive evaluation framework, grounded in science communication
principles, to analyze LLM responses to climate change topics. Our framework
emphasizes both the presentational and epistemological adequacy of answers,
offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our
framework discerns up to 30 distinct issues in model outputs. The task is a
real-world example of a growing number of challenging problems where AI can
complement and lift human performance. We introduce a novel and practical
protocol for scalable oversight that uses AI Assistance and relies on raters
with relevant educational backgrounds. We evaluate several recent LLMs and
conduct a comprehensive analysis of the results, shedding light on both the
potential and the limitations of LLMs in the realm of climate communication.


# Graph data modelling for outcome prediction in oropharyngeal cancer patients

[Link to the paper](http://arxiv.org/abs/2310.02931v1)

## Authors
- Nithya Bhasker
- Stefan Leger
- Alexander Zwanenburg
- Chethan Babu Reddy
- Sebastian Bodenstedt
- Steffen Löck
- Stefanie Speidel

## Summary
  Graph neural networks (GNNs) are becoming increasingly popular in the medical
domain for the tasks of disease classification and outcome prediction. Since
patient data is not readily available as a graph, most existing methods either
manually define a patient graph, or learn a latent graph based on pairwise
similarities between the patients. There are also hypergraph neural network
(HGNN)-based methods that were introduced recently to exploit potential higher
order associations between the patients by representing them as a hypergraph.
In this work, we propose a patient hypergraph network (PHGN), which has been
investigated in an inductive learning setup for binary outcome prediction in
oropharyngeal cancer (OPC) patients using computed tomography (CT)-based
radiomic features for the first time. Additionally, the proposed model was
extended to perform time-to-event analyses, and compared with GNN and baseline
linear models.


# Giant Activity-Induced Stress Plateau in Entangled Polymer Solutions

[Link to the paper](http://arxiv.org/abs/2310.02929v1)

## Authors
- Davide Breoni
- Christina Kurzthaler
- Benno Liebchen
- Hartmut Löwen
- Suvendu Mandal

## Summary
  We study the viscoelastic properties of highly entangled, flexible,
self-propelled polymers using Brownian dynamics simulations. Our results show
that the active motion of the polymer increases the height of the stress
plateau by orders of magnitude due to the emergence of grip forces at
entanglement points. Identifying the activity-induced energy of a single
polymer and the ratio of polymer length to self-propulsion velocity as relevant
energy and time scales, we find the stress autocorrelation functions collapse
across P\'eclet numbers. We predict that the long-time viscosity scales with
polymer length squared $\sim L^2$, in contrast to equilibrium counterparts
$\sim L^3$. These insights offer prospects for designing new materials with
activity-responsive mechanical properties.


# Probing statistical isotropy of the universe with the Planck CMB data

[Link to the paper](http://arxiv.org/abs/2310.02928v1)

## Authors
- C. E. Kester
- A. Bernui
- W. S. Hipólito-Ricaldi

## Summary
  We study the angular distribution of the cosmic microwave background (CMB)
temperature fluctuations to probe the statistical isotropy of the universe by
using precise full-sky CMB data with a model-independent approach. We
investigate the temperature-temperature angular correlations in the four Planck
foreground-cleaned CMB maps, recently released. This study performs a
directional analysis on the CMB sphere looking for directions where
temperature-temperature angular correlations are extremes. Our analyses confirm
a preferred axis in the CMB sphere, pointing in the direction $(l,b) \simeq
(250^{\circ}, 130^{\circ})$, at $98\%$-$99\%$ confidence level, direction where
the CMB angular correlations show the maximum excess over the antipodal
direction. This preferred direction is unexpected in the $\Lambda$CDM
cosmological model, and represents a significant deviation compared to results
obtained applying the same procedure to simulated statistically isotropic CMB
maps. This result confirms the North-South asymmetry in the most recent Planck
data, a phenomenon that is one of the previously reported CMB anomalies. We
perform a robust detection of the North-South asymmetry in the
temperature-temperature angular correlations -- with slightly different
statistical significance -- in the four Planck foreground-cleaned CMB maps.
Moreover, we perform consistency tests by adding foregrounds and noise, both
Planck data products, to the CMB map in study, and also investigate and discard
possible bias in our methodology. After these detailed analyses we conclude
that the North-South asymmetry phenomenon is present, with high statistical
significance, in the Planck CMB maps studied, result that confirms previous
reports in the literature in the last 20 years.


# Joint Network Lifetime Maximization and Relay Selection Design in Underwater Acoustic Sensor Networks

[Link to the paper](http://arxiv.org/abs/2310.02927v1)

## Authors
- Z. Mohammadi
- M. Soleimanpour-Moghadam
- S. Talebi
- H. Ahmadi

## Summary
  The paper proposes a new approach to minimize the number of relays while
maximizing the lifetime of underwater acoustic sensor networks (UASNs). This
involves formulating the relay node placement (RNP) problem as a
multi-objective optimization problem and employing the multi-objective
lexico-graphic method (MOLM) to solve it. To achieve the optimal solution, the
MOLM consists of two steps. First, the problem of lifetime maximization is
tackled to find RNP solutions. This transforms the RNP into a non-convex
optimization problem which is then converted into a convex programming
equivalent. The proposed method has the same computational complexity as
previous relay-node adjustment (RA) and difference convex algorithm (DCA)
methods. The second step introduces a novel relay node selection to reach the
optimal number of relays. Simulation results demonstrate that it has superior
network lifetime and efficiency compared to RA and DCA.


# Extensions to the SENSEI In situ Framework for Heterogeneous Architectures

[Link to the paper](http://arxiv.org/abs/2310.02926v1)

## Authors
- Burlen Loring
- E. Wes Bethel
- Gunther H. Weber
- Michael W. Mahoney

## Summary
  The proliferation of GPUs and accelerators in recent supercomputing systems,
so called heterogeneous architectures, has led to increased complexity in
execution environments and programming models as well as to deeper memory
hierarchies on these systems. In this work, we discuss challenges that arise in
in situ code coupling on these heterogeneous architectures. In particular, we
present data and execution model extensions to the SENSEI in situ framework
that are targeted at the effective use of systems with heterogeneous
architectures. We then use these new data and execution model extensions to
investigate several in situ placement and execution configurations and to
analyze the impact these choices have on overall performance.


# DecisionProgramming.jl --A framework for modelling decision problems using mathematical programming

[Link to the paper](http://arxiv.org/abs/2307.13299v2)

## Authors
- Helmi Hankimaa
- Olli Herrala
- Fabricio Oliveira
- Jaan Tollander de Balsch

## Summary
  We present DecisionProgramming.jl, a new Julia package for modelling decision
problems as mixed-integer programming (MIP) equivalents. The package allows the
user to pose decision problems as influence diagrams which are then
automatically converted to an equivalent MIP formulation. This MIP formulation
is implemented using JuMP.jl, a Julia package providing an algebraic syntax for
formulating mathematical programming problems. In this paper, we show novel MIP
formulations used in the package, which considerably improve the computational
performance of the MIP solver. We also present a novel heuristic that can be
employed to warm start the solution, as well as provide heuristic solutions to
more computationally challenging problems. Lastly, we describe a novel case
study showcasing decision programming as an alternative framework for modelling
multi-stage stochastic dynamic programming problems.


# Fusion-stable structures on triangulation categories

[Link to the paper](http://arxiv.org/abs/2310.02917v1)

## Authors
- Yu Qiu
- Xiaoting Zhang

## Summary
  Let $\mathcal{G}$ be a fusion category acting on a triangulated category
$\mathcal{D}$, in the sense that $\mathcal{D}$ is a $\mathcal{G}$-module
category. Our motivation example is fusion-weighted species, which is
essentially Heng's construction. We study $\mathcal{G}$-stable tilting, cluster
and stability structures on $\mathcal{D}$. In particular, we prove the
deformation theorem for $\mathcal{G}$-stable stability conditions.
  A first application is that Duffield-Tumarkin's categorification of cluster
exchange graphs of finite Coxeter-Dynkin type can be naturally realized as
fusion-stable cluster exchange graphs. Another application is that the
universal cover of the hyperplane arrangements of any finite Coxeter-Dynkin
type can be realized as the space of fusion-stable stability conditions for
certain ADE Dynkin quiver.


# Improved Probabilistic Image-Text Representations

[Link to the paper](http://arxiv.org/abs/2305.18171v2)

## Authors
- Sanghyuk Chun

## Summary
  Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,
suffers from the inherent ambiguity arising from multiplicity and imperfect
annotations. Deterministic functions are not sufficiently powerful to capture
ambiguity, prompting the exploration of probabilistic embeddings to tackle the
challenge. However, the existing probabilistic ITM approach encounters two key
shortcomings; the burden of heavy computations due to the Monte Carlo
approximation, and the loss saturation issue in the face of abundant false
negatives. To overcome the issues, this paper presents an improved
Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new
probabilistic distance with a closed-form solution. In addition, two
optimization techniques are proposed to enhance PCME++ further; first, the
incorporation of pseudo-positives to prevent the loss saturation problem under
massive false negatives; second, mixed sample data augmentation for
probabilistic matching. Experimental results on MS-COCO Caption and two
extended benchmarks, CxC and ECCV Caption, demonstrate the effectiveness of
PCME++ compared to state-of-the-art ITM methods. The robustness of PCME++ is
also evaluated under noisy image-text correspondences. In addition, the
potential applicability of PCME++ in automatic prompt tuning for zero-shot
classification is shown. The code is available at
https://naver-ai.github.io/pcmepp/.


# CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion

[Link to the paper](http://arxiv.org/abs/2303.11916v2)

## Authors
- Geonmo Gu
- Sanghyuk Chun
- Wonjae Kim
- HeeJae Jun
- Yoohoon Kang
- Sangdoo Yun

## Summary
  This paper proposes a novel diffusion-based model, CompoDiff, for solving
Composed Image Retrieval (CIR) with latent diffusion and presents a newly
created dataset, named SynthTriplets18M, of 18 million reference images,
conditions, and corresponding target image triplets to train the model.
CompoDiff and SynthTriplets18M tackle the shortages of the previous CIR
approaches, such as poor generalizability due to the small dataset scale and
the limited types of conditions. CompoDiff not only achieves a new zero-shot
state-of-the-art on four CIR benchmarks, including FashionIQ, CIRR, CIRCO, and
GeneCIS, but also enables a more versatile and controllable CIR by accepting
various conditions, such as negative text and image mask conditions, and the
controllability to the importance between multiple queries or the trade-off
between inference speed and the performance which are unavailable with existing
CIR methods. The code and dataset are available at
https://github.com/navervision/CompoDiff


# f(R) Gravity in an Ellipsoidal Universe

[Link to the paper](http://arxiv.org/abs/2310.02914v1)

## Authors
- Cemsinan Deliduman
- Oguzhan Kasikci
- Vildan Keles Tugyanoglu

## Summary
  We propose a new model of cosmology based on an anisotropic background and a
specific $f(R)$ theory of gravity. It is shown that field equations of $f(R)$
gravity in a Bianchi type I background give rise to a modified Friedmann
equation. This model contains two important parameters: $\gamma$ and $\delta$.
We, thus, simply call our model $\gamma\delta$CDM. It is distinguished in two
important aspects from the $\Lambda$CDM model: firstly, the contribution of
different energy densities to the Hubble parameter are weighted with different
weights, and then, dependence of energy densities to redshift is modified as
well. This unorthodox relation of energy content to Hubble parameter brings
forth a new way of interpreting the cosmological history. This solution does
not allow the existence of a cosmological constant component, however, a dark
energy contribution with dependence on redshift is possible. We tested
observational relevance of the new solution by best fitting to different data
sets. We found that our model could accommodate the idea of cosmological
coupling of black holes.


# Positivity for toric Kac polynomials in higher depth

[Link to the paper](http://arxiv.org/abs/2310.02912v1)

## Authors
- Tanguy Vernet

## Summary
  We prove that the polynomials counting locally free, absolutely
indecomposable, rank 1 representations of quivers over rings of truncated power
series have non-negative coefficients. This is a generalisation to higher depth
of positivity for toric Kac polynomials. The proof goes by inductively
contracting/deleting arrows of the quiver and is inspired from a previous work
of Abdelgadir, Mellit and Rodriguez-Villegas on toric Kac polynomials. We also
relate counts of absolutely indecomposable quiver representations in higher
depth and counts of jets over fibres of quiver moment maps. This is expressed
in a plethystic identity involving generating series of these counts. In rank
1, we prove a cohomological upgrade of this identity, by computing the
compactly supported cohomology of jet spaces over preprojective stacks. This is
reminiscent of PBW isomorphisms for preprojective cohomological Hall algebras.
Finally, our plethystic identity allows us to prove two conjectures by Wyss on
the asymptotic behaviour of both counts, when depth goes to infinity.


# Quantum spin liquid in the easy-axis Heisenberg model on frustrated lattices

[Link to the paper](http://arxiv.org/abs/2307.03545v2)

## Authors
- Martin Ulaga
- Jure Kokalj
- Alexander Wietek
- Andrej Zorko
- Peter Prelovšek

## Summary
  Motivated by recent experiments on a compound {displaying Ising-like
short-range correlations on the triangular lattice, we study the anisotropic
easy-axis spin-$1/2$ Heisenberg model on the triangular and kagome lattice} by
performing numerical calculations of finite-temperature properties, in
particular of static spin structure factor and of thermodynamic quantities, on
systems with up to 36 sites. On the triangular lattice, the low-temperature
spin structure factor {exhibits long-range} spin correlations in the whole
range of anisotropies, whereas thermodynamic quantities reveal a crossover upon
increasing the anisotropy, most pronounced in the vanishing generalized Wilson
ratio in the easy-axis regime. In contrast, on the kagome lattice, the spin
structure factor is short-range, and thermodynamic quantities evolve steadily
between the easy-axis and the isotropic case, consistent with the
interpretation in terms of {a} spin liquid.


# Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers Using Cryptocurrencies for Purchases

[Link to the paper](http://arxiv.org/abs/2310.02911v1)

## Authors
- Massimiliano Silenzi
- Umut Can Cabuk

## Summary
  The swiftly maturing sector of cryptocurrencies proffers an array of
challenges and prospects for both enterprises and consumers. This study
explores the knowledge, expertise, and purchasing behaviors of individuals
engaged in shopping using cryptocurrencies to furnish an exhaustive
understanding of this distinctive consumer cohort. By analyzing data from our
survey of 516 participants, our findings illuminate a range of knowledge
levels, encompassing neophytes to connoisseurs, with a significant segment
exhibiting high procurement frequency amidst constrained expertise. Regression
analyses unveil that, although knowledge significantly influences purchase
behaviors, its explanatory capacity remains restricted. Additionally, a K-means
cluster analysis discloses three disparate crypto-shopper profiles, each
possessing unique knowledge and expertise levels. These insights contravene
conventional wisdom regarding the nexus between domain knowledge and adoption,
insinuating that the appeal of cryptocurrencies transcends technical knowledge.
The revelations of this research are instrumental for enterprises aspiring to
address the diverse needs of the crypto-shopper demographic, accentuating the
imperative of personalized strategies and user experiences. This exploration
furthermore lays the groundwork for ensuing research focused on unraveling the
extensive implications of crypto acceptance and its confluence with consumer
conduct.


# Implementing Landauer-Büttiker approach in hybrid quantum Hall-superconducting devices

[Link to the paper](http://arxiv.org/abs/2310.02910v1)

## Authors
- Lingfei Zhao
- Ethan G. Arnault
- Trevyn F. Q. Larson
- Kenji Watanabe
- Takashi Taniguchi
- François Amet
- Gleb Finkelstein

## Summary
  We develop the expressions for the non-local resistances of hybrid quantum
Hall -- superconductor devices. Our approximation depends on the crucial
assumption that the superconducting phase correlations between contacts do not
play a role. We verify the validity of our approach in two devices: in the
first one, a single superconducting contact couples to multiple edge states,
while in the second, there are multiple superconducting electrodes. The results
obtained for the second device suggest that the superconducting phase coherence
is not maintained over our micron-scale graphene devices. Possible violations
of this assumption may be used as an indication that superconducting phase
coherence is induced in quantum Hall edges.


# Pseudo-Hermiticity protects the energy-difference conservation in the scattering

[Link to the paper](http://arxiv.org/abs/2310.02908v1)

## Authors
- H. S. Xu
- L. Jin

## Summary
  Symmetry plays a fundamentally important role in physics. In this work, we
find a conservation law, $S^{\dagger}(H_{c}^{\dagger})S(H_{c})=I$, which is
valid for any non-Hermitian scattering center $H_c$. As a result, the
reflections and transmissions of a non-Hermitian system $\left\{ r,t\right\}$
and its Hermitian conjugation system $\left\{ \bar{r},\bar{t}\right\} $ satisfy
the conservation law $\bar{r}^{\ast}r+\bar{t}^{\ast}t=1$, instead of the energy
conservation law that applies to incoming and outgoing waves in a Hermitian
system. Consequently, the pseudo-Hermiticity of a non-Hermitian system ensures
an energy-difference conservation. Furthermore, we demonstrate that the
energy-difference conservation is respectively valid and invalid in two
prototypical anti-$\mathcal{PT}$-symmetric systems, where the energy-difference
conservation is protected by the pseudo-Hermiticity. Our findings provide
profound insight into the conservation law, the pseudo-Hermiticity, and the
anti-$\mathcal{PT}$-symmetry in non-Hermitian systems.


# Robustified ANNs Reveal Wormholes Between Human Category Percepts

[Link to the paper](http://arxiv.org/abs/2308.06887v2)

## Authors
- Guy Gaziv
- Michael J. Lee
- James J. DiCarlo

## Summary
  The visual object category reports of artificial neural networks (ANNs) are
notoriously sensitive to tiny, adversarial image perturbations. Because human
category reports (aka human percepts) are thought to be insensitive to those
same small-norm perturbations -- and locally stable in general -- this argues
that ANNs are incomplete scientific models of human visual perception.
Consistent with this, we show that when small-norm image perturbations are
generated by standard ANN models, human object category percepts are indeed
highly stable. However, in this very same "human-presumed-stable" regime, we
find that robustified ANNs reliably discover low-norm image perturbations that
strongly disrupt human percepts. These previously undetectable human perceptual
disruptions are massive in amplitude, approaching the same level of sensitivity
seen in robustified ANNs. Further, we show that robustified ANNs support
precise perceptual state interventions: they guide the construction of low-norm
image perturbations that strongly alter human category percepts toward specific
prescribed percepts. These observations suggest that for arbitrary starting
points in image space, there exists a set of nearby "wormholes", each leading
the subject from their current category perceptual state into a semantically
very different state. Moreover, contemporary ANN models of biological visual
processing are now accurate enough to consistently guide us to those portals.


# Whole-body MPC for highly redundant legged manipulators: experimental evaluation with a 37 DoF dual-arm quadruped

[Link to the paper](http://arxiv.org/abs/2310.02907v1)

## Authors
- Ioannis Dadiotis
- Arturo Laurenzi
- Nikos Tsagarakis

## Summary
  Recent progress in legged locomotion has rendered quadruped manipulators a
promising solution for performing tasks that require both mobility and
manipulation (loco-manipulation). In the real world, task specifications and/or
environment constraints may require the quadruped manipulator to be equipped
with high redundancy as well as whole-body motion coordination capabilities.
This work presents an experimental evaluation of a whole-body Model Predictive
Control (MPC) framework achieving real-time performance on a dual-arm quadruped
platform consisting of 37 actuated joints. To the best of our knowledge this is
the legged manipulator with the highest number of joints to be controlled with
real-time whole-body MPC so far. The computational efficiency of the MPC while
considering the full robot kinematics and the centroidal dynamics model builds
upon an open-source DDP-variant solver and a state-of-the-art optimal control
problem formulation. Differently from previous works on quadruped manipulators,
the MPC is directly interfaced with the low-level joint impedance controllers
without the need of designing an instantaneous whole-body controller. The
feasibility on the real hardware is showcased using the CENTAURO platform for
the challenging task of picking a heavy object from the ground. Dynamic
stepping (trotting) is also showcased for first time with this robot. The
results highlight the potential of replanning with whole-body information in a
predictive control loop.


# Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts

[Link to the paper](http://arxiv.org/abs/2310.02906v1)

## Authors
- Shiyi Du
- Xiaosong Wang
- Yongyi Lu
- Yuyin Zhou
- Shaoting Zhang
- Alan Yuille
- Kang Li
- Zongwei Zhou

## Summary
  Image synthesis approaches, e.g., generative adversarial networks, have been
popular as a form of data augmentation in medical image analysis tasks. It is
primarily beneficial to overcome the shortage of publicly accessible data and
associated quality annotations. However, the current techniques often lack
control over the detailed contents in generated images, e.g., the type of
disease patterns, the location of lesions, and attributes of the diagnosis. In
this work, we adapt the latest advance in the generative model, i.e., the
diffusion model, with the added control flow using lesion-specific visual and
textual prompts for generating dermatoscopic images. We further demonstrate the
advantage of our diffusion model-based framework over the classical generation
models in both the image quality and boosting the segmentation performance on
skin lesions. It can achieve a 9% increase in the SSIM image quality measure
and an over 5% increase in Dice coefficients over the prior arts.


# Computationally Efficient Quadratic Neural Networks

[Link to the paper](http://arxiv.org/abs/2310.02901v1)

## Authors
- Mathew Mithra Noel
- Venkataraman Muthiah-Nakarajan

## Summary
  Higher order artificial neurons whose outputs are computed by applying an
activation function to a higher order multinomial function of the inputs have
been considered in the past, but did not gain acceptance due to the extra
parameters and computational cost. However, higher order neurons have
significantly greater learning capabilities since the decision boundaries of
higher order neurons can be complex surfaces instead of just hyperplanes. The
boundary of a single quadratic neuron can be a general hyper-quadric surface
allowing it to learn many nonlinearly separable datasets. Since quadratic forms
can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional
parameters are needed instead of $n^2$. A quadratic Logistic regression model
is first presented. Solutions to the XOR problem with a single quadratic neuron
are considered. The complete vectorized equations for both forward and backward
propagation in feedforward networks composed of quadratic neurons are derived.
A reduced parameter quadratic neural network model with just $ n $ additional
parameters per neuron that provides a compromise between learning ability and
computational cost is presented. Comparison on benchmark classification
datasets are used to demonstrate that a final layer of quadratic neurons
enables networks to achieve higher accuracy with significantly fewer hidden
layer neurons. In particular this paper shows that any dataset composed of $C$
bounded clusters can be separated with only a single layer of $C$ quadratic
neurons.


# Lorentz invariants of pure three-qubit states

[Link to the paper](http://arxiv.org/abs/2310.02900v1)

## Authors
- A R Usha Devi
-  Sudha
- H Akshata Shenoy
- H S Karthik
- B N Karthik

## Summary
  Extending the mathematical framework of Phys. Rev. A 102, 052419 (2020) we
construct Lorentz invariant quantities of pure three-qubit states. This method
serves as a bridge between the well-known local unitary (LU) invariants viz.
concurrences and three-tangle of an arbitrary three-qubit pure state and the
Lorentz invariants of its reduced two-qubit systems.


# Some bidding games converging to their unique pure equilibrium

[Link to the paper](http://arxiv.org/abs/2310.02898v1)

## Authors
- Benjamin Heymann
- Alejandro Jofré

## Summary
  We introduce a class of Bayesian bidding games for which we prove that the
set of pure Nash equilibria is a (non-empty) sublattice and we give a
sufficient condition for uniqueness that is often verified in the context of
markets with inelastic demand. We propose a dynamic that converges to the
extrema of the equilibrium set and derive a scheme to compute the extreme Nash
equilibria.


# Recovery of Training Data from Overparameterized Autoencoders: An Inverse Problem Perspective

[Link to the paper](http://arxiv.org/abs/2310.02897v1)

## Authors
- Koren Abitbul
- Yehuda Dar

## Summary
  We study the recovery of training data from overparameterized autoencoder
models. Given a degraded training sample, we define the recovery of the
original sample as an inverse problem and formulate it as an optimization task.
In our inverse problem, we use the trained autoencoder to implicitly define a
regularizer for the particular training dataset that we aim to retrieve from.
We develop the intricate optimization task into a practical method that
iteratively applies the trained autoencoder and relatively simple computations
that estimate and address the unknown degradation operator. We evaluate our
method for blind inpainting where the goal is to recover training images from
degradation of many missing pixels in an unknown pattern. We examine various
deep autoencoder architectures, such as fully connected and U-Net (with various
nonlinearities and at diverse train loss values), and show that our method
significantly outperforms previous methods for training data recovery from
autoencoders. Importantly, our method greatly improves the recovery performance
also in settings that were previously considered highly challenging, and even
impractical, for such retrieval.


# ValiTex -- a unified validation framework for computational text-based measures of social science constructs

[Link to the paper](http://arxiv.org/abs/2307.02863v4)

## Authors
- Lukas Birkenmaier
- Claudia Wagner
- Clemens Lechner

## Summary
  Guidance on how to validate computational text-based measures of social
science constructs is fragmented. While scholars generally acknowledge the
importance of validating their text-based measures, they often lack common
terminology and a unified framework to do so. This paper introduces ValiTex, a
new validation framework designed to assist scholars in validly measuring
social science constructs based on textual data. ValiTex prescribes researchers
to demonstrate three types of validity evidence: substantive evidence
(outlining the theoretical underpinning of the measure), structural evidence
(examining the properties of the text model and its output), and external
evidence (testing for how the measure relates to independent information). In
addition to the framework, ValiTex offers valuable practical guidance through a
checklist that is adaptable for different use cases. The checklist clearly
defines and outlines specific validation steps while also offering a
knowledgeable evaluation of the importance of each validation step to establish
validity. We demonstrate the utility of the framework by applying it to a use
case of detecting sexism from social media data.


# On the nature of the two-positron bond: Evidence for a novel bond type

[Link to the paper](http://arxiv.org/abs/2306.12899v3)

## Authors
- Mohammad Goli
- Dario Bressanini
- Shant Shahbazian

## Summary
  The nature of the newly proposed two-positron bond in (PsH)2, which is
composed of two protons, four electrons and two positrons, is considered in
this contribution. The study is done at the multi-component-Hartree-Fock
(MC-HF) and the Diffusion Monte Carlo (DMC) levels of theory by comparing ab
initio data, analyzing the spatial structure of the DMC wavefunction, and
applying the multi-component quantum theory of atoms in molecules and the
two-component interacting quantum atoms energy partitioning schemes to the
MC-HF wavefunction. The analysis demonstrates that (PsH)2 to a good
approximation may be conceived of two slightly perturbed PsH atoms, bonded
through a two-positron bond. In contrast to the usual two-electron bonds, the
positron exchange phenomenon is quite marginal in the considered two-positron
bond. The dominant stabilizing mechanism of bonding is a novel type of
classical electrostatic interaction between the positrons, which are mainly
localized between nuclei, and the surrounding electrons. To emphasize its
uniqueness, this mechanism of bonding is proposed to be called gluonic which
has also been previously identified as the main deriving mechanism behind
formation of the one-positron bond in [H-, e+, H-]. We conclude that the
studied two-positron bond should not be classified as a covalent bond and it
must be seen as a brand-new type of bond, foreign to the electronic bonding
modes discovered so far in the purely electronic systems.


# CoLiDE: Concomitant Linear DAG Estimation

[Link to the paper](http://arxiv.org/abs/2310.02895v1)

## Authors
- Seyed Saman Saboksayr
- Gonzalo Mateos
- Mariano Tepper

## Summary
  We deal with the combinatorial problem of learning directed acyclic graph
(DAG) structure from observational data adhering to a linear structural
equation model (SEM). Leveraging advances in differentiable, nonconvex
characterizations of acyclicity, recent efforts have advocated a continuous
constrained optimization paradigm to efficiently explore the space of DAGs.
Most existing methods employ lasso-type score functions to guide this search,
which (i) require expensive penalty parameter retuning when the
$\textit{unknown}$ SEM noise variances change across problem instances; and
(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we
propose a new convex score function for sparsity-aware learning of linear DAGs,
which incorporates concomitant estimation of scale and thus effectively
decouples the sparsity parameter from the exogenous noise levels.
Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE
($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG
$\textbf{E}$stimation), a regression-based criterion amenable to efficient
gradient computation and closed-form estimation of noise variances in
heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods
without incurring added complexity, especially when the DAGs are larger and the
noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced
stability manifested via reduced standard deviations in several domain-specific
metrics, underscoring the robustness of our novel linear DAG estimator.


# Human-centric Behavior Description in Videos: New Benchmark and Model

[Link to the paper](http://arxiv.org/abs/2310.02894v1)

## Authors
- Lingru Zhou
- Yiqi Gao
- Manqing Zhang
- Peng Wu
- Peng Wang
- Yanning Zhang

## Summary
  In the domain of video surveillance, describing the behavior of each
individual within the video is becoming increasingly essential, especially in
complex scenarios with multiple individuals present. This is because describing
each individual's behavior provides more detailed situational analysis,
enabling accurate assessment and response to potential risks, ensuring the
safety and harmony of public places. Currently, video-level captioning datasets
cannot provide fine-grained descriptions for each individual's specific
behavior. However, mere descriptions at the video-level fail to provide an
in-depth interpretation of individual behaviors, making it challenging to
accurately determine the specific identity of each individual. To address this
challenge, we construct a human-centric video surveillance captioning dataset,
which provides detailed descriptions of the dynamic behaviors of 7,820
individuals. Specifically, we have labeled several aspects of each person, such
as location, clothing, and interactions with other elements in the scene, and
these people are distributed across 1,012 videos. Based on this dataset, we can
link individuals to their respective behaviors, allowing for further analysis
of each person's behavior in surveillance videos. Besides the dataset, we
propose a novel video captioning approach that can describe individual behavior
in detail on a person-level basis, achieving state-of-the-art results. To
facilitate further research in this field, we intend to release our dataset and
code.


# Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization

[Link to the paper](http://arxiv.org/abs/2306.01095v3)

## Authors
- Navid Ansari
- Hans-Peter Seidel
- Vahid Babaei

## Summary
  Bayesian optimization (BO) provides a powerful framework for optimizing
black-box, expensive-to-evaluate functions. It is therefore an attractive tool
for engineering design problems, typically involving multiple objectives.
Thanks to the rapid advances in fabrication and measurement methods as well as
parallel computing infrastructure, querying many design problems can be heavily
parallelized. This class of problems challenges BO with an unprecedented setup
where it has to deal with very large batches, shifting its focus from sample
efficiency to iteration efficiency. We present a novel Bayesian optimization
framework specifically tailored to address these limitations. Our key
contribution is a highly scalable, sample-based acquisition function that
performs a non-dominated sorting of not only the objectives but also their
associated uncertainty. We show that our acquisition function in combination
with different Bayesian neural network surrogates is effective in
data-intensive environments with a minimal number of iterations. We demonstrate
the superiority of our method by comparing it with state-of-the-art
multi-objective optimizations. We perform our evaluation on two real-world
problems -- airfoil design and 3D printing -- showcasing the applicability and
efficiency of our approach. Our code is available at:
https://github.com/an-on-ym-ous/lbn_mobo


# A Grammatical Compositional Model for Video Action Detection

[Link to the paper](http://arxiv.org/abs/2310.02887v1)

## Authors
- Zhijun Zhang
- Xu Zou
- Jiahuan Zhou
- Sheng Zhong
- Ying Wu

## Summary
  Analysis of human actions in videos demands understanding complex human
dynamics, as well as the interaction between actors and context. However, these
interaction relationships usually exhibit large intra-class variations from
diverse human poses or object manipulations, and fine-grained inter-class
differences between similar actions. Thus the performance of existing methods
is severely limited. Motivated by the observation that interactive actions can
be decomposed into actor dynamics and participating objects or humans, we
propose to investigate the composite property of them. In this paper, we
present a novel Grammatical Compositional Model (GCM) for action detection
based on typical And-Or graphs. Our model exploits the intrinsic structures and
latent relationships of actions in a hierarchical manner to harness both the
compositionality of grammar models and the capability of expressing rich
features of DNNs. The proposed model can be readily embodied into a neural
network module for efficient optimization in an end-to-end manner. Extensive
experiments are conducted on the AVA dataset and the Something-Else task to
demonstrate the superiority of our model, meanwhile the interpretability is
enhanced through an inference parsing procedure.


# Deep Stochastic Mechanics

[Link to the paper](http://arxiv.org/abs/2305.19685v2)

## Authors
- Elena Orlova
- Aleksei Ustimenko
- Ruoxi Jiang
- Peter Y. Lu
- Rebecca Willett

## Summary
  This paper introduces a novel deep-learning-based approach for numerical
simulation of a time-evolving Schr\"odinger equation inspired by stochastic
mechanics and generative diffusion models. Unlike existing approaches, which
exhibit computational complexity that scales exponentially in the problem
dimension, our method allows us to adapt to the latent low-dimensional
structure of the wave function by sampling from the Markovian diffusion.
Depending on the latent dimension, our method may have far lower computational
complexity in higher dimensions. Moreover, we propose novel equations for
stochastic quantum mechanics, resulting in linear computational complexity with
respect to the number of dimensions. Numerical simulations verify our
theoretical findings and show a significant advantage of our method compared to
other deep-learning-based approaches used for quantum mechanics.


# Gravitational Waves from Domain Wall Collapse, and Application to Nanohertz Signals with QCD-coupled Axions

[Link to the paper](http://arxiv.org/abs/2306.17146v2)

## Authors
- Naoya Kitajima
- Junseok Lee
- Kai Murai
- Fuminobu Takahashi
- Wen Yin

## Summary
  We study for the first time the gravitational waves generated during the
collapse of domain walls, incorporating the potential bias in the lattice
simulations. The final stages of domain wall collapse are crucial for the
production of gravitational waves, but have remained unexplored due to
computational difficulties. As a significant application of this new result, we
show that the observed NANOGrav, EPTA, PPTA, and CPTA data, which indicate
stochastic gravitational waves in the nanohertz regime, can be attributed to
axion domain walls coupled to QCD. In our model, non-perturbative effects of
QCD induce a temperature-dependent bias around the QCD crossover, inducing the
rapid collapse of the domain walls. We use sophisticated lattice simulations
that account for the temperature-dependent bias to measure the gravitational
waves resulting from the domain wall annihilation. We also discuss the future
prospects for accelerator-based searches for the axion and the potential for
the formation and detection of primordial black holes.


# Coherence of Group-IV Color Centers

[Link to the paper](http://arxiv.org/abs/2310.02884v1)

## Authors
- Isaac B. W. Harris
- Dirk Englund

## Summary
  Group-IV color centers in diamond (SiV, GeV, SnV) have emerged as leading
solid-state spin-photon interfaces for quantum information processing
applications. However, these qubits require cryogenic temperatures to achieve
high fidelity operation due to interactions with the thermal phonon bath. In
this work, we: (i) derive a detailed model of the decoherence from first-order
acoustic phonon processes acting on the spin-orbit fine structure of these
color centers; (ii) demonstrate agreement of the model's predicted coherence
times with previous measurements; (iii) identify regimes to suppress
phonon-mediated decoherence by changing magnetic-field and strain bias to allow
higher temperature operation. This methodology enables prediction of
decoherence processes in other color centers and solid-state qubit systems
coupled to a thermal bath via a parasitic two-level system. By
experiment-anchored decoherence models, we facilitate optimizing qubit
coherence for specific applications and devices.


# Online Mechanism Design with Predictions

[Link to the paper](http://arxiv.org/abs/2310.02879v1)

## Authors
- Eric Balkanski
- Vasilis Gkatzelis
- Xizhi Tan
- Cherlin Zhu

## Summary
  Aiming to overcome some of the limitations of worst-case analysis, the
recently proposed framework of "algorithms with predictions" allows algorithms
to be augmented with a (possibly erroneous) machine-learned prediction that
they can use as a guide. In this framework, the goal is to obtain improved
guarantees when the prediction is correct, which is called \emph{consistency},
while simultaneously guaranteeing some worst-case bounds even when the
prediction is arbitrarily wrong, which is called \emph{robustness}. The vast
majority of the work on this framework has focused on a refined analysis of
online algorithms augmented with predictions regarding the future input. A
subsequent line of work has also successfully adapted this framework to
mechanism design, where the prediction is regarding the private information of
strategic agents. In this paper, we initiate the study of online mechanism
design with predictions, which combines the challenges of online algorithms
with predictions and mechanism design with predictions.
  We consider the well-studied problem of designing a revenue-maximizing
auction to sell a single item to strategic bidders who arrive and depart over
time, each with an unknown, private, value for the item. We study the
learning-augmented version of this problem where the auction designer is given
a prediction regarding the maximum value over all agents. Our main result is a
strategyproof mechanism whose revenue guarantees are $\alpha$-consistent with
respect to the highest value and $(1-\alpha^2)/4$-robust with respect to the
second-highest value, for $\alpha \in [0,1]$. We show that this tradeoff is
optimal within a broad and natural family of auctions, meaning that any
$\alpha$-consistent mechanism in that family has robustness at most
$(1-\alpha^2)/4$. Finally, we extend our mechanism to also achieve expected
revenues proportional to the prediction quality.


# Learning Type Inference for Enhanced Dataflow Analysis

[Link to the paper](http://arxiv.org/abs/2310.00673v2)

## Authors
- Lukas Seidel
- Sedick David Baker Effendi
- Xavier Pinho
- Konrad Rieck
- Brink van der Merwe
- Fabian Yamaguchi

## Summary
  Statically analyzing dynamically-typed code is a challenging endeavor, as
even seemingly trivial tasks such as determining the targets of procedure calls
are non-trivial without knowing the types of objects at compile time.
Addressing this challenge, gradual typing is increasingly added to
dynamically-typed languages, a prominent example being TypeScript that
introduces static typing to JavaScript. Gradual typing improves the developer's
ability to verify program behavior, contributing to robust, secure and
debuggable programs. In practice, however, users only sparsely annotate types
directly. At the same time, conventional type inference faces
performance-related challenges as program size grows. Statistical techniques
based on machine learning offer faster inference, but although recent
approaches demonstrate overall improved accuracy, they still perform
significantly worse on user-defined types than on the most common built-in
types. Limiting their real-world usefulness even more, they rarely integrate
with user-facing applications. We propose CodeTIDAL5, a Transformer-based model
trained to reliably predict type annotations. For effective result retrieval
and re-integration, we extract usage slices from a program's code property
graph. Comparing our approach against recent neural type inference systems, our
model outperforms the current state-of-the-art by 7.85% on the
ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore,
we present JoernTI, an integration of our approach into Joern, an open source
static analysis tool, and demonstrate that the analysis benefits from the
additional type information. As our model allows for fast inference times even
on commodity CPUs, making our system available through Joern leads to high
accessibility and facilitates security research.


# Isolated horizons of the Hopf bundle structure transversal to the null direction, the horizon equations and embeddability in NUT-like spacetimes

[Link to the paper](http://arxiv.org/abs/2308.14044v2)

## Authors
- Denis Dobkowski-Ryłko
- Jerzy Lewandowski
- Maciej Ossowski

## Summary
  Isolated horizons that admit the Hopf bundle structure $H\rightarrow S_2$ are
investigated, however the null direction is allowed not to be tangent to the
bundle fibres. The geometry of such horizons is characterised by data set on a
topological two-dimensional sphere, singular at its poles. The horizon
equations induced by Einstein's equations are imposed. The existence of regular
extremal horizons satisfying the vacuum (with cosmological constant) equation
of extremality, obtained from singular solutions on the sphere is pointed out.
All horizons (with assumed topology and in the generic case) satisfying the
$\Lambda$ vacuum type D equation are derived. They are compared to the Killing
horizons contained in the accelerated Kerr-NUT-(Anti) de Sitter spacetimes.
Both families of horizons have the same dimension, but the problem of mutual
correspondence needs to be better understood. If the cosmological constant
takes special values determined by the other parameters the bundle fibers
become tangent to the null direction. As an additional but also important
result, spacetimes of the topology $H\times \mathbb{R}$ locally isometric to
the accelerated Kerr-NUT-(Anti) de Sitter spacetimes are constructed for every
value of the mass, Kerr, NUT parameters, the cosmological constant and the
acceleration. When the acceleration parameter is not zero, the conical
singularity can be removed whenever the NUT parameter does not vanish either.


# Incorporating prior information into distributed lag nonlinear models with zero-inflated monotone regression trees

[Link to the paper](http://arxiv.org/abs/2301.12937v2)

## Authors
- Daniel Mork
- Ander Wilson

## Summary
  In environmental health research there is often interest in the effect of an
exposure on a health outcome assessed on the same day and several subsequent
days or lags. Distributed lag nonlinear models (DLNM) are a well-established
statistical framework for estimating an exposure-lag-response function. We
propose methods to allow for prior information to be incorporated into DLNMs.
First, we impose a monotonicity constraint in the exposure-response at lagged
time periods which matches with knowledge on how biological mechanisms respond
to increased levels of exposures. Second, we introduce variable selection into
the DLNM to identify lagged periods of susceptibility with respect to the
outcome of interest. The variable selection approach allows for direct
application of informative priors on which lags have nonzero association with
the outcome. We propose a tree-of-trees model that uses two layers of trees:
one for splitting the exposure time frame and one for fitting exposure-response
functions over different time periods. We introduce a zero-inflated alternative
to the tree splitting prior in Bayesian additive regression trees to allow for
lag selection and the addition of informative priors. We develop a
computational approach for efficient posterior sampling and perform a
comprehensive simulation study to compare our method to existing DLNM
approaches. We apply our method to estimate time-lagged extreme temperature
relationships with mortality during summer or winter in Chicago, IL.


# Stationarity without mean reversion: Improper Gaussian process regression and improper kernels

[Link to the paper](http://arxiv.org/abs/2310.02877v1)

## Authors
- Luca Ambrogioni

## Summary
  Gaussian processes (GP) regression has gained substantial popularity in
machine learning applications. The behavior of a GP regression depends on the
choice of covariance function. Stationary covariance functions are favorite in
machine learning applications. However, (non-periodic) stationary covariance
functions are always mean reverting and can therefore exhibit pathological
behavior when applied to data that does not relax to a fixed global mean value.
In this paper, we show that it is possible to use improper GP prior with
infinite variance to define processes that are stationary but not mean
reverting. To this aim, we introduce a large class of improper kernels that can
only be defined in this improper regime. Specifically, we introduce the Smooth
Walk kernel, which produces infinitely smooth samples, and a family of improper
Mat\'ern kernels, which can be defined to be $j$-times differentiable for any
integer $j$. The resulting posterior distributions can be computed analytically
and it involves a simple correction of the usual formulas. By analyzing both
synthetic and real data, we demonstrate that these improper kernels solve some
known pathologies of mean reverting GP regression while retaining most of the
favourable properties of ordinary smooth stationary kernels.


# Hate Speech Detection in Limited Data Contexts using Synthetic Data Generation

[Link to the paper](http://arxiv.org/abs/2310.02876v1)

## Authors
- Aman Khullar
- Daniel Nkemelu
- Cuong V. Nguyen
- Michael L. Best

## Summary
  A growing body of work has focused on text classification methods for
detecting the increasing amount of hate speech posted online. This progress has
been limited to only a select number of highly-resourced languages causing
detection systems to either under-perform or not exist in limited data
contexts. This is majorly caused by a lack of training data which is expensive
to collect and curate in these settings. In this work, we propose a data
augmentation approach that addresses the problem of lack of data for online
hate speech detection in limited data contexts using synthetic data generation
techniques. Given a handful of hate speech examples in a high-resource language
such as English, we present three methods to synthesize new examples of hate
speech data in a target language that retains the hate sentiment in the
original examples but transfers the hate targets. We apply our approach to
generate training data for hate speech classification tasks in Hindi and
Vietnamese. Our findings show that a model trained on synthetic data performs
comparably to, and in some cases outperforms, a model trained only on the
samples available in the target domain. This method can be adopted to bootstrap
hate speech detection models from scratch in limited data contexts. As the
growth of social media within these contexts continues to outstrip response
efforts, this work furthers our capacities for detection, understanding, and
response to hate speech.


# Approximating Robot Configuration Spaces with few Convex Sets using Clique Covers of Visibility Graphs

[Link to the paper](http://arxiv.org/abs/2310.02875v1)

## Authors
- Peter Werner
- Alexandre Amice
- Tobia Marcucci
- Daniela Rus
- Russ Tedrake

## Summary
  Many computations in robotics can be dramatically accelerated if the robot
configuration space is described as a collection of simple sets. For example,
recently developed motion planners rely on a convex decomposition of the free
space to design collision-free trajectories using fast convex optimization. In
this work, we present an efficient method for approximately covering complex
configuration spaces with a small number of polytopes. The approach constructs
a visibility graph using sampling and generates a clique cover of this graph to
find clusters of samples that have mutual line of sight. These clusters are
then inflated into large, full-dimensional, polytopes. We evaluate our method
on a variety of robotic systems and show that it consistently covers larger
portions of free configuration space, with fewer polytopes, and in a fraction
of the time compared to previous methods.


# CMSSW Scaling Limits on Many-Core Machines

[Link to the paper](http://arxiv.org/abs/2310.02872v1)

## Authors
- Christopher Jones
- Patrick Gartung

## Summary
  Today the LHC offline computing relies heavily on CPU resources, despite the
interest in compute accelerators, such as GPUs, for the longer term future. The
number of cores per CPU socket has continued to increase steadily, reaching the
levels of 64 cores (128 threads) with recent AMD EPYC processors, and 128 cores
on Ampere Altra Max ARM processors. Over the course of the past decade, the CMS
data processing framework, CMSSW, has been transformed from a single-threaded
framework into a highly concurrent one. The first multithreaded version was
brought into production by the start of the LHC Run 2 in 2015. Since then, the
framework's threading efficiency has gradually been improved by adding more
levels of concurrency and reducing the amount of serial code paths. The latest
addition was support for concurrent Runs. In this work we review the
concurrency model of the CMSSW, and measure its scalability with real CMS
applications, such as simulation and reconstruction, on mode rn many-core
machines. We show metrics such as event processing throughput and application
memory usage with and without the contribution of I/O, as I/O has been the
major scaling limitation for the CMS applications.


# Resource Theory of Non-absolute Separability

[Link to the paper](http://arxiv.org/abs/2212.11105v2)

## Authors
- Ayan Patra
- Arghya Maity
- Aditi Sen De

## Summary
  We develop a resource theory for non-absolutely separable states (non-AS) in
which absolutely separable states (AS) that cannot be entangled by any global
unitaries are recognised as free states and any convex mixture of global
unitary operations can be performed without incurring any costs. We employ two
approaches to quantify non-absolute separability (NAS) -- one based on distance
measures and the other one through the use of a witness operator. We prove that
both the NAS measures obey all the conditions which should be followed by a
``good'' NAS measure. We demonstrate that NAS content is equal and maximal in
all pure states for a fixed dimension. We then establish a connection between
the distance-based NAS measure and the entanglement quantifier. We illustrate
our results with a class of non-AS states, namely Werner states.


# Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness

[Link to the paper](http://arxiv.org/abs/2308.16175v2)

## Authors
- Jiuhai Chen
- Jonas Mueller

## Summary
  We introduce BSDetector, a method for detecting bad and speculative answers
from a pretrained Large Language Model by estimating a numeric confidence score
for any output it generated. Our uncertainty quantification technique works for
any LLM accessible only via a black-box API, whose training data remains
unknown. By expending a bit of extra computation, users of any LLM API can now
get the same response as they would ordinarily, as well as a confidence
estimate that cautions when not to trust this response. Experiments on both
closed and open-form Question-Answer benchmarks reveal that BSDetector more
accurately identifies incorrect LLM responses than alternative uncertainty
estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple
responses from the LLM and considering the one with the highest confidence
score, we can additionally obtain more accurate responses from the same LLM,
without any extra training steps. In applications involving automated
evaluation with LLMs, accounting for our confidence scores leads to more
reliable evaluation in both human-in-the-loop and fully-automated settings
(across both GPT 3.5 and 4).


# Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric

[Link to the paper](http://arxiv.org/abs/2310.02870v1)

## Authors
- Shiyun Wa
- Xinai Lu
- Minjuan Wang

## Summary
  As Artificial Intelligence (AI) integrates deeper into diverse sectors, the
quest for powerful models has intensified. While significant strides have been
made in boosting model capabilities and their applicability across domains, a
glaring challenge persists: many of these state-of-the-art models remain as
black boxes. This opacity not only complicates the explanation of model
decisions to end-users but also obstructs insights into intermediate processes
for model designers. To address these challenges, we introduce InterpreTabNet,
a model designed to enhance both classification accuracy and interpretability
by leveraging the TabNet architecture with an improved attentive module. This
design ensures robust gradient propagation and computational stability.
Additionally, we present a novel evaluation metric, InterpreStability, which
quantifies the stability of a model's interpretability. The proposed model and
metric mark a significant stride forward in explainable models' research,
setting a standard for transparency and interpretability in AI model design and
application across diverse sectors. InterpreTabNet surpasses other leading
solutions in tabular data analysis across varied application scenarios, paving
the way for further research into creating deep-learning models that are both
highly accurate and inherently explainable. The introduction of the
InterpreStability metric ensures that the interpretability of future models can
be measured and compared in a consistent and rigorous manner. Collectively,
these contributions have the potential to promote the design principles and
development of next-generation interpretable AI models, widening the adoption
of interpretable AI solutions in critical decision-making environments.


# Instruction Tuning for Large Language Models: A Survey

[Link to the paper](http://arxiv.org/abs/2308.10792v3)

## Authors
- Shengyu Zhang
- Linfeng Dong
- Xiaoya Li
- Sen Zhang
- Xiaofei Sun
- Shuhe Wang
- Jiwei Li
- Runyi Hu
- Tianwei Zhang
- Fei Wu
- Guoyin Wang

## Summary
  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey


# Computing high-dimensional optimal transport by flow neural networks

[Link to the paper](http://arxiv.org/abs/2305.11857v3)

## Authors
- Chen Xu
- Xiuyuan Cheng
- Yao Xie

## Summary
  Flow-based models are widely used in generative tasks, including normalizing
flow, where a neural network transports from a data distribution $P$ to a
normal distribution. This work develops a flow-based model that transports from
$P$ to an arbitrary $Q$ where both distributions are only accessible via finite
samples. We propose to learn the dynamic optimal transport between $P$ and $Q$
by training a flow neural network. The model is trained to find an invertible
transport map between $P$ and $Q$ optimally by minimizing the transport cost.
The trained optimal transport flow allows for performing many downstream tasks,
including infinitesimal density ratio estimation and distribution interpolation
in the latent space for generative models. The effectiveness of the proposed
model on high-dimensional data is empirically demonstrated in mutual
information estimation, energy-based generative models, and image-to-image
translation.


# Two-photon excitation and absorption spectroscopy of gaseous and supercritical xenon

[Link to the paper](http://arxiv.org/abs/2304.12803v2)

## Authors
- Thilo vom Hövel
- Franz Huybrechts
- Eric Boltersdorf
- Christian Wahl
- Frank Vewinger
- Martin Weitz

## Summary
  Spectroscopy of gases under high-pressure conditions is of interest in
various fields such as plasma physics and astrophysics. Recently, it has also
been proposed to utilize a high-pressure noble gas environment as a
thermalization medium to extend the wavelength range of photon Bose-Einstein
condensates to the vacuum-ultraviolet regime, from the presently accessible
visible and near-infrared spectral regimes. In this work, we report on
experimental results of two-photon spectroscopy of gaseous and supercritical
xenon for pressures as high as $95 \; \text{bar}$, probing the transitions from
the $5p^6$ electronic ground-state to the $5p^56p$ and $5p^56p^\prime$
excited-state configurations. Aiming at the exploration of possible pumping
schemes for future vacuum-ultraviolet photon condensates, we have recorded
degenerate two-photon excitation spectra of such dense xenon samples. In
further measurements, we have investigated whether irradiation of an auxiliary
light field can enhance the reabsorption of the emission on the second excimer
continuum of xenon, which is subject to a large Stokes shift. To this end,
absorption measurements have been conducted, driving the $5p^6 \rightarrow
5p^56p$ two-photon transitions nondegenerately.


# Thermal State Preparation via Rounding Promises

[Link to the paper](http://arxiv.org/abs/2210.01670v2)

## Authors
- Patrick Rall
- Chunhao Wang
- Pawel Wocjan

## Summary
  A promising avenue for the preparation of Gibbs states on a quantum computer
is to simulate the physical thermalization process. The Davies generator
describes the dynamics of an open quantum system that is in contact with a heat
bath. Crucially, it does not require simulation of the heat bath itself, only
the system we hope to thermalize. Using the state-of-the-art techniques for
quantum simulation of the Lindblad equation, we devise a technique for the
preparation of Gibbs states via thermalization as specified by the Davies
generator.
  In doing so, we encounter a severe technical challenge: implementation of the
Davies generator demands the ability to estimate the energy of the system
unambiguously. That is, each energy of the system must be deterministically
mapped to a unique estimate. Previous work showed that this is only possible if
the system satisfies an unphysical 'rounding promise' assumption. We solve this
problem by engineering a random ensemble of rounding promises that
simultaneously solves three problems: First, each rounding promise admits
preparation of a 'promised' thermal state via a Davies generator. Second, these
Davies generators have a similar mixing time as the ideal Davies generator.
Third, the average of these promised thermal states approximates the ideal
thermal state.


# FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation

[Link to the paper](http://arxiv.org/abs/2309.16364v2)

## Authors
- Songlin Wei
- Jiazhao Zhang
- Yang Wang
- Fanbo Xiang
- Hao Su
- He Wang

## Summary
  Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.


# On Separability of Covariance in Multiway Data Analysis

[Link to the paper](http://arxiv.org/abs/2302.02415v2)

## Authors
- Dogyoon Song
- Alfred O. Hero

## Summary
  Multiway data analysis aims to uncover patterns in data structured as
multi-indexed arrays, and the covariance of such data plays a crucial role in
various machine learning applications. However, the intrinsically high
dimension of multiway covariance presents significant challenges. To address
these challenges, factorized covariance models have been proposed that rely on
a separability assumption: the multiway covariance can be accurately expressed
as a sum of Kronecker products of mode-wise covariances. This paper is
concerned with the accuracy of such separable models for representing multiway
covariances. We reduce the question of whether a given covariance can be
represented as a separable multiway covariance to an equivalent question about
separability of quantum states. Based on this equivalence, we establish that
generic multiway covariances tend to be not separable. Moreover, we show that
determining the best separable approximation of a generic covariance is
NP-hard. Our results suggest that factorized covariance models might not
accurately approximate covariance, without additional assumptions ensuring
separability. To balance these negative results, we propose an iterative
Frank-Wolfe algorithm for computing Kronecker-separable covariance
approximations with some additional side information. We establish an oracle
complexity bound and empirically observe its consistent convergence to a
separable limit point, often close to the ``best'' separable approximation.
These results suggest that practical methods may be able to find a
Kronecker-separable approximation of covariances, despite the worst-case NP
hardness results.


# Locality bounds for quantum dynamics at low energy

[Link to the paper](http://arxiv.org/abs/2310.02856v1)

## Authors
- Andrew Osborne
- Chao Yin
- Andrew Lucas

## Summary
  We discuss the generic slowing down of quantum dynamics in low energy density
states of spatially local Hamiltonians. Beginning with quantum walks of a
single particle, we prove that for certain classes of Hamiltonians
(deformations of lattice-regularized $H\propto p^{2k}$), the ``butterfly
velocity" of particle motion at low temperatures must scale as $T^{(2k-1)/2k}$,
as expected from dimensional analysis. We generalize these results to obtain
bounds on the typical velocities of particles in many-body systems, where for
certain families of Hubbard-like models we obtain similar scaling.


# Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark Detection

[Link to the paper](http://arxiv.org/abs/2310.02855v1)

## Authors
- Dongqian Guo
- Wencheng Han

## Summary
  Cephalometric landmark detection on lateral skull X-ray images plays a
crucial role in the diagnosis of certain dental diseases. Accurate and
effective identification of these landmarks presents a significant challenge.
Based on extensive data observations and quantitative analyses, we discovered
that visual features from different receptive fields affect the detection
accuracy of various landmarks differently. As a result, we employed an image
pyramid structure, integrating multiple resolutions as input to train a series
of models with different receptive fields, aiming to achieve the optimal
feature combination for each landmark. Moreover, we applied several data
augmentation techniques during training to enhance the model's robustness
across various devices and measurement alternatives. We implemented this method
in the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge
and achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate
(SDR) 2.0mm of 74.18% in the final testing phase.


# How to fix a broken confidence estimator: Evaluating post-hoc methods for selective classification with deep neural networks

[Link to the paper](http://arxiv.org/abs/2305.15508v2)

## Authors
- Luís Felipe P. Cattelan
- Danilo Silva

## Summary
  This paper addresses the problem of selective classification for deep neural
networks, where a model is allowed to abstain from low-confidence predictions
to avoid potential errors. We focus on so-called post-hoc methods, which
replace the confidence estimator of a given classifier without retraining or
modifying it, thus being practically appealing. Considering neural networks
with softmax outputs, our goal is to identify the best confidence estimator
that can be computed directly from the unnormalized logits. This problem is
motivated by the intriguing observation in recent work that many classifiers
appear to have a "broken" confidence estimator, in the sense that their
selective classification performance is much worse than what could be expected
by their corresponding accuracies. We perform an extensive experimental study
of many existing and proposed confidence estimators applied to 84 pretrained
ImageNet classifiers available from popular repositories. Our results show that
a simple $p$-norm normalization of the logits, followed by taking the maximum
logit as the confidence estimator, can lead to considerable gains in selective
classification performance, completely fixing the pathological behavior
observed in many classifiers. As a consequence, the selective classification
performance of any classifier becomes almost entirely determined by its
corresponding accuracy. Moreover, these results are shown to be consistent
under distribution shift. We also investigate why certain classifiers innately
have a good confidence estimator that apparently cannot be improved by post-hoc
methods.


# Introduction to 2-dimensional Topological Quantum Field Theory

[Link to the paper](http://arxiv.org/abs/2206.12448v2)

## Authors
- Leon Menger

## Summary
  Mostly self-contained script on functorial topological quantum field
theories. These notes give a slow introduction to the basic notions of category
theory which serve a closer investigation of cobordisms and (commutative)
Frobenius algebras. In the fourth chapter the axiomatic definition of TQFTs is
motivated and some folklore results about the equivalence of (symmetric)
monoidal functors and (commutative) Frobenius algebras are proven. The script
can serve as material for an introductory course.


# PersA-FL: Personalized Asynchronous Federated Learning

[Link to the paper](http://arxiv.org/abs/2210.01176v2)

## Authors
- Mohammad Taha Toghani
- Soomin Lee
- César A. Uribe

## Summary
  We study the personalized federated learning problem under asynchronous
updates. In this problem, each client seeks to obtain a personalized model that
simultaneously outperforms local and global models. We consider two
optimization-based frameworks for personalization: (i) Model-Agnostic
Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a
joint model adapted for each client through fine-tuning, whereas ME requires a
bi-level optimization problem with implicit gradients to enforce
personalization via regularized losses. We focus on improving the scalability
of personalized federated learning by removing the synchronous communication
assumption. Moreover, we extend the studied function class by removing
boundedness assumptions on the gradient norm. Our main technical contribution
is a unified proof for asynchronous federated learning with bounded staleness
that we apply to MAML and ME personalization frameworks. For the smooth and
non-convex functions class, we show the convergence of our method to a
first-order stationary point. We illustrate the performance of our method and
its tolerance to staleness through experiments for classification tasks over
heterogeneous datasets.


# SU(d)-Symmetric Random Unitaries: Quantum Scrambling, Error Correction, and Machine Learning

[Link to the paper](http://arxiv.org/abs/2309.16556v2)

## Authors
- Zimu Li
- Han Zheng
- Yunfei Wang
- Liang Jiang
- Zi-Wen Liu
- Junyu Liu

## Summary
  Quantum information processing in the presence of continuous symmetry is of
wide importance and exhibits many novel physical and mathematical phenomena.
SU(d) is a continuous symmetry group of particular interest since it represents
a fundamental type of non-Abelian symmetry and also plays a vital role in
quantum computation. Here, we explicate the applications of SU(d)-symmetric
random unitaries in three different contexts ranging from physics to quantum
computing: information scrambling with non-Abelian conserved quantities,
covariant quantum error correcting random codes, and geometric quantum machine
learning. First, we show that, in the presence of SU(d) symmetry, the local
conserved quantities would exhibit residual values even at $t \rightarrow
\infty$ which decays as $\Omega(1/n^{3/2})$ under local Pauli basis for qubits
and $\Omega(1/n^{(d+2)^2/2})$ under local symmetric basis for general qudits
with respect to the system size, in contrast to $O(1/n)$ decay for U(1) case
and the exponential decay for no-symmetry case in the sense of out-of-time
ordered correlator (OTOC). Second, we show that SU(d)-symmetric unitaries can
be used to construct asymptotically optimal (in the sense of saturating the
fundamental limits on the code error that have been called the approximate
Eastin-Knill theorems) SU(d)-covariant codes that encodes any constant $k$
logical qudits, extending [Kong \& Liu; PRXQ 3, 020314 (2022)]. Finally, we
derive an overpartameterization threshold via the quantum neural tangent kernel
(QNTK) required for exponential convergence guarantee of generic ansatz for
geometric quantum machine learning, which reveals that the number of parameters
required scales only with the dimension of desired subspaces rather than that
of the entire Hilbert space. We expect that our work invites further research
on quantum information with continuous symmetries.


# "My sex-related data is more sensitive than my financial data and I want the same level of security and privacy": User Risk Perceptions and Protective Actions in Female-oriented Technologies

[Link to the paper](http://arxiv.org/abs/2306.05956v2)

## Authors
- Maryam Mehrnezhad
- Teresa Almeida

## Summary
  The digitalization of the reproductive body has engaged myriads of
cutting-edge technologies in supporting people to know and tackle their
intimate health. Generally understood as female technologies (aka
female-oriented technologies or 'FemTech'), these products and systems collect
a wide range of intimate data which are processed, transferred, saved and
shared with other parties. In this paper, we explore how the "data-hungry"
nature of this industry and the lack of proper safeguarding mechanisms,
standards, and regulations for vulnerable data can lead to complex harms or
faint agentic potential. We adopted mixed methods in exploring users'
understanding of the security and privacy (SP) of these technologies. Our
findings show that while users can speculate the range of harms and risks
associated with these technologies, they are not equipped and provided with the
technological skills to protect themselves against such risks. We discuss a
number of approaches, including participatory threat modelling and SP by
design, in the context of this work and conclude that such approaches are
critical to protect users in these sensitive systems.


# Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models

[Link to the paper](http://arxiv.org/abs/2310.02848v1)

## Authors
- Siyuan Yang
- Lu Zhang
- Liqian Ma
- Yu Liu
- JingJing Fu
- You He

## Summary
  Image inpainting aims to fill in the missing pixels with visually coherent
and semantically plausible content. Despite the great progress brought from
deep generative models, this task still suffers from i. the difficulties in
large-scale realistic data collection and costly model training; and ii. the
intrinsic limitations in the traditionally user-defined binary masks on objects
with unclear boundaries or transparent texture. In this paper, we propose
MagicRemover, a tuning-free method that leverages the powerful diffusion models
for text-guided image inpainting. We introduce an attention guidance strategy
to constrain the sampling process of diffusion models, enabling the erasing of
instructed areas and the restoration of occluded content. We further propose a
classifier optimization algorithm to facilitate the denoising stability within
less sampling steps. Extensive comparisons are conducted among our MagicRemover
and state-of-the-art methods including quantitative evaluation and user study,
demonstrating the significant improvement of MagicRemover on high-quality image
inpainting. We will release our code at https://github.com/exisas/Magicremover.


# On the Length of Strongly Monotone Descending Chains over $\mathbb{N}^d$

[Link to the paper](http://arxiv.org/abs/2310.02847v1)

## Authors
- Sylvain Schmitz
- Lia Schütze

## Summary
  A recent breakthrough by K\"unnemann, Mazowiecki, Sch\"utze, Sinclair-Banks,
and Wegrzycki (ICALP, 2023) bounds the running time for the coverability
problem in $d$-dimensional vector addition systems under unary encoding to
$n^{2^{O(d)}}$, improving on Rackoff's $n^{2^{O(d\lg d)}}$ upper bound (Theor.
Comput. Sci., 1978), and provides conditional matching lower bounds.
  In this paper, we revisit Lazi\'c and Schmitz' "ideal view" of the backward
coverability algorithm (Inform. Comput., 2021) in the light of this
breakthrough. We show that the controlled strongly monotone descending chains
of downwards-closed sets over $\mathbb{N}^d$ that arise from the dual backward
coverability algorithm of Lazi\'c and Schmitz on $d$-dimensional unary vector
addition systems also enjoy this tight $n^{2^{O(d)}}$ upper bound on their
length, and that this also translates into the same bound on the running time
of the backward coverability algorithm.
  Furthermore, our analysis takes place in a more general setting than that of
Lazi\'c and Schmitz, which allows to show the same results and improve on the
2EXPSPACE upper bound derived by Benedikt, Duff, Sharad, and Worrell (LICS,
2017) for the coverability problem in invertible affine nets.


# Phases of cold holographic QCD: baryons, pions and rho mesons

[Link to the paper](http://arxiv.org/abs/2302.10675v3)

## Authors
- Nicolas Kovensky
- Aaron Poole
- Andreas Schmitt

## Summary
  We improve the holographic description of isospin-asymmetric baryonic matter
within the Witten-Sakai-Sugimoto model by accounting for a realistic pion mass,
computing the pion condensate dynamically, and including rho meson condensation
by allowing the gauge field in the bulk to be anisotropic. This description
takes into account the coexistence of baryonic matter with pion and rho meson
condensates. Our main result is the zero-temperature phase diagram in the plane
of baryon and isospin chemical potentials. We find that the effective pion mass
in the baryonic medium increases with baryon density and that, as a
consequence, there is no pion condensation in neutron-star matter. Our improved
description also predicts that baryons are disfavored at low baryon chemical
potentials even for arbitrarily large isospin chemical potential. Instead, rho
meson condensation sets in on top of the pion condensate at an isospin chemical
potential of about $9.4\, m_\pi$. We further observe a highly non-monotonic
phase boundary regarding the disappearance of pion condensation.


# Stiefel-Whitney Classes Of Representations Of Dihedral Groups

[Link to the paper](http://arxiv.org/abs/2307.14647v2)

## Authors
- Sujeet Bhalerao
- Rohit Joshi
- Neha Malik

## Summary
  We compute the Stiefel-Whitney Classes for representations of dihedral groups
$D_m$ in terms of character values of order two elements. We also provide
criteria to identify representations V which lift to the double covers of the
orthogonal group O(V ) and those with non-trivial mod 2 Euler class.


# Note on a Translation from First-Order Logic into the Calculus of Relations Preserving Validity and Finite Validity

[Link to the paper](http://arxiv.org/abs/2310.02845v1)

## Authors
- Yoshiki Nakamura

## Summary
  In this note, we give a linear-size translation from formulas of first-order
logic into equations of the calculus of relations preserving validity and
finite validity. Our translation also gives a linear-size conservative
reduction from formulas of first-order logic into formulas of the
three-variable fragment of first-order logic.


# MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology Whole Slide Images

[Link to the paper](http://arxiv.org/abs/2309.10650v2)

## Authors
- Amaya Gallagher-Syed
- Luca Rossi
- Felice Rivellese
- Costantino Pitzalis
- Myles Lewis
- Michael Barnes
- Gregory Slabaugh

## Summary
  Whole Slide Images (WSIs) present a challenging computer vision task due to
their gigapixel size and presence of numerous artefacts. Yet they are a
valuable resource for patient diagnosis and stratification, often representing
the gold standard for diagnostic tasks. Real-world clinical datasets tend to
come as sets of heterogeneous WSIs with labels present at the patient-level,
with poor to no annotations. Weakly supervised attention-based multiple
instance learning approaches have been developed in recent years to address
these challenges, but can fail to resolve both long and short-range
dependencies. Here we propose an end-to-end multi-stain self-attention graph
(MUSTANG) multiple instance learning pipeline, which is designed to solve a
weakly-supervised gigapixel multi-image classification task, where the label is
assigned at the patient-level, but no slide-level labels or region annotations
are available. The pipeline uses a self-attention based approach by restricting
the operations to a highly sparse k-Nearest Neighbour Graph of embedded WSI
patches based on the Euclidean distance. We show this approach achieves a
state-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAM
model. Our approach is highly modular and can easily be modified to suit
different clinical datasets, as it only requires a patient-level label without
annotations and accepts WSI sets of different sizes, as the graphs can be of
varying sizes and structures. The source code can be found at
https://github.com/AmayaGS/MUSTANG.


# MF-Box: Multi-fidelity and multi-scale emulation for the matter power spectrum

[Link to the paper](http://arxiv.org/abs/2306.03144v2)

## Authors
- Ming-Feng Ho
- Simeon Bird
- Martin A. Fernandez
- Christian R. Shelton

## Summary
  We introduce MF-Box, an extended version of MFEmulator, designed as a fast
surrogate for power spectra, trained using N-body simulation suites from
various box sizes and particle loads. To demonstrate MF-Box's effectiveness, we
design simulation suites that include low-fidelity suites (L1 and L2) at $256
\,\mathrm{Mpc}/h$ and $100 \,\mathrm{Mpc}/h$, each with $128^3$ particles, and
a high-fidelity suite (HF) with $512^3$ particles at $256 \,\mathrm{Mpc}/h$,
representing a higher particle load compared to the low-fidelity suites. MF-Box
acts as a probabilistic resolution correction function, learning most of the
cosmological dependencies from L1 and L2 simulations and rectifying resolution
differences with just 3 HF simulations using a Gaussian process. MF-Box
successfully emulates power spectra from our HF testing set with a relative
error of $< 3\%$ up to $k \simeq 7 \,h/\mathrm{Mpc}$ at $z \in [0, 3]$, while
maintaining a cost similar to our previous multi-fidelity approach, which was
accurate only up to $z = 1$. The addition of an extra low-fidelity node in a
smaller box significantly improves emulation accuracy for MF-Box at $k > 2
\,h/\mathrm{Mpc}$, increasing it by a factor of $10$. We conduct an error
analysis of MF-Box based on computational budget, providing guidance for
optimizing budget allocation per fidelity node. Our proposed MF-Box enables
future surveys to efficiently combine simulation suites of varying quality,
effectively expanding the range of emulation capabilities while ensuring cost
efficiency.


# Caustics in Self-gravitating N-body systems and Cosmological Large Scale Structures

[Link to the paper](http://arxiv.org/abs/2309.15296v2)

## Authors
- George Savvidy

## Summary
  Extended galaxy surveys revealed that at a large scale the Universe consists
of matter concentrations in the form of galactic clusters, filaments and vast
regions devoid of galaxies. In this paper we demonstrate the generation of
caustics in self-gravitating N-body system and the formation of cosmological
large-scale structure due to the gravitational focusing. The gravitational
caustics are space regions where the density of matter is higher than the
average density in the surrounding Universe. The gravitational caustics can
represent galaxies, galactic clusters and filaments and the regions between
caustics, the voids.
  In our approach the dynamics of a self-gravitating N-body system is
formulated in terms of a geodesic flow on a curved Riemannian manifold of
dimension 3N equipped by the Maupertuis's metric. The regions of negative
sectional curvatures are responsible for the exponential instability of
geodesic trajectories, while the regions of positive sectional curvatures are
responsible for the gravitational focusing of geodesics and generation of
caustics. The stability of geodesic trajectories is analysed by means of the
Jacobi equation and the gravitational focusing of geodesics by means of the
Raychaundhuri equation. By solving these equations we estimated the
characteristic relaxation time scales and the time scale of generation of
gravitational caustics.


# Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation

[Link to the paper](http://arxiv.org/abs/2310.02842v1)

## Authors
- Chen Dun
- Mirian Del Carmen Hipolito Garcia
- Guoqing Zheng
- Ahmed Hassan Awadallah
- Anastasios Kyrillidis
- Robert Sim

## Summary
  Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training "interference" in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.


# Traversable wormholes in bi-metric gravity

[Link to the paper](http://arxiv.org/abs/2305.13013v2)

## Authors
- Mostafizur Rahman
- Anjan A Sen
- Sunil Singh Bohra

## Summary
  The ghost-free bi-metric gravity theory is a viable theory of gravity that
explores the interaction between a massless and a massive graviton and can be
described in terms of two dynamical metrics. In this paper, we present an exact
static, spherically symmetric vacuum solution within this theory. The solution
is spatially Schwarzschild-de Sitter, with the value of the cosmological
constant determined by the graviton mass and the interaction parameters of the
theory. Notably, for specific parameter ranges, the solution represents a
traversable Lorentzian wormhole that violates the weak energy condition near
its throat. Furthermore, we have investigated the evolution of scalar and
electromagnetic fields in this wormhole spacetime and observed the presence of
arbitrarily long-lived quasi-resonant modes in the quasinormal spectrum.


# Many Cores, Many Models: GPU Programming Model vs. Vendor Compatibility Overview

[Link to the paper](http://arxiv.org/abs/2309.05445v3)

## Authors
- Andreas Herten

## Summary
  In recent history, GPUs became a key driver of compute performance in HPC.
With the installation of the Frontier supercomputer, they became the enablers
of the Exascale era; further largest-scale installations are in progress
(Aurora, El Capitan, JUPITER). But the early-day dominance by NVIDIA and their
CUDA programming model has changed: The current HPC GPU landscape features
three vendors (AMD, Intel, NVIDIA), each with native and derived programming
models. The choices are ample, but not all models are supported on all
platforms, especially if support for Fortran is needed; in addition, some
restrictions might apply. It is hard for scientific programmers to navigate
this abundance of choices and limits.
  This paper gives a guide by matching the GPU platforms with supported
programming models, presented in a concise table and further elaborated in
detailed comments. An assessment is made regarding the level of support of a
model on a platform.


# The post-Newtonian motion around an oblate spheroid: the mixed orbital effects due to the Newtonian oblateness and the post-Newtonian mass monopole accelerations

[Link to the paper](http://arxiv.org/abs/2310.02838v1)

## Authors
- Lorenzo Iorio

## Summary
  When a test particle moves about an oblate spheroid, it is acted upon, among
other things, by two standard perturbing accelerations. One, of Newtonian
origin, is due to the quadrupole mass moment $J_2$ of the orbited body. The
other one, of the order of $\mathcal{O}\left(1/c^2\right)$, is caused by the
static, post-Newtonian field arising solely from the mass of the central
object. Both of them concur to induce \textrm{indirect}, \textrm{mixed} orbital
effects of the order of $\mathcal{O}\left(J_2/c^2\right)$. They are of the same
order of magnitude of the \textrm{direct} ones induced by the post-Newtonian
acceleration arising in presence of an oblate source, not treated here. We
calculate these less known features of motion in their full generality in terms
of the osculating Keplerian orbital elements. Subtleties pertaining the correct
calculation of their mixed net \textrm{precessions} per orbit to the full order
of $\mathcal{O}\left(J_2/c^2\right)$ are elucidated. The obtained results hold
for arbitrary orbital geometries and for any orientation of the body's spin
axis $\boldsymbol{\hat{k}}$ in space. The method presented is completely
general, and can be extended to any pair of post-Keplerian accelerations
entering the equations of motion of the satellite, irrespectively of their
physical nature.


# Quantum computing quantum Monte Carlo with hybrid tensor network for electronic structure calculations

[Link to the paper](http://arxiv.org/abs/2303.18095v2)

## Authors
- Shu Kanno
- Hajime Nakamura
- Takao Kobayashi
- Shigeki Gocho
- Miho Hatanaka
- Naoki Yamamoto
- Qi Gao

## Summary
  Quantum computing quantum Monte Carlo (QC-QMC) is an algorithm that can be
combined with quantum algorithms such as variational quantum eigensolver (VQE)
to obtain the ground state with higher accuracy than either VQE or QMC alone.
Here we propose an algorithm combining QC-QMC with hybrid tensor network (HTN)
to extend the applicability of QC-QMC for the system beyond the size of a
single quantum device, called HTN+QMC. For HTN in a two-layer quantum-quantum
tree tensor, HTN+QMC for an $O(n^2)$-qubit trial wave function can be executed
by using only a $n$-qubit device excluding ancilla qubits. Our algorithm is
evaluated on the Heisenberg chain model, the graphite-based Hubbard model, the
hydrogen plane model, and MonoArylBiImidazole using full configuration
interaction QMC. We found that the algorithm can achieve energy accuracy
several orders of magnitude higher than VQE or QMC, and HTN+QMC gives the same
energy accuracy as QC-QMC when the system is appropriately decomposed.
Moreover, we develop a pseudo-Hadamard test technique that enables efficient
overlap calculations between a trial wave function and a standard basis state.
In a real device experiment using the technique, we obtained almost the same
accuracy as the statevector simulator, indicating the noise robustness of
HTN+QMC.


# On extra dimensions and the cosmological constant problem

[Link to the paper](http://arxiv.org/abs/2310.02837v1)

## Authors
- Grzegorz Plewa

## Summary
  We consider the idea of large extra dimensions as a potential resolution to
the cosmological constant problem. We discuss a very simple model of a massive
scalar field with coordinate-dependent mass, satisfying Dirichlet boundary
conditions on a brane. We quantize the theory calculating the zero-point
energy. Based on the results, we find the lower bound for the uncertainty
product in the uncertainty principle. We show that the zero-point energy
density could be small if extra dimensions are present. We start with an
arbitrary dimensionality of space, later restricting ourselves to ten and
eleven dimensions. In both cases the energy is parameterized by the number of
extra dimensions and additional dimensionless saturation parameter, expressing
the deviation from perfect saturation of the uncertainty principle. Letting the
parameter to be small and of order of the fine-structure constant, we reproduce
the experimental value of the cosmological constant in four dimensions.


# From Zero to Turbulence: Generative Modeling for 3D Flow Simulation

[Link to the paper](http://arxiv.org/abs/2306.01776v2)

## Authors
- Marten Lienen
- David Lüdke
- Jan Hansen-Palmus
- Stephan Günnemann

## Summary
  Simulations of turbulent flows in 3D are one of the most expensive
simulations in computational fluid dynamics (CFD). Many works have been written
on surrogate models to replace numerical solvers for fluid flows with faster,
learned, autoregressive models. However, the intricacies of turbulence in three
dimensions necessitate training these models with very small time steps, while
generating realistic flow states requires either long roll-outs with many steps
and significant error accumulation or starting from a known, realistic flow
state - something we aimed to avoid in the first place. Instead, we propose to
approach turbulent flow simulation as a generative task directly learning the
manifold of all possible turbulent flow states without relying on any initial
flow state. For our experiments, we introduce a challenging 3D turbulence
dataset of high-resolution flows and detailed vortex structures caused by
various objects and derive two novel sample evaluation metrics for turbulent
flows. On this dataset, we show that our generative model captures the
distribution of turbulent flows caused by unseen objects and generates
high-quality, realistic samples amenable for downstream applications without
access to any initial state.


# Realistic Neutral Atom Image Simulation

[Link to the paper](http://arxiv.org/abs/2310.02836v1)

## Authors
- Jonas Winklmann
- Dimitrios Tsevas
- Martin Schulz

## Summary
  Neutral atom quantum computers require accurate single atom detection for the
preparation and readout of their qubits. This is usually done using
fluorescence imaging. The occupancy of an atom site in these images is often
somewhat ambiguous due to the stochastic nature of the imaging process.
Further, the lack of ground truth makes it difficult to rate the accuracy of
reconstruction algorithms. We introduce a bottom-up simulator that is capable
of generating sample images of neutral atom experiments from a description of
the actual state in the simulated system. Possible use cases include the
creation of exemplary images for demonstration purposes, fast training
iterations for deconvolution algorithms, and generation of labeled data for
machine-learning-based atom detection approaches. The implementation is
available through our GitHub as a C library or wrapped Python package. We show
the modeled effects and implementation of the simulations at different stages
of the imaging process. Not all real-world phenomena can be reproduced
perfectly. The main discrepancies are that the simulator allows for only one
characterization of optical aberrations across the whole image, supports only
discrete atom locations, and does not model all effects of CMOS cameras
perfectly. Nevertheless, our experiments show that the generated images closely
match real-world pictures to the point that they are practically
indistinguishable and can be used as labeled data for training the next
generation of detection algorithms.


# Delving into CLIP latent space for Video Anomaly Recognition

[Link to the paper](http://arxiv.org/abs/2310.02835v1)

## Authors
- Luca Zanella
- Benedetta Liberatori
- Willi Menapace
- Fabio Poiesi
- Yiming Wang
- Elisa Ricci

## Summary
  We tackle the complex problem of detecting and recognising anomalies in
surveillance videos at the frame level, utilising only video-level supervision.
We introduce the novel method AnomalyCLIP, the first to combine Large Language
and Vision (LLV) models, such as CLIP, with multiple instance learning for
joint video anomaly detection and classification. Our approach specifically
involves manipulating the latent CLIP feature space to identify the normal
event subspace, which in turn allows us to effectively learn text-driven
directions for abnormal events. When anomalous frames are projected onto these
directions, they exhibit a large feature magnitude if they belong to a
particular class. We also introduce a computationally efficient Transformer
architecture to model short- and long-term temporal dependencies between
frames, ultimately producing the final anomaly score and class prediction
probabilities. We compare AnomalyCLIP against state-of-the-art methods
considering three major anomaly detection benchmarks, i.e. ShanghaiTech,
UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines
in recognising video anomalies.


# Post-Newtonian orbital effects induced by the mass quadrupole and spin octupole moments of an axisymmetric body

[Link to the paper](http://arxiv.org/abs/2310.02834v1)

## Authors
- Lorenzo Iorio

## Summary
  The post-Newtonian orbital effects induced by the mass quadrupole and spin
octupole moments of an isolated, oblate spheroid of constant density that is
rigidly and uniformly rotating on the motion of a test particle are
analytically worked out for an arbitrary orbital configuration and without any
preferred orientation of the body's spin axis. The resulting expressions are
specialized to the cases of a) equatorial and b) polar orbits. The opportunity
offered by a hypothetical new spacecraft moving around Jupiter along a
Juno-like highly elliptical, polar orbit to measure them is preliminarily
studied. Although more difficult to be practically implemented, also the case
of a less elliptical orbit is considered since it yields much larger figures
for the relativistic effects of interest. The possibility of using the S-stars
orbiting the supermassive black hole in Sgr A$^\ast$ at the Galactic Center as
probes to potentially constrain some parameters of the predicted extended mass
distribution surrounding the hole by means of the aforementioned orbital
effects is briefly examined.


# All Sizes Matter: Improving Volumetric Brain Segmentation on Small Lesions

[Link to the paper](http://arxiv.org/abs/2310.02829v1)

## Authors
- Ayhan Can Erdur
- Daniel Scholz
- Josef A. Buchner
- Stephanie E. Combs
- Daniel Rueckert
- Jan C. Peeken

## Summary
  Brain metastases (BMs) are the most frequently occurring brain tumors. The
treatment of patients having multiple BMs with stereo tactic radiosurgery
necessitates accurate localization of the metastases. Neural networks can
assist in this time-consuming and costly task that is typically performed by
human experts. Particularly challenging is the detection of small lesions since
they are often underrepresented in exist ing approaches. Yet, lesion detection
is equally important for all sizes. In this work, we develop an ensemble of
neural networks explicitly fo cused on detecting and segmenting small BMs. To
accomplish this task, we trained several neural networks focusing on individual
aspects of the BM segmentation problem: We use blob loss that specifically
addresses the imbalance of lesion instances in terms of size and texture and
is, therefore, not biased towards larger lesions. In addition, a model using a
subtraction sequence between the T1 and T1 contrast-enhanced sequence focuses
on low-contrast lesions. Furthermore, we train additional models only on small
lesions. Our experiments demonstrate the utility of the ad ditional blob loss
and the subtraction sequence. However, including the specialized small lesion
models in the ensemble deteriorates segmentation results. We also find
domain-knowledge-inspired postprocessing steps to drastically increase our
performance in most experiments. Our approach enables us to submit a
competitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge
2023.


# Measurement of the Positive Muon Anomalous Magnetic Moment to 0.20 ppm

[Link to the paper](http://arxiv.org/abs/2308.06230v2)

## Authors
- D. P. Aguillard
- T. Albahri
- D. Allspach
- A. Anisenkov
- K. Badgley
- S. Baeßler
- I. Bailey
- L. Bailey
- V. A. Baranov
- E. Barlas-Yucel
- T. Barrett
- E. Barzi
- F. Bedeschi
- M. Berz
- M. Bhattacharya
- H. P. Binney
- P. Bloom
- J. Bono
- E. Bottalico
- T. Bowcock
- S. Braun
- M. Bressler
- G. Cantatore
- R. M. Carey
- B. C. K. Casey
- D. Cauz
- R. Chakraborty
- A. Chapelain
- S. Chappa
- S. Charity
- C. Chen
- M. Cheng
- R. Chislett
- Z. Chu
- T. E. Chupp
- C. Claessens
- M. E. Convery
- S. Corrodi
- L. Cotrozzi
- J. D. Crnkovic
- S. Dabagov
- P. T. Debevec
- S. Di Falco
- G. Di Sciascio
- B. Drendel
- A. Driutti
- V. N. Duginov
- M. Eads
- A. Edmonds
- J. Esquivel
- M. Farooq
- R. Fatemi
- C. Ferrari
- M. Fertl
- A. T. Fienberg
- A. Fioretti
- D. Flay
- S. B. Foster
- H. Friedsam
- N. S. Froemming
- C. Gabbanini
- I. Gaines
- M. D. Galati
- S. Ganguly
- A. Garcia
- J. George
- L. K. Gibbons
- A. Gioiosa
- K. L. Giovanetti
- P. Girotti
- W. Gohn
- L. Goodenough
- T. Gorringe
- J. Grange
- S. Grant
- F. Gray
- S. Haciomeroglu
- T. Halewood-Leagas
- D. Hampai
- F. Han
- J. Hempstead
- D. W. Hertzog
- G. Hesketh
- E. Hess
- A. Hibbert
- Z. Hodge
- K. W. Hong
- R. Hong
- T. Hu
- Y. Hu
- M. Iacovacci
- M. Incagli
- P. Kammel
- M. Kargiantoulakis
- M. Karuza
- J. Kaspar
- D. Kawall
- L. Kelton
- A. Keshavarzi
- D. S. Kessler
- K. S. Khaw
- Z. Khechadoorian
- N. V. Khomutov
- B. Kiburg
- M. Kiburg
- O. Kim
- N. Kinnaird
- E. Kraegeloh
- V. A. Krylov
- N. A. Kuchinskiy
- K. R. Labe
- J. LaBounty
- M. Lancaster
- S. Lee
- B. Li
- D. Li
- L. Li
- I. Logashenko
- A. Lorente Campos
- Z. Lu
- A. Lucà
- G. Lukicov
- A. Lusiani
- A. L. Lyon
- B. MacCoy
- R. Madrak
- K. Makino
- S. Mastroianni
- J. P. Miller
- S. Miozzi
- B. Mitra
- J. P. Morgan
- W. M. Morse
- J. Mott
- A. Nath
- J. K. Ng
- H. Nguyen
- Y. Oksuzian
- Z. Omarov
- R. Osofsky
- S. Park
- G. Pauletta
- G. M. Piacentino
- R. N. Pilato
- K. T. Pitts
- B. Plaster
- D. Počanić
- N. Pohlman
- C. C. Polly
- J. Price
- B. Quinn
- M. U. H. Qureshi
- S. Ramachandran
- E. Ramberg
- R. Reimann
- B. L. Roberts
- D. L. Rubin
- L. Santi
- C. Schlesier
- A. Schreckenberger
- Y. K. Semertzidis
- D. Shemyakin
- M. Sorbara
- J. Stapleton
- D. Still
- D. Stöckinger
- C. Stoughton
- D. Stratakis
- H. E. Swanson
- G. Sweetmore
- D. A. Sweigart
- M. J. Syphers
- D. A. Tarazona
- T. Teubner
- A. E. Tewsley-Booth
- V. Tishchenko
- N. H. Tran
- W. Turner
- E. Valetov
- D. Vasilkova
- G. Venanzoni
- V. P. Volnykh
- T. Walton
- A. Weisskopf
- L. Welty-Rieger
- P. Winter
- Y. Wu
- B. Yu
- M. Yucel
- Y. Zeng
- C. Zhang

## Summary
  We present a new measurement of the positive muon magnetic anomaly, $a_\mu
\equiv (g_\mu - 2)/2$, from the Fermilab Muon $g\!-\!2$ Experiment using data
collected in 2019 and 2020. We have analyzed more than 4 times the number of
positrons from muon decay than in our previous result from 2018 data. The
systematic error is reduced by more than a factor of 2 due to better running
conditions, a more stable beam, and improved knowledge of the magnetic field
weighted by the muon distribution, $\tilde{\omega}'^{}_p$, and of the anomalous
precession frequency corrected for beam dynamics effects, $\omega_a$. From the
ratio $\omega_a / \tilde{\omega}'^{}_p$, together with precisely determined
external parameters, we determine $a_\mu = 116\,592\,057(25) \times 10^{-11}$
(0.21 ppm). Combining this result with our previous result from the 2018 data,
we obtain $a_\mu\text{(FNAL)} = 116\,592\,055(24) \times 10^{-11}$ (0.20 ppm).
The new experimental world average is $a_\mu (\text{Exp}) =
116\,592\,059(22)\times 10^{-11}$ (0.19 ppm), which represents a factor of 2
improvement in precision.


# Highly strain-tunable charge valley transport in bismuth

[Link to the paper](http://arxiv.org/abs/2309.05285v2)

## Authors
- Suguru Hosoi
- Fumu Tachibana
- Mai Sakaguchi
- Kentaro Ishida
- Masaaki Shimozawa
- Koichi Izawa
- Yuki Fuseya
- Yuto Kinoshita
- Masashi Tokunaga

## Summary
  The manipulation of the valley degree of freedom can boost the technological
development of novel functional devices based on valleytronics. The current
mainstream platform for valleytronics is to produce a monolayer with inversion
asymmetry, in which the strain-band engineering through the substrates can
serve to improve the performance of valley-based devices. However, pinpointing
the effective role of strain is inevitable for the precise design of the
desired valley structure. Here, we demonstrate the valley-dependent charge
transport continuously controllable via the external strain for bulk bismuth
crystals with three equivalent electron valleys. The strain response of
resistance, namely elastoresistance, exhibits the evolutions in both
antisymmetric and symmetric channels with decreasing temperature. The
elastoresistance behaviors mainly reflect the significant changes in valley
density depending on the symmetry of induced strain, evidenced by our developed
semiclassical transport theory and strain-dependent quantum oscillation
measurements. These facts suggest the successful tune and evaluation of the
valley populations through strain-dependent charge valley transport.


# Series expansions in closed and open quantum many-body systems with multiple quasiparticle types

[Link to the paper](http://arxiv.org/abs/2302.01000v2)

## Authors
- L. Lenke
- A. Schellenberger
- K. P. Schmidt

## Summary
  The established approach of perturbative continuous unitary transformations
(pCUTs) constructs effective quantum many-body Hamiltonians as perturbative
series that conserve the number of one quasiparticle type. We extend the pCUT
method to similarity transformations - dubbed $\mathrm{pcst}^{\texttt{++}}$ -
allowing for multiple quasiparticle types with complex-valued energies. This
enlarges the field of application to closed and open quantum many-body systems
with unperturbed operators corresponding to arbitrary superimposed ladder
spectra. To this end, a generalized counting operator is combined with the
quasiparticle generator for open quantum systems recently introduced by
Schmiedinghoff and Uhrig (arXiv:2203.15532). The $\mathrm{pcst}^{\texttt{++}}$
then yields model-independent block-diagonal effective Hamiltonians and
Lindbladians allowing a linked-cluster expansion in the thermodynamic limit
similar to the conventional pCUT method. We illustrate the application of the
$\mathrm{pcst}^{\texttt{++}}$ method by discussing representative closed, open,
and non-Hermitian quantum systems.


# Improving Vision Anomaly Detection with the Guidance of Language Modality

[Link to the paper](http://arxiv.org/abs/2310.02821v1)

## Authors
- Dong Chen
- Kaihang Pan
- Guoming Wang
- Yueting Zhuang
- Siliang Tang

## Summary
  Recent years have seen a surge of interest in anomaly detection for tackling
industrial defect detection, event detection, etc. However, existing
unsupervised anomaly detectors, particularly those for the vision modality,
face significant challenges due to redundant information and sparse latent
space. Conversely, the language modality performs well due to its relatively
single data. This paper tackles the aforementioned challenges for vision
modality from a multimodal point of view. Specifically, we propose Cross-modal
Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and
Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue
and sparse space issue, respectively. CMER masks parts of the raw image and
computes the matching score with the text. Then, CMER discards irrelevant
pixels to make the detector focus on critical contents. To learn a more compact
latent space for the vision anomaly detector, CMLE learns a correlation
structure matrix from the language modality, and then the latent space of
vision modality will be learned with the guidance of the matrix. Thereafter,
the vision latent space will get semantically similar images closer. Extensive
experiments demonstrate the effectiveness of the proposed methods.
Particularly, CMG outperforms the baseline that only uses images by 16.81%.
Ablation experiments further confirm the synergy among the proposed methods, as
each component depends on the other to achieve optimal performance.


# Totally nonnegative part of the Peterson variety in Lie type A

[Link to the paper](http://arxiv.org/abs/2310.02819v1)

## Authors
- Hiraku Abe
- Haozhi Zeng

## Summary
  The Peterson variety (which we denote by $Y$) is a subvariety of the flag
variety, introduced by Dale Peterson to describe the quantum cohomology rings
of all the partial flag varieties. Motivated by the mirror symmetry for partial
flag varieties, Rietsch studied the totally nonnegative part $Y_{\ge0}$ and its
cell decomposition. Based on the structure of those cells, Rietsch gave the
following conjecture in Lie type A; as a cell decomposed space, $Y_{\ge0}$ is
homeomorphic to the cube $[0,1]^{\dim_{\mathbb{C}}Y}$. In this paper, we give a
proof of Rietsch's conjecture on $Y_{\ge0}$ in Lie type A by using toric
geometry which is closely related to the Peterson variety.


# Peterson varieties and toric orbifolds associated to Cartan matrices

[Link to the paper](http://arxiv.org/abs/2310.02818v1)

## Authors
- Hiraku Abe
- Haozhi Zeng

## Summary
  The Peterson variety is a remarkable variety introduced by Dale Peterson to
describe the quantum cohomology rings of all the partial flag varieties. The
rational cohomology ring of the Peterson variety is known to be isomorphic to
that of a particular toric orbifold which naturally arises from the given root
system. In this paper, we show that it is not an accidental algebraic
coincidence; we construct an explicit morphism from the Peterson variety to the
toric orbifold which induces a ring isomorphism between their rational
cohomology rings.


# CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity

[Link to the paper](http://arxiv.org/abs/2310.02815v1)

## Authors
- Hao Shi
- Chengshan Pang
- Jiaming Zhang
- Kailun Yang
- Yuhao Wu
- Huajian Ni
- Yining Lin
- Rainer Stiefelhagen
- Kaiwei Wang

## Summary
  Roadside camera-driven 3D object detection is a crucial task in intelligent
transportation systems, which extends the perception range beyond the
limitations of vision-centric vehicles and enhances road safety. While previous
studies have limitations in using only depth or height information, we find
both depth and height matter and they are in fact complementary. The depth
feature encompasses precise geometric cues, whereas the height feature is
primarily focused on distinguishing between various categories of height
intervals, essentially providing semantic context. This insight motivates the
development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D
object detection framework that integrates depth and height to construct robust
BEV representations. In essence, CoBEV estimates each pixel's depth and height
distribution and lifts the camera features into 3D space for lateral fusion
using the newly proposed two-stage complementary feature selection (CFS)
module. A BEV feature distillation framework is also seamlessly integrated to
further enhance the detection accuracy from the prior knowledge of the
fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D
detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as
the private Supremind-Road dataset, demonstrating that CoBEV not only achieves
the accuracy of the new state-of-the-art, but also significantly advances the
robustness of previous methods in challenging long-distance scenarios and noisy
camera disturbance, and enhances generalization by a large margin in
heterologous settings with drastic changes in scene and camera parameters. For
the first time, the vehicle AP score of a camera model reaches 80% on
DAIR-V2X-I in terms of easy mode. The source code will be made publicly
available at https://github.com/MasterHow/CoBEV.


# Expanding Small-Scale Datasets with Guided Imagination

[Link to the paper](http://arxiv.org/abs/2211.13976v5)

## Authors
- Yifan Zhang
- Daquan Zhou
- Bryan Hooi
- Kai Wang
- Jiashi Feng

## Summary
  The power of DNNs relies heavily on the quantity and quality of training
data. However, collecting and annotating data on a large scale is often
expensive and time-consuming. To address this issue, we explore a new task,
termed dataset expansion, aimed at expanding a ready-to-use small dataset by
automatically creating new labeled samples. To this end, we present a Guided
Imagination Framework (GIF) that leverages cutting-edge generative models like
DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data
from the input seed data. Specifically, GIF conducts data imagination by
optimizing the latent features of the seed data in the semantically meaningful
space of the prior model, resulting in the creation of photo-realistic images
with new content. To guide the imagination towards creating informative samples
for model training, we introduce two key criteria, i.e., class-maintained
information boosting and sample diversity promotion. These criteria are
verified to be essential for effective dataset expansion: GIF-SD obtains 13.5%
higher model accuracy on natural image datasets than unguided expansion with
SD. With these essential criteria, GIF successfully expands small datasets in
various scenarios, boosting model accuracy by 36.9% on average over six natural
image datasets and by 13.5% on average over three medical datasets. The source
code is available at https://github.com/Vanint/DatasetExpansion.


# Improved Anisotropic Gaussian Filters

[Link to the paper](http://arxiv.org/abs/2303.13278v2)

## Authors
- Alex Keilmann
- Michael Godehardt
- Ali Moghiseh
- Claudia Redenbach
- Katja Schladitz

## Summary
  Elongated anisotropic Gaussian filters are used for the orientation
estimation of fibers. In cases where computed tomography images are noisy,
roughly resolved, and of low contrast, they are the method of choice even if
being efficient only in virtual 2D slices. However, minor inaccuracies in the
anisotropic Gaussian filters can carry over to the orientation estimation.
Therefore, this paper proposes a modified algorithm for 2D anisotropic Gaussian
filters and shows that this improves their precision. Applied to synthetic
images of fiber bundles, it is more accurate and robust to noise. Finally, the
effectiveness of the approach is shown by applying it to real-world images of
sheet molding compounds.


# A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability

[Link to the paper](http://arxiv.org/abs/2310.02807v1)

## Authors
- Zijie Geng
- Xijun Li
- Jie Wang
- Xiao Li
- Yongdong Zhang
- Feng Wu

## Summary
  In the past few years, there has been an explosive surge in the use of
machine learning (ML) techniques to address combinatorial optimization (CO)
problems, especially mixed-integer linear programs (MILPs). Despite the
achievements, the limited availability of real-world instances often leads to
sub-optimal decisions and biased solver assessments, which motivates a suite of
synthetic MILP instance generation techniques. However, existing methods either
rely heavily on expert-designed formulations or struggle to capture the rich
features of real-world instances. To tackle this problem, we propose G2MILP,
which to the best of our knowledge is the first deep generative framework for
MILP instances. Specifically, G2MILP represents MILP instances as bipartite
graphs, and applies a masked variational autoencoder to iteratively corrupt and
replace parts of the original graphs to generate new ones. The appealing
feature of G2MILP is that it can learn to generate novel and realistic MILP
instances without prior expert-designed formulations, while preserving the
structures and computational hardness of real-world datasets, simultaneously.
Thus the generated instances can facilitate downstream tasks for enhancing MILP
solvers under limited data availability. We design a suite of benchmarks to
evaluate the quality of the generated MILP instances. Experiments demonstrate
that our method can produce instances that closely resemble real-world datasets
in terms of both structures and computational hardness.


# Electronic Phase Transformations and Energy Gap Variations in Uniaxial and Biaxial Strained Monolayer VS$_2$ TMDs: A Comprehensive DFT and Beyond-DFT Study

[Link to the paper](http://arxiv.org/abs/2309.08393v2)

## Authors
- Oguzhan Orhan
- Şener Özönder
- Soner Ozgen

## Summary
  In the rapidly evolving field of 2D materials, transition metal
dichalcogenides (TMDs) have emerged as compelling candidates for electronic
applications. This study investigates the electronic structure of the H-phase
monolayer VS$_2$ belonging to TMD family and the influence of strain on its
band structure through Density Functional Theory (DFT). We employ two different
pseudopotential approximations and a suite of computational methods including
DFT+U, GAUPBE, G0W0, and self-GW to provide a nuanced understanding of its
electronic band structure. A highlight of the study is its focus on how both
uniaxial and biaxial strains, ranging from -5% to +5%, affect the electronic
properties of the H-phase monolayer VS$_2$. Our comprehensive analysis reveals
that these tensile strains significantly widen the energy gap, with uniaxial
strains having a more pronounced effect than their biaxial counterparts. In
addition, we identify an intriguing phase transition from a semiconducting to a
metallic state under compressive strains, this transition is attributed to both
symmetry breaking and bond length variation in the uniaxial case, the bond
length in biaxial. These key findings not only enrich our understanding of the
intricate electronic behavior of monolayer VS$_2$ under different strains but
also pave the way for the design of innovative electronic devices using strain
engineering.


# DOMINO: A Dual-System for Multi-step Visual Language Reasoning

[Link to the paper](http://arxiv.org/abs/2310.02804v1)

## Authors
- Peifang Wang
- Olga Golovneva
- Armen Aghajanyan
- Xiang Ren
- Muhao Chen
- Asli Celikyilmaz
- Maryam Fazel-Zarandi

## Summary
  Visual language reasoning requires a system to extract text or numbers from
information-dense images like charts or plots and perform logical or arithmetic
reasoning to arrive at an answer. To tackle this task, existing work relies on
either (1) an end-to-end vision-language model trained on a large amount of
data, or (2) a two-stage pipeline where a captioning model converts the image
into text that is further read by another large language model to deduce the
answer. However, the former approach forces the model to answer a complex
question with one single step, and the latter approach is prone to inaccurate
or distracting information in the converted text that can confuse the language
model. In this work, we propose a dual-system for multi-step multimodal
reasoning, which consists of a "System-1" step for visual information
extraction and a "System-2" step for deliberate reasoning. Given an input,
System-2 breaks down the question into atomic sub-steps, each guiding System-1
to extract the information required for reasoning from the image. Experiments
on chart and plot datasets show that our method with a pre-trained System-2
module performs competitively compared to prior work on in- and
out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on
only a small amount of data on multi-step reasoning, the accuracy of our method
is further improved and surpasses the best fully-supervised end-to-end approach
by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging
dataset with human-authored questions.


# AQUILA: Communication Efficient Federated Learning with Adaptive Quantization in Device Selection Strategy

[Link to the paper](http://arxiv.org/abs/2308.00258v2)

## Authors
- Zihao Zhao
- Yuzhu Mao
- Zhenpeng Shi
- Yang Liu
- Tian Lan
- Wenbo Ding
- Xiao-Ping Zhang

## Summary
  The widespread adoption of Federated Learning (FL), a privacy-preserving
distributed learning methodology, has been impeded by the challenge of high
communication overheads, typically arising from the transmission of large-scale
models. Existing adaptive quantization methods, designed to mitigate these
overheads, operate under the impractical assumption of uniform device
participation in every training round. Additionally, these methods are limited
in their adaptability due to the necessity of manual quantization level
selection and often overlook biases inherent in local devices' data, thereby
affecting the robustness of the global model. In response, this paper
introduces AQUILA (adaptive quantization in device selection strategy), a novel
adaptive framework devised to effectively handle these issues, enhancing the
efficiency and robustness of FL. AQUILA integrates a sophisticated device
selection method that prioritizes the quality and usefulness of device updates.
Utilizing the exact global model stored by devices, it enables a more precise
device selection criterion, reduces model deviation, and limits the need for
hyperparameter adjustments. Furthermore, AQUILA presents an innovative
quantization criterion, optimized to improve communication efficiency while
assuring model convergence. Our experiments demonstrate that AQUILA
significantly decreases communication costs compared to existing methods, while
maintaining comparable model performance across diverse non-homogeneous FL
settings, such as Non-IID data and heterogeneous model architectures.


# Trimap-guided Feature Mining and Fusion Network for Natural Image Matting

[Link to the paper](http://arxiv.org/abs/2112.00510v4)

## Authors
- Weihao Jiang
- Dongdong Yu
- Zhaozhi Xie
- Yaoyi Li
- Zehuan Yuan
- Hongtao Lu

## Summary
  Utilizing trimap guidance and fusing multi-level features are two important
issues for trimap-based matting with pixel-level prediction. To utilize trimap
guidance, most existing approaches simply concatenate trimaps and images
together to feed a deep network or apply an extra network to extract more
trimap guidance, which meets the conflict between efficiency and effectiveness.
For emerging content-based feature fusion, most existing matting methods only
focus on local features which lack the guidance of a global feature with strong
semantic information related to the interesting object. In this paper, we
propose a trimap-guided feature mining and fusion network consisting of our
trimap-guided non-background multi-scale pooling (TMP) module and global-local
context-aware fusion (GLF) modules. Considering that trimap provides strong
semantic guidance, our TMP module focuses effective feature mining on
interesting objects under the guidance of trimap without extra parameters.
Furthermore, our GLF modules use global semantic information of interesting
objects mined by our TMP module to guide an effective global-local
context-aware multi-level feature fusion. In addition, we build a common
interesting object matting (CIOM) dataset to advance high-quality image
matting. Particularly, results on the Composition-1k and our CIOM show that our
TMFNet achieves 13% and 25% relative improvement on SAD, respectively, against
a strong baseline with fewer parameters and 14% fewer FLOPs. Experimental
results on the Composition-1k test set, Alphamatting benchmark, and our CIOM
test set demonstrate that our method outperforms state-of-the-art approaches.
Our code and models are available at
https://github.com/Serge-weihao/TMF-Matting.


# Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning

[Link to the paper](http://arxiv.org/abs/2310.00648v2)

## Authors
- Lauren Hong
- Ting Wang

## Summary
  Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of
pre-trained language models (PLMs) to specific tasks. By tuning only a minimal
set of (extra) parameters, PEFT achieves performance comparable to full
fine-tuning. However, despite its prevalent use, the security implications of
PEFT remain largely unexplored. In this paper, we conduct a pilot study
revealing that PEFT exhibits unique vulnerability to trojan attacks.
Specifically, we present PETA, a novel attack that accounts for downstream
adaptation through bilevel optimization: the upper-level objective embeds the
backdoor into a PLM while the lower-level objective simulates PEFT to retain
the PLM's task-specific performance. With extensive evaluation across a variety
of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in
terms of both attack success rate and unaffected clean accuracy, even after the
victim user performs PEFT over the backdoored PLM using untainted data.
Moreover, we empirically provide possible explanations for PETA's efficacy: the
bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules,
thereby retaining the backdoor throughout PEFT. Based on this insight, we
explore a simple defense that omits PEFT in selected layers of the backdoored
PLM and unfreezes a subset of these layers' parameters, which is shown to
effectively neutralize PETA.


# Everest: GPU-Accelerated System For Mining Temporal Motifs

[Link to the paper](http://arxiv.org/abs/2310.02800v1)

## Authors
- Yichao Yuan
- Haojie Ye
- Sanketh Vedula Wynn Kaza
- Nishil Talati

## Summary
  Temporal motif mining is the task of finding the occurrences of subgraph
patterns within a large input temporal graph that obey the specified structural
and temporal constraints. Despite its utility in several critical application
domains that demand high performance (e.g., detecting fraud in financial
transaction graphs), the performance of existing software is limited on
commercial hardware platforms, in that it runs for tens of hours. This paper
presents Everest - a system that efficiently maps the workload of mining
(supports both enumeration and counting) temporal motifs to the highly parallel
GPU architecture. In particular, using an input temporal graph and a more
expressive user-defined temporal motif query definition compared to prior
works, Everest generates an execution plan and runtime primitives that optimize
the workload execution by exploiting the high compute throughput of a GPU.
Everest generates motif-specific mining code to reduce long-latency memory
accesses and frequent thread divergence operations. Everest incorporates novel
low-cost runtime mechanisms to enable load balancing to improve GPU hardware
utilization. To support large graphs that do not fit on GPU memory, Everest
also supports multi-GPU execution by intelligently partitioning the edge list
that prevents inter-GPU communication. Everest hides the implementation
complexity of presented optimizations away from the targeted system user for
better usability. Our evaluation shows that, using proposed optimizations,
Everest improves the performance of a baseline GPU implementation by 19x, on
average.


# Sparse Autoencoders Find Highly Interpretable Features in Language Models

[Link to the paper](http://arxiv.org/abs/2309.08600v3)

## Authors
- Hoagy Cunningham
- Aidan Ewart
- Logan Riggs
- Robert Huben
- Lee Sharkey

## Summary
  One of the roadblocks to a better understanding of neural networks' internals
is \textit{polysemanticity}, where neurons appear to activate in multiple,
semantically distinct contexts. Polysemanticity prevents us from identifying
concise, human-understandable explanations for what neural networks are doing
internally. One hypothesised cause of polysemanticity is
\textit{superposition}, where neural networks represent more features than they
have neurons by assigning features to an overcomplete set of directions in
activation space, rather than to individual neurons. Here, we attempt to
identify those directions, using sparse autoencoders to reconstruct the
internal activations of a language model. These autoencoders learn sets of
sparsely activating features that are more interpretable and monosemantic than
directions identified by alternative approaches, where interpretability is
measured by automated methods. Moreover, we show that with our learned set of
features, we can pinpoint the features that are causally responsible for
counterfactual behaviour on the indirect object identification task
\citep{wang2022interpretability} to a finer degree than previous
decompositions. This work indicates that it is possible to resolve
superposition in language models using a scalable, unsupervised method. Our
method may serve as a foundation for future mechanistic interpretability work,
which we hope will enable greater model transparency and steerability.


# GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects

[Link to the paper](http://arxiv.org/abs/2309.16264v2)

## Authors
- Qiaojun Yu
- Junbo Wang
- Wenhai Liu
- Ce Hao
- Liu Liu
- Lin Shao
- Weiming Wang
- Cewu Lu

## Summary
  Articulated objects like cabinets and doors are widespread in daily life.
However, directly manipulating 3D articulated objects is challenging because
they have diverse geometrical shapes, semantic categories, and kinetic
constraints. Prior works mostly focused on recognizing and manipulating
articulated objects with specific joint types. They can either estimate the
joint parameters or distinguish suitable grasp poses to facilitate trajectory
planning. Although these approaches have succeeded in certain types of
articulated objects, they lack generalizability to unseen objects, which
significantly impedes their application in broader scenarios. In this paper, we
propose a novel framework of Generalizable Articulation Modeling and
Manipulating for Articulated Objects (GAMMA), which learns both articulation
modeling and grasp pose affordance from diverse articulated objects with
different categories. In addition, GAMMA adopts adaptive manipulation to
iteratively reduce the modeling errors and enhance manipulation performance. We
train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive
experiments in SAPIEN simulation and real-world Franka robot. Results show that
GAMMA significantly outperforms SOTA articulation modeling and manipulation
algorithms in unseen and cross-category articulated objects. We will
open-source all codes and datasets in both simulation and real robots for
reproduction in the final version. Images and videos are published on the
project website at: http://sites.google.com/view/gamma-articulation


# Tracking Anything in Heart All at Once

[Link to the paper](http://arxiv.org/abs/2310.02792v1)

## Authors
- Chengkang Shen
- Hao Zhu
- You Zhou
- Yu Liu
- Si Yi
- Lili Dong
- Weipeng Zhao
- David J. Brady
- Xun Cao
- Zhan Ma
- Yi Lin

## Summary
  Myocardial motion tracking stands as an essential clinical tool in the
prevention and detection of Cardiovascular Diseases (CVDs), the foremost cause
of death globally. However, current techniques suffer incomplete and inaccurate
motion estimation of the myocardium both in spatial and temporal dimensions,
hindering the early identification of myocardial dysfunction. In addressing
these challenges, this paper introduces the Neural Cardiac Motion Field
(NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) to
model the 3D structure and the comprehensive 6D forward/backward motion of the
heart. This approach offers memory-efficient storage and continuous capability
to query the precise shape and motion of the myocardium throughout the cardiac
cycle at any specific point. Notably, NeuralCMF operates without the need for
paired datasets, and its optimization is self-supervised through the physics
knowledge priors both in space and time dimensions, ensuring compatibility with
both 2D and 3D echocardiogram video inputs. Experimental validations across
three representative datasets support the robustness and innovative nature of
the NeuralCMF, marking significant advantages over existing state-of-the-arts
in cardiac imaging and motion tracking.


# R-LGP: A Reachability-guided Logic-geometric Programming Framework for Optimal Task and Motion Planning on Mobile Manipulators

[Link to the paper](http://arxiv.org/abs/2310.02791v1)

## Authors
- Kim Tien Ly
- Valeriy Semenov
- Mattia Risiglione
- Wolfgang Merkt
- Ioannis Havoutis

## Summary
  This paper presents an optimization-based solution to task and motion
planning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has
shown promising capabilities for optimally dealing with hybrid TAMP problems
that involve abstract and geometric constraints. However, LGP does not scale
well to high-dimensional systems (e.g. mobile manipulators) and can suffer from
obstacle avoidance issues. In this work, we extend LGP with a sampling-based
reachability graph to enable solving optimal TAMP on high-DoF mobile
manipulators. The proposed reachability graph can incorporate environmental
information (obstacles) to provide the planner with sufficient geometric
constraints. This reachability-aware heuristic efficiently prunes infeasible
sequences of actions in the continuous domain, hence, it reduces replanning by
securing feasibility at the final full trajectory optimization. Our framework
proves to be time-efficient in computing optimal and collision-free solutions,
while outperforming the current state of the art on metrics of success rate,
planning time, path length and number of steps. We validate our framework on
the physical Toyota HSR robot and report comparisons on a series of mobile
manipulation tasks of increasing difficulty.


# Low Resource Summarization using Pre-trained Language Models

[Link to the paper](http://arxiv.org/abs/2310.02790v1)

## Authors
- Mubashir Munaf
- Hammad Afzal
- Naima Iltaf
- Khawir Mahmood

## Summary
  With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.


# Heat flow from a measurement apparatus monitoring a dissipative qubit

[Link to the paper](http://arxiv.org/abs/2310.02789v1)

## Authors
- Tsuyoshi Yamamoto
- Yasuhiro Tokura

## Summary
  We investigate the heat flow of a qubit coupled to heat baths under
continuous quantum measurement. In the steady-state limit, we show that heat
always flows from the measurement apparatus into the qubit regardless of the
measured qubit state and derive lower and upper bounds for the heat current
between the qubit and the measurement apparatus. Furthermore, we study the
transient dynamics of the heat current and the excess heat during the transient
regime.


# MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems

[Link to the paper](http://arxiv.org/abs/2310.02784v1)

## Authors
- Samuel Hsia
- Alicia Golden
- Bilge Acun-Uyan
- Newsha Ardalani
- Zachary DeVito
- Gu-Yeon Wei
- David Brooks
- Carole-Jean Wu

## Summary
  Training and deploying large machine learning (ML) models is time-consuming
and requires significant distributed computing infrastructures. Based on
real-world large model training on datacenter-scale infrastructures, we show
14~32% of all GPU hours are spent on communication with no overlapping
computation. To minimize the outstanding communication latency, in this work,
we develop an agile performance modeling framework to guide parallelization and
hardware-software co-design strategies. Using the suite of real-world large ML
models on state-of-the-art GPU training hardware, we demonstrate 2.24x and
5.27x throughput improvement potential for pre-training and inference
scenarios, respectively.


# Computational Entanglement Theory

[Link to the paper](http://arxiv.org/abs/2310.02783v1)

## Authors
- Rotem Arnon-Friedman
- Zvika Brakerski
- Thomas Vidick

## Summary
  We initiate a rigorous study of computational entanglement theory, inspired
by the emerging usefulness of ideas from quantum information theory in
computational complexity. We define new operational computational measures of
entanglement -- the computational one-shot entanglement cost and distillable
entanglement. We then show that the computational measures are fundamentally
different from their information-theoretic counterparts by presenting gaps
between them.
  We proceed by refining and extending the definition of pseudo-entanglement,
introduced by Aaronson et al., 2022, using the new operational measures; and we
present constructions of pseudo-entangled states (for our new definition) based
on post-quantum cryptographic assumptions.
  Finally, we discuss the relations between computational entanglement theory
and other topics, such as quantum cryptography and notions of pseudoentropy, as
well as the relevance of our new definitions to the study of the AdS/CFT
correspondence.
  We believe that, in addition to the contributions presented in the current
manuscript, our work opens multiple research directions, of relevance both to
the theoretical quantum information theory community as well as for future
applications of quantum networks and cryptography.


# LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater Segmentation with Planetary Simulators

[Link to the paper](http://arxiv.org/abs/2310.02781v1)

## Authors
- Jaewon La
- Jaime Phadke
- Matt Hutton
- Marius Schwinning
- Gabriele De Canio
- Florian Renk
- Lars Kunze
- Matthew Gadd

## Summary
  It is critical for probes landing on foreign planetary bodies to be able to
robustly identify and avoid hazards - as, for example, steep cliffs or deep
craters can pose significant risks to a probe's landing and operational
success. Recent applications of deep learning to this problem show promising
results. These models are, however, often learned with explicit supervision
over annotated datasets. These human-labelled crater databases, such as from
the Lunar Reconnaissance Orbiter Camera (LROC), may lack in consistency and
quality, undermining model performance - as incomplete and/or inaccurate labels
introduce noise into the supervisory signal, which encourages the model to
learn incorrect associations and results in the model making unreliable
predictions. Physics-based simulators, such as the Planet and Asteroid Natural
Scene Generation Utility, have, in contrast, perfect ground truth, as the
internal state that they use to render scenes is known with exactness. However,
they introduce a serious simulation-to-real domain gap - because of fundamental
differences between the simulated environment and the real-world arising from
modelling assumptions, unaccounted for physical interactions, environmental
variability, etc. Therefore, models trained on their outputs suffer when
deployed in the face of realism they have not encountered in their training
data distributions. In this paper, we therefore introduce a system to close
this "realism" gap while retaining label fidelity. We train a CycleGAN model to
synthesise LROC from Planet and Asteroid Natural Scene Generation Utility
(PANGU) images. We show that these improve the training of a downstream crater
segmentation network, with segmentation performance on a test set of real LROC
images improved as compared to using only simulated PANGU images.


# Expected flow networks in stochastic environments and two-player zero-sum games

[Link to the paper](http://arxiv.org/abs/2310.02779v1)

## Authors
- Marco Jiralerspong
- Bilun Sun
- Danilo Vucetic
- Tianyu Zhang
- Yoshua Bengio
- Gauthier Gidel
- Nikolay Malkin

## Summary
  Generative flow networks (GFlowNets) are sequential sampling models trained
to match a given distribution. GFlowNets have been successfully applied to
various structured object generation tasks, sampling a diverse set of
high-reward objects quickly. We propose expected flow networks (EFlowNets),
which extend GFlowNets to stochastic environments. We show that EFlowNets
outperform other GFlowNet formulations in stochastic tasks such as protein
design. We then extend the concept of EFlowNets to adversarial environments,
proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.
We show that AFlowNets learn to find above 80% of optimal moves in Connect-4
via self-play and outperform AlphaZero in tournaments.


# A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare

[Link to the paper](http://arxiv.org/abs/2310.02778v1)

## Authors
- Rui Yang
- Edison Marrese-Taylor
- Yuhe Ke
- Lechao Cheng
- Qingyu Chen
- Irene Li

## Summary
  Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.


# The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models

[Link to the paper](http://arxiv.org/abs/2310.02777v1)

## Authors
- Chenwei Wu
- Li Erran Li
- Stefano Ermon
- Patrick Haffner
- Rong Ge
- Zaiwei Zhang

## Summary
  Compositionality is a common property in many modalities including natural
languages and images, but the compositional generalization of multi-modal
models is not well-understood. In this paper, we identify two sources of
visual-linguistic compositionality: linguistic priors and the interplay between
images and texts. We show that current attempts to improve compositional
generalization rely on linguistic priors rather than on information in the
image. We also propose a new metric for compositionality without such
linguistic priors.


# Dynamic Shuffle: An Efficient Channel Mixture Method

[Link to the paper](http://arxiv.org/abs/2310.02776v1)

## Authors
- Kaijun Gong
- Zhuowen Yin
- Yushu Li
- Kailing Guo
- Xiangmin Xu

## Summary
  The redundancy of Convolutional neural networks not only depends on weights
but also depends on inputs. Shuffling is an efficient operation for mixing
channel information but the shuffle order is usually pre-defined. To reduce the
data-dependent redundancy, we devise a dynamic shuffle module to generate
data-dependent permutation matrices for shuffling. Since the dimension of
permutation matrix is proportional to the square of the number of input
channels, to make the generation process efficiently, we divide the channels
into groups and generate two shared small permutation matrices for each group,
and utilize Kronecker product and cross group shuffle to obtain the final
permutation matrices. To make the generation process learnable, based on
theoretical analysis, softmax, orthogonal regularization, and binarization are
employed to asymptotically approximate the permutation matrix. Dynamic shuffle
adaptively mixes channel information with negligible extra computation and
memory occupancy. Experiment results on image classification benchmark datasets
CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our method
significantly increases ShuffleNets' performance. Adding dynamic generated
matrix with learnable static matrix, we further propose static-dynamic-shuffle
and show that it can serve as a lightweight replacement of ordinary pointwise
convolution.


# Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks

[Link to the paper](http://arxiv.org/abs/2310.02772v1)

## Authors
- Ryuji Saiin
- Tomoya Shirakawa
- Sota Yoshihara
- Yoshihide Sawada
- Hiroyuki Kusumoto

## Summary
  In this article, we propose a new paradigm for training spiking neural
networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are
energy-efficient but difficult to train. Consequently, many researchers have
proposed various methods to solve this problem, among which online training
through time (OTTT) is a method that allows inferring at each time step while
suppressing the memory cost. However, to compute efficiently on GPUs, OTTT
requires operations with spike trains and weighted summation of spike trains
during forwarding. In addition, OTTT has shown a relationship with the Spike
Representation, an alternative training method, though theoretical agreement
with Spike Representation has yet to be proven. Our proposed method can solve
these problems; namely, SAF can halve the number of operations during the
forward process, and it can be theoretically proven that SAF is consistent with
the Spike Representation and OTTT, respectively. Furthermore, we confirmed the
above contents through experiments and showed that it is possible to reduce
memory and training time while maintaining accuracy.


# Controlling the interactions in a cold atom quantum impurity system

[Link to the paper](http://arxiv.org/abs/2310.02771v1)

## Authors
- Thomas Hewitt
- Tom Bertheas
- Manan Jain
- Yusuke Nishida
- Giovanni Barontini

## Summary
  We implement an experimental architecture in which a single atom of K is
trapped in an optical tweezer, and is immersed in a bath of Rb atoms at
ultralow temperatures. In this regime, the motion of the single trapped atom is
confined to the lowest quantum vibrational levels. This realizes an elementary
and fully controllable quantum impurity system. For the trapping of the K atom,
we use a species-selective dipole potential, that allows us to independently
manipulate the quantum impurity and the bath. We concentrate on the
characterization and control of the interactions between the two subsystems. To
this end, we perform Feshbach spectroscopy, detecting several inter-dimensional
confinement-induced Feshbach resonances for the KRb interspecies scattering
length, that parametrizes the strength of the interactions. We compare our data
to a theory for inter-dimensional scattering, finding good agreement. Notably,
we also detect a series of p-wave resonances stemming from the underlying
free-space s-wave interactions. We further determine how the resonances behave
as the temperature of the bath and the dimensionality of the interactions
change. Additionally, we are able to screen the quantum impurity from the bath
by finely tuning the wavelength of the light that produces the optical tweezer,
providing us with a new effective tool to control and minimize the
interactions. Our results open a range of new possibilities in quantum
simulations of quantum impurity models, quantum information, and quantum
thermodynamics, where the interactions between a quantized system and the bath
is a powerful yet largely underutilized resource.


# Hybrid packet switching assisted by classical frame for entanglement-based quantum networks

[Link to the paper](http://arxiv.org/abs/2310.02770v1)

## Authors
- Hao Zhang
- Yuan Li
- Chen Zhang
- Tao Huang

## Summary
  One of the first problems of studying the quantum internet is how to realize
quantum interconnection between users in a quantum network. To address above
problem, by referencing the classical Internet, developing the packet switching
of quantum networks is a promising way. In this paper, we propose a new hybrid
packet switching for entanglement-based quantum networks assisted by classical
frame. Different from the previous packet switching for quantum networks based
on single photon, the frame used in our scheme is pure classical rather than
the classical-quantum structure, and the transmission of classical and quantum
signals over physical channels can be independent, which makes our scheme is
also valid for quantum networks with heralded entanglement generation. Using
our hybrid packet switching, the process of building entanglement channel
between end nodes is analogous to the classical packet-switched networks, which
provides an effective way to build large-scale packet-switched
entanglement-based quantum internet. To verify the feasibility, we perform
end-to-end entanglement distribution using our hybrid packet switching in a
quantum network and simulate the fidelities of distributed state with respect
to the number of hops.


# Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language

[Link to the paper](http://arxiv.org/abs/2305.15842v2)

## Authors
- Nicola Messina
- Jan Sedmidubsky
- Fabrizio Falchi
- Tomáš Rebok

## Summary
  Due to recent advances in pose-estimation methods, human motion can be
extracted from a common video in the form of 3D skeleton sequences. Despite
wonderful application opportunities, effective and efficient content-based
access to large volumes of such spatio-temporal skeleton data still remains a
challenging problem. In this paper, we propose a novel content-based
text-to-motion retrieval task, which aims at retrieving relevant motions based
on a specified natural-language textual description. To define baselines for
this uncharted task, we employ the BERT and CLIP language representations to
encode the text modality and successful spatio-temporal models to encode the
motion modality. We additionally introduce our transformer-based approach,
called Motion Transformer (MoT), which employs divided space-time attention to
effectively aggregate the different skeleton joints in space and time. Inspired
by the recent progress in text-to-image/video matching, we experiment with two
widely-adopted metric-learning loss functions. Finally, we set up a common
evaluation protocol by defining qualitative metrics for assessing the quality
of the retrieved motions, targeting the two recently-introduced KIT
Motion-Language and HumanML3D datasets. The code for reproducing our results is
available at https://github.com/mesnico/text-to-motion-retrieval.


# Self duality in unconventional conformal supersymmetry

[Link to the paper](http://arxiv.org/abs/2310.02769v1)

## Authors
- Pedro D. Alvarez
- Cristóbal Corral
- Jorge Zanelli

## Summary
  In this work, we study (anti-)self duality conditions in unconventional
conformal supersymmetry. We focus on a theory constructed in a
Townsend-MacDowell-Mansouri form for an $SU(2,2|N)$ gauge connection with
matter fields in the adjoint representation. We found bosonic solutions that
correspond to analytic gravitational instantons with nontrivial torsion. These
configurations can be regarded as the torsional generalization of the
Taub-NUT/Bolt-AdS and Eguchi-Hanson metric and they are (anti-)self-dual with
respect to a generalized dual operator. We explore their global properties and
show that they saturate a BPS bound.


# Exact SOHS decompositions of trigonometric univariate polynomials with Gaussian coefficients

[Link to the paper](http://arxiv.org/abs/2202.06544v2)

## Authors
- Victor Magron
- Mohab Safey El Din
- Markus Schweighofer
- Trung Hieu Vu

## Summary
  Certifying the positivity of trigonometric polynomials is of first importance
for design problems in discrete-time signal processing. It is well known from
the Riesz-Fej\'ez spectral factorization theorem that any trigonometric
univariate polynomial positive on the unit circle can be decomposed as a
Hermitian square with complex coefficients. Here we focus on the case of
polynomials with Gaussian integer coefficients, i.e., with real and imaginary
parts being integers. We design, analyze and compare, theoretically and
practically,three hybrid numeric-symbolic algorithms computing weighted sums of
Hermitian squares decompositions for trigonometric univariate polynomials
positive on the unit circle with Gaussian coefficients. The numerical steps the
first and second algorithm rely on are complex root isolation and semidefinite
programming, respectively. An exact sum of Hermitian squares decomposition is
obtained thanks to compensation techniques. The third algorithm, also based on
complex semidefinite programming, is an adaptation of the rounding and
projection algorithm by Peyrl and Parrilo. For all three algorithms, we prove
bit complexity and output size estimates that are polynomial in the degree of
the input and linear in the maximum bitsize of its coefficients. We compare
their performance on randomly chosen benchmarks, and further design a certified
finite impulse filter.


# Likelihood-Based Methods Improve Parameter Estimation in Opinion Dynamics Models

[Link to the paper](http://arxiv.org/abs/2310.02766v1)

## Authors
- Jacopo Lenti
- Gianmarco De Francisci Morales
- Corrado Monti

## Summary
  We show that a maximum likelihood approach for parameter estimation in
agent-based models (ABMs) of opinion dynamics outperforms the typical
simulation-based approach. Simulation-based approaches simulate the model
repeatedly in search of a set of parameters that generates data similar enough
to the observed one. In contrast, likelihood-based approaches derive a
likelihood function that connects the unknown parameters to the observed data
in a statistically principled way. We compare these two approaches on the
well-known bounded-confidence model of opinion dynamics. We do so on three
realistic scenarios of increasing complexity depending on data availability:
(i) fully observed opinions and interactions, (ii) partially observed
interactions, (iii) observed interactions with noisy proxies of the opinions.
We highlight how identifying observed and latent variables is fundamental for
connecting the model to the data. To realize the likelihood-based approach, we
first cast the model into a probabilistic generative guise that supports a
proper data likelihood. Then, we describe the three scenarios via probabilistic
graphical models and show the nuances that go into translating the model.
Finally, we implement the resulting probabilistic models in an automatic
differentiation framework (PyTorch). This step enables easy and efficient
maximum likelihood estimation via gradient descent. Our experimental results
show that the maximum likelihood estimates are up to 4x more accurate and
require up to 200x less computational time.


# Quantum Monte Carlo simulations of thermodynamic properties of attractive SU($3$) Dirac fermions

[Link to the paper](http://arxiv.org/abs/2101.00804v3)

## Authors
- Xiang Li
- Han Xu
- Yu Wang

## Summary
  We employ the determinant quantum Monte Carlo method to study the
finite-temperature properties of the half-filled attractive SU($3$) Hubbard
model on a honeycomb lattice. We calculate the phase diagram in which the phase
boundary separates the disordered phase and the charge-density-wave (CDW) phase
and the transition temperature $T_{\text{tr}}(|U|)$ varies non-monotonically
with attractive Hubbard interaction $|U|$. As the Hubbard $|U|$ increases at
constant temperature $T<\text{max}(T_{\text{tr}}(|U|))$, the system first
undergoes a transition from thermal Dirac semimetal phase to CDW phase, and
eventually the CDW state is thermally melted at a strong Hubbard $|U|$ where
the system enters a trion liquid phase. In between the two transition points
the non-monotonic $|U|$ dependence of CDW order strength is strikingly
different from the zero-temperature monotonic behavior. In the trion CDW state
where off-site trions arise from quantum fluctuations (a fermion inside an
on-site trion hops to a nearest-neighbor site), the simulated triple occupancy
at constant Hubbard $|U|$ surprisingly increases with temperature, implying
that the formation of off-site trions is suppressed by the thermal
delocalization of on-site trions. We have also calculated the
entropy-temperature relations for various attractive Hubbrad interactions,
which exhibit the prominent characteristic of the Pomeranchuk effect. Our work
has revealed that the formation of on-site and off-site trions has significant
consequences for thermodynamic properties of SU(3) Dirac fermions.


# Small k-pairable states

[Link to the paper](http://arxiv.org/abs/2309.09956v2)

## Authors
- Nathan Claudet
- Mehdi Mhalla
- Simon Perdrix

## Summary
  A $k$-pairable $n$-qubit state is a resource state that allows Local
Operations and Classical Communication (LOCC) protocols to generate EPR-pairs
among any $k$-disjoint pairs of the $n$ qubits. Bravyi et al. introduced a
family of $k$-pairable $n$-qubit states, where $n$ grows exponentially with
$k$. Our primary contribution is to establish the existence of 'small' pairable
quantum states. Specifically, we present a family of $k$-pairable $n$-qubit
graph states, where $n$ is polynomial in $k$, namely $n=O(k^3\ln^3k)$. Our
construction relies on probabilistic methods. Furthermore, we provide an upper
bound on the pairability of any arbitrary quantum state based on the support of
any local unitary transformation that has the shared state as a fixed point.
This lower bound implies that the pairability of a graph state is at most half
of the minimum degree up to local complementation of the underlying graph,
i.e., $k(|G \rangle)\le \lceil \delta_{loc}(G)/2\rceil$. We also investigate
the related combinatorial problem of $k$-vertex-minor-universality: a graph $G$
is $k$-vertex-minor-universal if any graph on any $k$ of its vertices is a
vertex-minor of $G$. When a graph is $2k$-vertex-minor-universal, the
corresponding graph state is $k$-pairable. More precisely, one can create not
only EPR-pairs but also any stabilizer state on any $2k$ qubits through local
operations and classical communication. We establish the existence of
$k$-vertex-minor-universal graphs of order $O(k^4 \ln k)$. Finally, we explore
a natural extension of pairability in the presence of errors or malicious
parties and show that vertex-minor-universality ensures a robust form of
pairability.


# Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations

[Link to the paper](http://arxiv.org/abs/2305.08099v3)

## Authors
- Weiwei Lin
- Chenhang He
- Man-Wai Mak
- Youzhi Tu

## Summary
  Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have
demonstrated state-of-the-art performance on automatic speech recognition (ASR)
and proved to be extremely useful in low label-resource settings. However, the
success of SSL models has yet to transfer to utterance-level tasks such as
speaker, emotion, and language recognition, which still require supervised
fine-tuning of the SSL models to obtain good performance. We argue that the
problem is caused by the lack of disentangled representations and an
utterance-level learning objective for these tasks. Inspired by how HuBERT uses
clustering to discover hidden acoustic units, we formulate a factor analysis
(FA) model that uses the discovered hidden acoustic units to align the SSL
features. The underlying utterance-level representations are disentangled from
the content of speech using probabilistic inference on the aligned features.
Furthermore, the variational lower bound derived from the FA model provides an
utterance-level objective, allowing error gradients to be backpropagated to the
Transformer layers to learn highly discriminative acoustic units. When used in
conjunction with HuBERT's masked prediction training, our models outperform the
current best model, WavLM, on all utterance-level non-semantic tasks on the
SUPERB benchmark with only 20% of labeled data.


# A Hybrid Quantum-Classical Approach to the Electric Mobility Problem

[Link to the paper](http://arxiv.org/abs/2310.02760v1)

## Authors
- Margarita Veshchezerova
- Mikhail Somov
- David Bertsche
- Steffen Limmer
- Sebastian Schmitt
- Michael Perelshtein
- Ayush Joshi Tripathi

## Summary
  We suggest a hybrid quantum-classical routine for the NP-hard Electric
Vehicle Fleet Charging and Allocation Problem. The original formulation is a
Mixed Integer Linear Program with continuous variables and inequality
constraints. To separate inequality constraints that are difficult for quantum
routines we use a decomposition in master and pricing problems: the former
targets the assignment of vehicles to reservations and the latter suggests
vehicle exploitation plans that respect the battery state-of-charge
constraints. The master problem is equivalent to the search for an optimal set
partition. In our hybrid scheme, the master problem is reformulated in a
quadratic unconstrained binary optimization problem which can be solved with
quantum annealing on the DWave Advantage system. On large instances, we
benchmark the performance of the decomposition technique with classical and
quantum-inspired metaheuristics: simulated annealing, tabu search, and vector
annealing by NEC. The numerical results with purely classical solvers are
comparable to the solutions from the traditional mixed integer linear
programming approaches in terms of solution quality while being faster. In
addition, it scales better to larger instances. The major advantage of the
proposed approach is that it enables quantum-based methods for this realistic
problem with many inequality constraints. We show this by initial studies on
DWave hardware where optimal solutions can be found for small instances.


# Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms

[Link to the paper](http://arxiv.org/abs/2310.02759v1)

## Authors
- Bagiya Lakshmi S
- Sanjjushri Varshini R
- Rohith Mahadevan
- Raja CSP Raman

## Summary
  Automated Essay Score (AES) is proven to be one of the cutting-edge
technologies. Scoring techniques are used for various purposes. Reliable scores
are calculated based on influential variables. Such variables can be computed
by different methods based on the domain. The research is concentrated on the
user's understanding of a given topic. The analysis is based on a scoring index
by using Large Language Models. The user can then compare and contrast the
understanding of a topic that they recently learned. The results are then
contributed towards learning analytics and progression is made for enhancing
the learning ability. In this research, the focus is on summarizing a PDF
document and gauging a user's understanding of its content. The process
involves utilizing a Langchain tool to summarize the PDF and extract the
essential information. By employing this technique, the research aims to
determine how well the user comprehends the summarized content.


# Rethinking superpixel segmentation from biologically inspired mechanisms

[Link to the paper](http://arxiv.org/abs/2309.13438v2)

## Authors
- TingYu Zhao
- Bo Peng
- Yuan Sun
- DaiPeng Yang
- ZhenGuang Zhange
- Xi Wu

## Summary
  Recently, advancements in deep learning-based superpixel segmentation methods
have brought about improvements in both the efficiency and the performance of
segmentation. However, a significant challenge remains in generating
superpixels that strictly adhere to object boundaries while conveying rich
visual significance, especially when cross-surface color correlations may
interfere with objects. Drawing inspiration from neural structure and visual
mechanisms, we propose a biological network architecture comprising an Enhanced
Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel
segmentation. The ESM enhances semantic information by simulating the
interactive projection mechanisms of the visual cortex. Additionally, the BAL
emulates the spatial frequency characteristics of visual cortical cells to
facilitate the generation of superpixels with strong boundary adherence. We
demonstrate the effectiveness of our approach through evaluations on both the
BSDS500 dataset and the NYUv2 dataset.


# Evidence for 3XMM J185246.6+003317 as a massive magnetar with a low magnetic field

[Link to the paper](http://arxiv.org/abs/2210.06648v2)

## Authors
- Rafael C. R. de Lima
- Jonas P. Pereira
- Jaziel G. Coelho
- Rafael C. Nunes
- Paulo E. F. Stecchini
- Manuel Castro
- Pierre Gomes
- Rodrigo R. da Silva
- Claudia V. Rodrigues
- José C. N. de Araujo
- Michał Bejger
- Paweł Haensel
- J. Leszek Zdunik

## Summary
  3XMM J185246.6+003317 is a slowly rotating soft-gamma repeater (neutron star)
in the vicinity of the supernova remnant Kes\,79. So far, observations have
only set upper limits to its surface magnetic field and spindown, and there is
no estimate for its mass and radius. Using ray-tracing modelling and Bayesian
inference for the analysis of several light curves spanning a period of around
three weeks, we have found that it may be one of the most massive neutron stars
to date. In addition, our analysis suggests a multipolar magnetic field
structure with a subcritical field strength and a carbon atmosphere
composition. Due to the time-resolution limitation of the available light
curves, we estimate the surface magnetic field and the mass to be $\log_{10}
(B/{\rm G}) = 11.89^{+0.19}_{-0.93}$ and $M=2.09^{+0.16}_{-0.09}$~$M_{\odot}$
at $1\sigma$ confidence level, while the radius is estimated to be
$R=12.02^{+1.44}_{-1.42}$ km at $2\sigma$ confidence level. They were verified
by simulations, i.e., data injections with known model parameters, and their
subsequent recovery. The best-fitting model has three small hot spots, two of
them in the southern hemisphere. These are, however, just back-of-the-envelope
estimates and conclusions, based on a simple ray-tracing model with anisotropic
emission; we also estimate the impact of modelling on the parameter
uncertainties and the relevant phenomena on which to focus in more precise
analyses. We interpret the above best-fitting results as due to accretion of
supernova layers/interstellar medium onto 3XMM J185246.6+003317 leading to
burying and a subsequent re-emergence of the magnetic field, and a carbon
atmosphere being formed possibly due to hydrogen/helium diffusive nuclear
burning. Finally, we briefly discuss some consequences of our findings for
superdense matter constraints.


# AgEncID: Aggregate Encryption Individual Decryption of Key for FPGA Bitstream IP Cores in Cloud

[Link to the paper](http://arxiv.org/abs/2309.16282v2)

## Authors
- Mukta Debnath
- Krishnendu Guha
- Debasri Saha
- Susmita Sur-Kolay

## Summary
  Cloud computing platforms are progressively adopting Field Programmable Gate
Arrays to deploy specialized hardware accelerators for specific computational
tasks. However, the security of FPGA-based bitstream for Intellectual Property,
IP cores from unauthorized interception in cloud environments remains a
prominent concern. Existing methodologies for protection of such bitstreams
possess several limitations, such as requiring a large number of keys, tying
bitstreams to specific FPGAs, and relying on trusted third parties. This paper
proposes Aggregate Encryption and Individual Decryption, a cryptosystem based
on key aggregation to enhance the security of FPGA-based bitstream for IP cores
and to address the pitfalls of previous related works. In our proposed scheme,
IP providers can encrypt their bitstreams with a single key for a set S of FPGA
boards, with which the bitstreams can directly be decrypted on any of the FPGA
boards in S. Aggregate encryption of the key is performed in a way which
ensures that the key can solely be obtained onboard through individual
decryption employing the board's private key, thus facilitating secure key
provisioning. The proposed cryptosystem is evaluated mainly on Zynq FPGAs. The
outcomes demonstrate that our cryptosystem not only outperforms existing
techniques with respect to resource, time and energy significantly but also
upholds robust security assurances.


# Modeling of Annual and Daily Electricity Demand of Retrofitted Heat Pumps based on Gas Smart Meter Data

[Link to the paper](http://arxiv.org/abs/2310.02756v1)

## Authors
- Daniel R. Bayer
- Marco Pruckner

## Summary
  Currently, gas furnaces are common heating systems in Europe. Due to the
efforts for decarbonizing the complete energy sector, heat pumps should
continuously replace existing gas furnaces. At the same time, the
electrification of the heating sector represents a significant challenge for
the power grids and their operators. Thus, new approaches are required to
estimate the additional electricity demand to operate heat pumps. The
electricity required by a heat pump to produce a given amount of heat depends
on the Seasonal Performance Factor (SPF), which is hard to model in theory due
to many influencing factors and hard to measure in reality as the heat produced
by a heat pump is usually not measured. Therefore, we show in this paper that
collected smart meter data forms an excellent data basis on building level for
modeling heat demand and the SPF. We present a novel methodology to estimate
the mean SPF based on an unpaired dataset of heat pump electricity and gas
consumption data taken from buildings within the same city by comparing the
distributions using the Jensen-Shannon Divergence (JSD). Based on a real-world
dataset, we evaluate this novel method by predicting the electricity demand
required if all gas furnaces in a city were replaced by heat pumps and briefly
highlight possible use cases.


# MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis

[Link to the paper](http://arxiv.org/abs/2309.15521v2)

## Authors
- Angelo Yamachui Sitcheu
- Nils Friederich
- Simon Baeuerle
- Oliver Neumann
- Markus Reischl
- Ralf Mikut

## Summary
  Nowadays, Machine Learning (ML) is experiencing tremendous popularity that
has never been seen before. The operationalization of ML models is governed by
a set of concepts and methods referred to as Machine Learning Operations
(MLOps). Nevertheless, researchers, as well as professionals, often focus more
on the automation aspect and neglect the continuous deployment and monitoring
aspects of MLOps. As a result, there is a lack of continuous learning through
the flow of feedback from production to development, causing unexpected model
deterioration over time due to concept drifts, particularly when dealing with
scarce data. This work explores the complete application of MLOps in the
context of scarce data analysis. The paper proposes a new holistic approach to
enhance biomedical image analysis. Our method includes: a fingerprinting
process that enables selecting the best models, datasets, and model development
strategy relative to the image analysis task at hand; an automated model
development stage; and a continuous deployment and monitoring process to ensure
continuous learning. For preliminary results, we perform a proof of concept for
fingerprinting in microscopic image datasets.


# LC-Score: Reference-less estimation of Text Comprehension Difficulty

[Link to the paper](http://arxiv.org/abs/2310.02754v1)

## Authors
- Paul Tardy
- Charlotte Roze
- Paul Poupet

## Summary
  Being able to read and understand written text is critical in a digital era.
However, studies shows that a large fraction of the population experiences
comprehension issues. In this context, further initiatives in accessibility are
required to improve the audience text comprehension. However, writers are
hardly assisted nor encouraged to produce easy-to-understand content. Moreover,
Automatic Text Simplification (ATS) model development suffers from the lack of
metric to accurately estimate comprehension difficulty We present
\textsc{LC-Score}, a simple approach for training text comprehension metric for
any French text without reference \ie predicting how easy to understand a given
text is on a $[0, 100]$ scale. Our objective with this scale is to
quantitatively capture the extend to which a text suits to the \textit{Langage
Clair} (LC, \textit{Clear Language}) guidelines, a French initiative closely
related to English Plain Language. We explore two approaches: (i) using
linguistically motivated indicators used to train statistical models, and (ii)
neural learning directly from text leveraging pre-trained language models. We
introduce a simple proxy task for comprehension difficulty training as a
classification task. To evaluate our models, we run two distinct human
annotation experiments, and find that both approaches (indicator based and
neural) outperforms commonly used readability and comprehension metrics such as
FKGL and SAMSA.


# MUNCH: Modelling Unique 'N Controllable Heads

[Link to the paper](http://arxiv.org/abs/2310.02753v1)

## Authors
- Debayan Deb
- Suvidha Tripathi
- Pranit Puri

## Summary
  The automated generation of 3D human heads has been an intriguing and
challenging task for computer vision researchers. Prevailing methods synthesize
realistic avatars but with limited control over the diversity and quality of
rendered outputs and suffer from limited correlation between shape and texture
of the character. We propose a method that offers quality, diversity, control,
and realism along with explainable network design, all desirable features to
game-design artists in the domain. First, our proposed Geometry Generator
identifies disentangled latent directions and generate novel and diverse
samples. A Render Map Generator then learns to synthesize multiply high-fidelty
physically-based render maps including Albedo, Glossiness, Specular, and
Normals. For artists preferring fine-grained control over the output, we
introduce a novel Color Transformer Model that allows semantic color control
over generated maps. We also introduce quantifiable metrics called Uniqueness
and Novelty and a combined metric to test the overall performance of our model.
Demo for both shapes and textures can be found:
https://munch-seven.vercel.app/. We will release our model along with the
synthetic dataset.


# On the Observables of Renormalizable Interactions

[Link to the paper](http://arxiv.org/abs/2310.00586v2)

## Authors
- Kang-Sin Choi

## Summary
  The physical observables in quantum field theory, requiring renormalization,
must not depend on regularizations and be finite. We identify observable
renormalizable operators and show that any sensible regularization gives the
same finite quantum corrections to them and cancels the divergences including
the quadratic. We calculate the Higgs mass correction from the top quark loop,
which is finite, does not contain the Wilsonian cutoff and exhibits power
running.


# SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning

[Link to the paper](http://arxiv.org/abs/2310.02751v1)

## Authors
- JunHoo Lee
- Jayeon Yoo
- Nojun Kwak

## Summary
  In this paper, we hypothesize that gradient-based meta-learning (GBML)
implicitly suppresses the Hessian along the optimization trajectory in the
inner loop. Based on this hypothesis, we introduce an algorithm called SHOT
(Suppressing the Hessian along the Optimization Trajectory) that minimizes the
distance between the parameters of the target and reference models to suppress
the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does
not increase the computational complexity of the baseline model much. It is
agnostic to both the algorithm and architecture used in GBML, making it highly
versatile and applicable to any GBML baseline. To validate the effectiveness of
SHOT, we conduct empirical tests on standard few-shot learning tasks and
qualitatively analyze its dynamics. We confirm our hypothesis empirically and
demonstrate that SHOT outperforms the corresponding baseline. Code is available
at: https://github.com/JunHoo-Lee/SHOT


# Adaptive Quantum State Tomography with Active Learning

[Link to the paper](http://arxiv.org/abs/2203.15719v5)

## Authors
- Hannah Lange
- Matjaž Kebrič
- Maximilian Buser
- Ulrich Schollwöck
- Fabian Grusdt
- Annabelle Bohrdt

## Summary
  Recently, tremendous progress has been made in the field of quantum science
and technologies: different platforms for quantum simulation as well as quantum
computing, ranging from superconducting qubits to neutral atoms, are starting
to reach unprecedentedly large systems. In order to benchmark these systems and
gain physical insights, the need for efficient tools to characterize quantum
states arises. The exponential growth of the Hilbert space with system size
renders a full reconstruction of the quantum state prohibitively demanding in
terms of the number of necessary measurements. Here we propose and implement an
efficient scheme for quantum state tomography using active learning. Based on a
few initial measurements, the active learning protocol proposes the next
measurement basis, designed to yield the maximum information gain. We apply the
active learning quantum state tomography scheme to reconstruct different
multi-qubit states with varying degree of entanglement as well as to ground
states of the XXZ model in 1D and a kinetically constrained spin chain. In all
cases, we obtain a significantly improved reconstruction as compared to a
reconstruction based on the exact same number of measurements and measurement
configurations, but with randomly chosen basis configurations. Our scheme is
highly relevant to gain physical insights in quantum many-body systems as well
as for benchmarking and characterizing quantum devices, e.g. for quantum
simulation, and paves the way for scalable adaptive protocols to probe,
prepare, and manipulate quantum systems.


# Realization and simulations of the new SPES Beam Cooler

[Link to the paper](http://arxiv.org/abs/2310.02750v1)

## Authors
- A. Ruzzon
- M. Maggiore
- C. Roncolato
- G. Ban
- J. F. Cam
- C. Gautier
- C. Vandamme

## Summary
  In order to allow a good separation of isotopes in a High Resolution Mass
Spectrometer (HRMS), the transverse emittance and the energy spread of the beam
should have very low values, for this reason a Beam Cooler (BC) is planned to
be located between the ISOL target, i.e. the beam source, and the HRMS in the
new project Selective Production of Exotic Species (SPES).
  In the SPES project the spectrometer resolution must be higher than dm/m=5E-5
and thus the features of the beam at the entrance of the HRMS should be at
least emittance (95%, normalized) < 8.3E-3 pi mm mrad and sigma_E < 1.5eV.
  A new BC has been designed and realized by the Laboratoire de Physique
Corpuscolaire (LPC) at Caen, France, for the SPES facility at Laboratori
Nazionali di Legnaro (LNL), near Padova, Italy.
  BCs cool down the beam thanks to a dissipative process in which the thermal
energy passes from the beam ions to another medium whose constituent is
typically much lighter, Helium gas in our case. This process takes place inside
a confinement system that on the one hand to limit the spread of the cooling
medium, and on the other to allow the beam to continue along the required
trajectory, in the presented device it is a row of radio frequency quadrupoles
in an almost closed chamber.
  This document presents the main features of the new BC together with the
results of a preliminary study where the beam dynamic has been simulated. The
analysis embeds also the investigation of the gas distribution inside and
outside the BC.
  The beam dynamic simulations are based on the Simion code while the
estimation of the gas distribution is computed with MolFlow+.
  Simulations show that accurately setting the BC leads to a large improvement
of the emittance while the energy spread still needs to be improved. Limiting
the gas pressure in the acceleration zone seems to allow the required final
boost.


# Hybrid Quantum Machine Learning Assisted Classification of COVID-19 from Computed Tomography Scans

[Link to the paper](http://arxiv.org/abs/2310.02748v1)

## Authors
- Leo Sünkel
- Darya Martyniuk
- Julia J. Reichwald
- Andrei Morariu
- Raja Havish Seggoju
- Philipp Altmann
- Christoph Roch
- Adrian Paschke

## Summary
  Practical quantum computing (QC) is still in its infancy and problems
considered are usually fairly small, especially in quantum machine learning
when compared to its classical counterpart. Image processing applications in
particular require models that are able to handle a large amount of features,
and while classical approaches can easily tackle this, it is a major challenge
and a cause for harsh restrictions in contemporary QC. In this paper, we apply
a hybrid quantum machine learning approach to a practically relevant problem
with real world-data. That is, we apply hybrid quantum transfer learning to an
image processing task in the field of medical image processing. More
specifically, we classify large CT-scans of the lung into COVID-19, CAP, or
Normal. We discuss quantum image embedding as well as hybrid quantum machine
learning and evaluate several approaches to quantum transfer learning with
various quantum circuits and embedding techniques.


# Averaging generalized scalar field cosmologies IV: locally rotationally symmetric Bianchi V model

[Link to the paper](http://arxiv.org/abs/2310.02741v1)

## Authors
- Alfredo D. Millano
- Genly Leon

## Summary
  This research focuses on scalar field cosmologies with a generalized harmonic
potential. Our attention is centred on the anisotropic LRS Bianchi I and III
metrics, Bianchi V metrics, and their isotropic limits. We provide a
comprehensive overview of the first two metrics classes and offer new findings
for Bianchi V metrics. We show that the Hubble parameter is a time-dependent
perturbation parameter that controls the magnitude of the error between
full-system and time-averaged solutions as it decreases, such that those
complete and time-averaged systems have the same asymptotic behaviour.
Therefore, oscillations entering the system can be controlled and smoothed out,
which simplifies the problem at hand.


# Quantum-secured single-pixel imaging with enhanced security

[Link to the paper](http://arxiv.org/abs/2209.06365v4)

## Authors
- Jaesung Heo
- Junghyun Kim
- Taek Jeong
- Yong Sup Ihn
- Duk Y. Kim
- Zaeill Kim
- Yonggi Jo

## Summary
  In this paper, we propose a novel quantum-secured single-pixel imaging method
that utilizes non-classical correlations of a photon pair. Our method can
detect any attempts to deceive it by exploiting a non-classical correlation of
photon pairs while rejecting strong chaotic light illumination through photon
heralding. A security analysis based on polarization-correlation has been
conducted, demonstrating that our method has improved security compared to
existing quantum-secured imaging. More specifically, a partial deceiving
attack, which sends a mixture of a true and a false signal, can be detected
with our proposed analysis, while currently employed methods cannot. We also
provide proof-of-principle demonstrations of our method and trustworthy images
reconstructed using our security analysis. Our method can be developed using
matured techniques used in quantum secure communication, thus offering a
promising direction for practical applications in secure imaging.


